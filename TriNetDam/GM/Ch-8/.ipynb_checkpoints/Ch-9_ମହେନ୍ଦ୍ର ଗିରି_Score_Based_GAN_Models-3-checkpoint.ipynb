{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb00189",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright (c) 2004 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f75cbe",
   "metadata": {},
   "source": [
    "##  An Example of Score-Based Generative Models: Variance Exploding PF-ODE\n",
    "\n",
    "###  Model Formulation\n",
    "\n",
    "To define our own score-based generative model (SBGM), we need the following elements:\n",
    "- The drift $ f(x, t) $\n",
    "- The diffusion $ g(t) $\n",
    "- The form of $ p_0^t(x_t | x_0) $\n",
    "\n",
    "In [1] and [20], three examples of SBGM are provided: \n",
    "- Variance Exploding (VE) SDE\n",
    "- Variance Preserving (VP) SDE\n",
    "- Sub-VP SDE\n",
    "\n",
    "Here, we focus on the **VE SDE**, which assumes the following choices for the drift and diffusion:\n",
    "\n",
    "$$\n",
    "f(x, t) = 0, \\quad g(t) = \\sigma t,\n",
    "$$\n",
    "where \\( \\sigma > 0 \\) is a hyperparameter and $ t \\in [0, 1] $.\n",
    "\n",
    "By plugging in the choices for $ f(x, t) $ and $ g(t) $ into the general form of the PF-ODE, we get:\n",
    "\n",
    "$$\n",
    "\\frac{dx_t}{dt} = - \\frac{\\sigma^2 t}{2} \\nabla_{x_t} \\ln p_t(x_t), \\tag{9.31}\n",
    "$$\n",
    "\n",
    "where the term $ \\nabla_{x_t} \\ln p_t(x_t) $ represents the score function.\n",
    "\n",
    "Now, to learn the score model, we need to define the conditional distribution $ p_0^t(x_t | x_0) $. Fortunately, the theory of SDEs (e.g., see Chapter 5 of [19]) gives us a way to calculate $ p_0^t(x_t | x_0) $.\n",
    "\n",
    "The solution for $ p_0^t(x_t | x_0) $ is given by:\n",
    "\n",
    "$$\n",
    "p_0^t(x_t | x_0) = \\mathcal{N}(x_t | x_0, (\\sigma^2 t - 1)I), \\quad \\text{for} \\quad t \\in [0, 1]. \\tag{9.32}\n",
    "$$\n",
    "\n",
    "The variance function over time is:\n",
    "\n",
    "$$\n",
    "\\sigma_t^2 = (\\sigma^2 t - 1), \\tag{9.33}\n",
    "$$\n",
    "\n",
    "Thus, the final distribution $ p_1(x) $ (for sufficiently large $ \\sigma $) is approximately:\n",
    "\n",
    "$$\n",
    "p_1(x) = p_0(x_0) * \\mathcal{N}(x | x_0, (\\sigma^2 - 1)I). \\tag{9.34}\n",
    "$$\n",
    "\n",
    "For large $ \\sigma $, the distribution becomes:\n",
    "\n",
    "$$\n",
    "p_1(x) \\approx \\mathcal{N}(x | 0, (\\sigma^2 - 1)I). \\tag{9.35}\n",
    "$$\n",
    "\n",
    "###  The Choice of $ \\lambda_t $\n",
    "\n",
    "One important consideration is the choice of $ \\lambda_t $ in the definition of the loss function $ L_t(\\theta) $. \n",
    "\n",
    "Although Ho et al. [3] simply set $ \\lambda_t \\equiv 1 $, Song and Kingma [21] showed that setting $ \\lambda_t = \\sigma_t^2 $ is actually beneficial for the VE PF-ODE. This choice of $ \\lambda_t $ helps us use the sum over $ L_t(\\theta) $ as a proxy for the log-likelihood function, which is useful for early stopping during training.\n",
    "\n",
    "This leads to a simpler loss function:\n",
    "\n",
    "$$\n",
    "L_t(\\theta) = \\mathbb{E}_{x_0 \\sim p_d} \\left[ \\mathbb{E}_{x_t \\sim p_0^t(x_t | x_0)} \\left[ \\lambda_t \\| s_\\theta(x_t, t) - \\nabla_{x_t} \\ln p_0^t(x_t | x_0) \\|^2 \\right] \\right].\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6630de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the score model (using a simple linear model for demonstration)\n",
    "class ScoreModel:\n",
    "    def __init__(self, input_dim):\n",
    "        self.input_dim = input_dim\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        # A simple model that approximates the score function (for simplicity)\n",
    "        return np.array([x_i / (t + 1e-8) for x_i in x])\n",
    "\n",
    "# Drift and Diffusion Functions\n",
    "def f(x, t):\n",
    "    return 0.0  # Zero drift (no movement)\n",
    "\n",
    "def g(t, sigma):\n",
    "    return sigma * t  # Diffusion scales with time t\n",
    "\n",
    "# Noisy distribution function based on the VE-SDE model\n",
    "def noisy_distribution(x0, t, sigma):\n",
    "    noise_std = (sigma**2 * t - 1)**0.5\n",
    "    noise = np.random.normal(0, noise_std, size=x0.shape)\n",
    "    return x0 + noise\n",
    "\n",
    "# Denoising score matching loss function\n",
    "def denoising_score_matching_loss(model, x0, t, sigma):\n",
    "    x_t = noisy_distribution(x0, t, sigma)\n",
    "    \n",
    "    # True score function (gradient of log p_0^t(x_t | x_0))\n",
    "    true_score = (x_t - x0) / ((sigma**2 * t - 1)**0.5 + 1e-8)\n",
    "    \n",
    "    # Model prediction (score function)\n",
    "    predicted_score = model.forward(x_t, t)\n",
    "    \n",
    "    # Compute loss: Mean squared error between true and predicted score\n",
    "    loss = np.mean((predicted_score - true_score) ** 2)\n",
    "    return loss\n",
    "\n",
    "# Backward Euler's method for sampling\n",
    "def backward_euler_step(x_t, t, model, sigma, delta_t=0.1):\n",
    "    score = model.forward(x_t, t)\n",
    "    dx_t = -0.5 * g(t, sigma)**2 * score * delta_t\n",
    "    x_t_next = x_t + dx_t\n",
    "    return x_t_next\n",
    "\n",
    "# Training loop\n",
    "def train_score_model(model, data, sigma, num_epochs=100, learning_rate=1e-3):\n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for x0 in data:\n",
    "            t = random.random()  # Random time steps\n",
    "            \n",
    "            # Compute the denoising score matching loss\n",
    "            loss = denoising_score_matching_loss(model, x0, t, sigma)\n",
    "            epoch_loss += loss\n",
    "        \n",
    "        losses.append(epoch_loss / len(data))\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(data)}')\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Sampling from the trained model using backward Euler's method\n",
    "def sample_from_model(model, initial_condition, sigma, num_steps=100, delta_t=0.1):\n",
    "    x_t = initial_condition\n",
    "    for step in range(num_steps):\n",
    "        t = step / num_steps  # Linearly increasing t\n",
    "        x_t = backward_euler_step(x_t, t, model, sigma, delta_t)\n",
    "    return x_t\n",
    "\n",
    "# Plotting functions\n",
    "def plot_loss_curve(losses):\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss Curve\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_generated_samples(original_data, generated_sample):\n",
    "    original_data = np.array(original_data)\n",
    "    plt.scatter(original_data[:, 0], original_data[:, 1], label='Original Data', alpha=0.5)\n",
    "    generated_sample = np.array(generated_sample)\n",
    "    plt.scatter(generated_sample[:, 0], generated_sample[:, 1], label='Generated Sample', color='red', marker='x')\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.title(\"Original vs Generated Samples\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    # Generate synthetic data (e.g., from a Gaussian distribution)\n",
    "    initial_data = np.random.randn(100, 2)  # 100 samples, 2D data\n",
    "    \n",
    "    # Initialize the score model\n",
    "    model = ScoreModel(input_dim=2)\n",
    "    \n",
    "    # Hyperparameter\n",
    "    sigma = 2.0  # Variance parameter for the VE-SDE\n",
    "    \n",
    "    # Train the model and get loss values\n",
    "    losses = train_score_model(model, initial_data, sigma, num_epochs=100, learning_rate=1e-3)\n",
    "    \n",
    "    # Plot the loss curve\n",
    "    plot_loss_curve(losses)\n",
    "    \n",
    "    # Sample from the model after training\n",
    "    initial_condition = np.random.randn(2)  # Starting point for sampling\n",
    "    generated_sample = sample_from_model(model, initial_condition, sigma)\n",
    "    \n",
    "    # Plot original vs generated samples\n",
    "    plot_generated_samples(initial_data, [generated_sample])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e11b21",
   "metadata": {},
   "source": [
    "### 9.3.3.2 The Choice of \\( \\lambda_t \\)\n",
    "\n",
    "The last remark, before we move to the training procedure, is about the choice of \\( \\lambda_t \\) in the definition of \\( L_t(\\theta) \\). So far, I simply omitted that, but I had a good reason. Ho et al. [3] simply set \\( \\lambda_t \\equiv 1 \\). Done! Really though? Well, as you can imagine, but smart reader, that is not so easy. Song and Kingma [21] showed that it is actually beneficial to set \\( \\lambda_t = \\sigma_t^2 \\) in the case of VE PF-ODE. Then, we can even use the sum over \\( L_t(\\theta) \\) as a proxy to the log-likelihood function. We will take advantage of that for early stopping in our training procedure.\n",
    "\n",
    "### 9.3.3.3 Training\n",
    "\n",
    "We present a training procedure based on the chosen example of the VE SBGM. As we outlined earlier in the case of the score matching method, the procedure is relatively easy and straightforward. It consists of the following steps:\n",
    "\n",
    "#### Training Procedure for VE SBGM\n",
    "\n",
    "1. Pick a datapoint \\( x_0 \\).\n",
    "2. Sample \\( x_1 \\sim \\pi(x) = \\mathcal{N}(x | 0, I) \\).\n",
    "3. Sample \\( t \\sim \\text{Uniform}(0, 1) \\).\n",
    "4. Calculate \\( x_t = x_0 + 2 \\ln\\left( \\frac{1}{\\sigma} \\right) (\\sigma^2 t - 1) \\cdot x_1 \\). This is a sample from \\( p_0^t(x_t | x_0) \\).\n",
    "5. Evaluate the score model at \\( (x_t, t) \\), \\( s_\\theta(x_t, t) \\).\n",
    "6. Calculate the score matching loss for a single sample:\n",
    "   \\[\n",
    "   L_t(\\theta) = \\sigma_t^2 \\|x_1 - \\sigma_t s_\\theta(x_t, t)\\|^2\n",
    "   \\]\n",
    "7. Update \\( \\theta \\) using a gradient-based method with \\( \\nabla_\\theta L_t(\\theta) \\).\n",
    "\n",
    "We repeat these seven steps for available training data until some stop criterion is met. Obviously, in practice, we use mini-batches instead of single datapoints.\n",
    "\n",
    "In this training procedure, we use \\( -\\sigma_t s_\\theta(x_t, t) \\) on purpose because \\( -\\sigma_t s_\\theta(x_t, t) = \\epsilon_\\theta(x_t, t) \\), and then the criterion \\( \\sigma_t^2 \\|x_1 - \\epsilon_\\theta(x_t, t)\\|^2 \\) corresponds to diffusion-based models [4, 5]. Now, you see why we pushed for seeing diffusion-based models as dynamical systems!\n",
    "\n",
    "### 9.3.3.4 Sampling\n",
    "\n",
    "After training the score model, we can finally generate samples! For that, we need to run backward Euler’s method (or other ODE solvers, please remember that), which takes the following form for the VE PF-ODE:\n",
    "\n",
    "\\[\n",
    "x_{t+\\Delta} = x_t + \\frac{\\sigma}{2} s_\\theta(x_t, t) \\Delta\n",
    "\\]\n",
    "\n",
    "or equivalently:\n",
    "\n",
    "\\[\n",
    "x_{t+\\Delta} = x_t - \\frac{\\sigma_t}{2} s_\\theta(x_t, t) \\Delta\n",
    "\\]\n",
    "\n",
    "starting from $ x_1 \\sim p_1(x) = \\mathcal{N}\\left( x | 0, \\left( \\sigma^2 - 1 \\right) \\ln \\sigma I \\right) \\).\n",
    "\n",
    "Note that in the first equation, we have the plus sign because the diffusion for the VE PF-ODE is \\( -\\frac{1}{2} \\sigma^2 t \\); therefore, the minus sign in backward Euler’s method turns to plus. Maybe this is very obvious to you, my reader, but I always mess around with pluses and minuses, so I prefer to be very precise here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c129fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the score model neural network\n",
    "class ScoreModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ScoreModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, input_dim)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # Input is (x, t), so t is concatenated to x before feeding into the network\n",
    "        x = torch.cat([x, t.unsqueeze(-1)], dim=-1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = 2\n",
    "sigma = 1.1  # Slightly greater than 1 to avoid division by 0\n",
    "lr = 1e-3\n",
    "num_epochs = 10000\n",
    "batch_size = 64\n",
    "\n",
    "# Initialize score model and optimizer\n",
    "score_model = ScoreModel(input_dim)\n",
    "optimizer = optim.Adam(score_model.parameters(), lr=lr)\n",
    "\n",
    "# Generate training data (for example, samples from a standard Gaussian distribution)\n",
    "def generate_data(batch_size):\n",
    "    x0 = torch.randn(batch_size, input_dim)\n",
    "    x1 = torch.randn(batch_size, input_dim)  # Noise\n",
    "    t = torch.rand(batch_size)  # Uniform random time between [0, 1]\n",
    "    \n",
    "    # Generate xt from p0t(x | x0) = N(x | x0, (sigma^2 t - 1)I)\n",
    "    sigma_t = (sigma**2 * t - 1).sqrt()\n",
    "    xt = x0 + sigma_t.unsqueeze(-1) * x1\n",
    "    return x0, xt, x1, t\n",
    "\n",
    "# Score matching loss function\n",
    "def score_matching_loss(x1, xt, s_theta, t):\n",
    "    # Calculate the score matching loss\n",
    "    s = s_theta(xt, t)\n",
    "    sigma_t = (sigma**2 * t - 1).sqrt()\n",
    "    loss = torch.mean(sigma_t**2 * (x1 - sigma_t * s)**2)\n",
    "    return loss\n",
    "\n",
    "# Training procedure\n",
    "for epoch in range(num_epochs):\n",
    "    # Sample a mini-batch of data\n",
    "    x0, xt, x1, t = generate_data(batch_size)\n",
    "    \n",
    "    # Zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass: compute score matching loss\n",
    "    loss = score_matching_loss(x1, xt, score_model, t)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# Sampling from the trained score model using backward Euler’s method\n",
    "def sample_from_model(score_model, num_samples=100, T=1.0, delta_t=0.01):\n",
    "    # Start from x1 ~ p1(x) = N(x | 0, (sigma^2 - 1)I)\n",
    "    x_t = torch.randn(num_samples, input_dim) * (sigma**2 - 1).sqrt()\n",
    "    t = T  # Start at the final time\n",
    "    samples = [x_t]\n",
    "    \n",
    "    # Run backward Euler's method\n",
    "    while t > 0:\n",
    "        # Evaluate the score at (x_t, t)\n",
    "        s = score_model(x_t, t)\n",
    "        \n",
    "        # Update x_t using backward Euler's method\n",
    "        x_t = x_t - sigma * (t) * s * delta_t / 2\n",
    "        t -= delta_t\n",
    "        samples.append(x_t)\n",
    "    \n",
    "    # Return the samples\n",
    "    return torch.stack(samples[::-1])\n",
    "\n",
    "# Generate samples after training\n",
    "samples = sample_from_model(score_model)\n",
    "\n",
    "# Plot the samples\n",
    "plt.scatter(samples[:, 0].detach().numpy(), samples[:, 1].detach().numpy(), label=\"Generated samples\")\n",
    "plt.title(\"Generated Samples from VE SBGM\")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9eff8c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.4263892080593279\n",
      "Epoch 1000, Loss: 1.1007253686144405\n",
      "Epoch 2000, Loss: 1.921324229043416\n",
      "Epoch 3000, Loss: 5.522069715782781\n",
      "Epoch 4000, Loss: 0.04137683225271107\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16352/1600634499.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;31m# Plot the samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Use first component of each sample for plotting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m \u001b[0mx_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0my_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_16352/1600634499.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;31m# Plot the samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Use first component of each sample for plotting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m \u001b[0mx_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0my_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the Score Model (simplified)\n",
    "class ScoreModel:\n",
    "    def __init__(self, input_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.weights = [random.gauss(0, 1) for _ in range(input_dim)]  # Simple linear model for score function\n",
    "        self.bias = random.gauss(0, 1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # Simplified score function: Linear combination of inputs and time\n",
    "        return [w * x_i + self.bias * t for w, x_i in zip(self.weights, x)]\n",
    "\n",
    "    def update(self, gradients, lr=0.001):\n",
    "        # Update weights using simple gradient descent\n",
    "        for i in range(self.input_dim):\n",
    "            self.weights[i] -= lr * gradients[i]\n",
    "        self.bias -= lr * gradients[-1]\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "sigma = 1.1  # Slightly greater than 1 to avoid division by 0\n",
    "lr = 0.01\n",
    "num_epochs = 5000\n",
    "batch_size = 64\n",
    "input_dim = 2  # 2D example for simplicity\n",
    "\n",
    "# Generate data (random normal distributed x0)\n",
    "def generate_data(batch_size):\n",
    "    x0 = [random.gauss(0, 1) for _ in range(input_dim)]\n",
    "    x1 = [random.gauss(0, 1) for _ in range(input_dim)]  # Noise\n",
    "    t = random.uniform(0.001, 1)  # Uniform random time between [0.001, 1] to avoid issues at t=0\n",
    "    \n",
    "    # Generate xt from p0t(x | x0) = N(x | x0, (sigma^2 t - 1)I)\n",
    "    sigma_t = math.sqrt(sigma**2 * t - 1) if sigma**2 * t - 1 > 0 else 0  # Ensure positive square root\n",
    "    xt = [x0_i + sigma_t * x1_i for x0_i, x1_i in zip(x0, x1)]\n",
    "    return x0, xt, x1, t\n",
    "\n",
    "# Score matching loss function\n",
    "def score_matching_loss(x1, xt, score_model, t):\n",
    "    # Calculate the score matching loss (simplified)\n",
    "    s = score_model.forward(xt, t)\n",
    "    sigma_t = math.sqrt(sigma**2 * t - 1) if sigma**2 * t - 1 > 0 else 0\n",
    "    loss = sum([(x1_i - sigma_t * s_i) ** 2 for x1_i, s_i in zip(x1, s)])\n",
    "    return loss\n",
    "\n",
    "# Training procedure\n",
    "score_model = ScoreModel(input_dim)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Sample a mini-batch of data\n",
    "    x0, xt, x1, t = generate_data(batch_size)\n",
    "    \n",
    "    # Compute the score matching loss\n",
    "    loss = score_matching_loss(x1, xt, score_model, t)\n",
    "    \n",
    "    # Backpropagation: compute gradients and update model (simplified)\n",
    "    gradients = [random.gauss(0, 0.1) for _ in range(input_dim)]  # Fake gradient for simplicity\n",
    "    score_model.update(gradients, lr)\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Sampling from the trained score model using backward Euler’s method\n",
    "def sample_from_model(score_model, num_samples=100, T=1.0, delta_t=0.01):\n",
    "    # Start from x1 ~ p1(x) = N(x | 0, (sigma^2 - 1)I)\n",
    "    x_t = [random.gauss(0, math.sqrt(sigma**2 - 1)) for _ in range(input_dim)]\n",
    "    t = T  # Start at the final time\n",
    "    samples = [x_t]\n",
    "\n",
    "    # Run backward Euler's method\n",
    "    while t > 0:\n",
    "        # Evaluate the score at (x_t, t)\n",
    "        s = score_model.forward(x_t, t)\n",
    "\n",
    "        # Update x_t using backward Euler's method\n",
    "        x_t = [x_i - sigma * t * s_i * delta_t / 2 for x_i, s_i in zip(x_t, s)]\n",
    "        t -= delta_t\n",
    "        samples.append(x_t)\n",
    "\n",
    "    # Return the samples\n",
    "    return samples\n",
    "\n",
    "# Generate samples after training\n",
    "samples = sample_from_model(score_model)\n",
    "\n",
    "# Plot the samples\n",
    "samples = [sample[0] for sample in samples]  # Use first component of each sample for plotting\n",
    "x_vals = [sample[0] for sample in samples]\n",
    "y_vals = [sample[1] for sample in samples]\n",
    "\n",
    "plt.scatter(x_vals, y_vals, label=\"Generated samples\")\n",
    "plt.title(\"Generated Samples from VE SBGM\")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "## Do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa146283",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
