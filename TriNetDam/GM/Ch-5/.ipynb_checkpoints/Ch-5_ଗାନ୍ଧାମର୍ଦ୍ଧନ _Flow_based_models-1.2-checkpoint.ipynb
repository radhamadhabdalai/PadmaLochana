{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8c9ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright (c) 2004 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41004f96",
   "metadata": {},
   "source": [
    "## Reparameterization Trick\n",
    "\n",
    "So far, we played around with the log-likelihood, and we ended up with the ELBO. However, there is still a problem with calculating the expected value, because it contains an integral! Therefore, the question is how we can calculate it and why it is better than the MC approximation of the log-likelihood without the variational posterior. In fact, we will use the MC approximation, but now, instead of sampling from the prior $ p(z) $, we will sample from the variational posterior $ q_\\phi(z|x) $.\n",
    "\n",
    "Is it better? Yes, because the variational posterior typically assigns more probability mass to a smaller region than the prior. If you examine the variance of the variational posterior, you will probably notice that the variational posteriors are almost deterministic (whether it is good or bad is an open question). As a result, we should get a better approximation!\n",
    "\n",
    "However, there is still an issue with the variance of the approximation. If we sample $ z $ from $ q_\\phi(z|x) $, plug them into the ELBO, and calculate gradients with respect to the parameters of a neural network $ \\phi $, the variance of the gradient may still be pretty large! A possible solution to that, first noticed by statisticians (e.g., see [8]), is the approach of reparameterizing the distribution.\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Fig.3 An example of reparameterizing a Gaussian distribution: We scale .ϵ distributed according to the standard Gaussian by .σ, and shift it by .μ.\n",
    "\n",
    "### Reparameterization Trick\n",
    "\n",
    "The idea is to express a random variable as a composition of primitive transformations (e.g., arithmetic operations, logarithm, etc.) of an independent random variable with a simple distribution. For example, if we consider a Gaussian random variable $ z $ with a mean $ \\mu $ and variance $ \\sigma^2 $, and an independent random variable $ \\epsilon \\sim N(\\epsilon | 0, 1) $, then the following holds:\n",
    "\n",
    "$$\n",
    "z = \\mu + \\sigma \\cdot \\epsilon\n",
    "$$\n",
    "\n",
    "Now, if we sample $ \\epsilon $ from the standard Gaussian, and apply the above transformation, then we get a sample from $ N(z | \\mu, \\sigma) $. This idea can be generalized to other distributions as well.\n",
    "\n",
    "The reparameterization trick can be used in the encoder $ q_\\phi(z|x) $. By using this trick, we can drastically reduce the variance of the gradient because the randomness comes from the independent source $ p(\\epsilon) $, and we calculate the gradient with respect to a deterministic function (i.e., a neural network), not random variables.\n",
    "\n",
    "### Application in VAEs\n",
    "\n",
    "In the VAE framework, we will apply the reparameterization trick to the latent variable $ z $. The encoder $ q_\\phi(z|x) $ outputs two values: the mean $ \\mu_\\phi(x) $ and log-variance $ \\log \\sigma^2_\\phi(x) $ for each input $ x $. Instead of sampling directly from $ q_\\phi(z|x) $, we sample an independent Gaussian $ \\epsilon \\sim N(\\epsilon | 0, 1) $ and apply the following transformation:\n",
    "\n",
    "$$\n",
    "z_\\phi = \\mu_\\phi(x) + \\sigma_\\phi(x) \\cdot \\epsilon\n",
    "$$\n",
    "\n",
    "Here, $ \\mu_\\phi(x) $ and $ \\sigma_\\phi(x) $ are outputs of the encoder neural network. This ensures that we can backpropagate through the reparameterization, reducing the variance of the gradient estimates.\n",
    "\n",
    "### Training the VAE\n",
    "\n",
    "The training objective for the VAE is to minimize the **negative ELBO**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\phi, \\theta; x) = - \\mathbb{E}_{q_\\phi(z|x)} \\left[\\ln p_\\theta(x|z)\\right] + \\text{KL}\\left(q_\\phi(z|x) || p(z)\\right)\n",
    "$$\n",
    "\n",
    "In practice, we approximate the expectation $ \\mathbb{E}_{q_\\phi(z|x)} $ by taking a single sample from the variational posterior:\n",
    "\n",
    "$$\n",
    "z_\\phi = \\mu_\\phi(x) + \\sigma_\\phi(x) \\cdot \\epsilon\n",
    "$$\n",
    "\n",
    "where $ \\epsilon \\sim N(0, I) $. This allows the model to be trained using stochastic gradient descent, where we only need a single sample of $ z $ during each training iteration.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- The reparameterization trick is used to express a random variable $ z $ as a deterministic transformation of an independent random variable $ \\epsilon $, making the gradient computation more stable.\n",
    "- For VAEs, we apply the reparameterization trick to sample $ z $ from the variational posterior $ q_\\phi(z|x) $.\n",
    "- The training objective is the negative ELBO, and the reparameterization trick allows us to sample $ z $ efficiently and compute gradients for optimization. \n",
    "\n",
    "##  VAE Code Implementation\n",
    "\n",
    "### Encoder Class\n",
    "\n",
    "The encoder class takes in an input $ x $, passes it through the encoder network, and generates the mean $ \\mu_\\phi(x) $ and log-variance $ \\log \\sigma_\\phi^2(x) $ of the variational posterior $ q_\\phi(z|x) $. We then use the **reparameterization trick** to sample from this posterior.\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, encoder_net):\n",
    "        super(Encoder, self).__init__()\n",
    "        # The init of the encoder network.\n",
    "        self.encoder = encoder_net\n",
    "\n",
    "    # Reparameterization trick for Gaussians.\n",
    "    @staticmethod\n",
    "    def reparameterization(mu, log_var):\n",
    "        # The formula is the following:\n",
    "        # z = mu + std * epsilon\n",
    "        # epsilon ~ Normal(0, 1)\n",
    "        \n",
    "        # Get the standard deviation from the log-variance.\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        \n",
    "        # Sample epsilon from Normal(0, 1).\n",
    "        eps = torch.randn_like(std)\n",
    "        \n",
    "        # Final output\n",
    "        return mu + std * eps\n",
    "\n",
    "    # Output of the encoder network (mean and log-variance).\n",
    "    def encode(self, x):\n",
    "        # Calculate the output of the encoder network of size 2M.\n",
    "        h_e = self.encoder(x)\n",
    "        \n",
    "        # Split the output into the mean and log-variance.\n",
    "        mu_e, log_var_e = torch.chunk(h_e, 2, dim=1)\n",
    "        \n",
    "        return mu_e, log_var_e\n",
    "\n",
    "    # Sampling procedure using reparameterization.\n",
    "    def sample(self, x=None, mu_e=None, log_var_e=None):\n",
    "        if mu_e is None and log_var_e is None:\n",
    "            # Calculate mean and log-variance from the encoder.\n",
    "            mu_e, log_var_e = self.encode(x)\n",
    "        \n",
    "        # Apply the reparameterization trick.\n",
    "        return self.reparameterization(mu_e, log_var_e)\n",
    "\n",
    "    # Log-probability of the sample used for ELBO calculation.\n",
    "    def log_prob(self, x=None, mu_e=None, log_var_e=None, z=None):\n",
    "        if x is not None:\n",
    "            # Calculate corresponding sample if only x is provided.\n",
    "            mu_e, log_var_e = self.encode(x)\n",
    "            z = self.sample(mu_e=mu_e, log_var_e=log_var_e)\n",
    "        \n",
    "        # Return the log-normal distribution.\n",
    "        return log_normal_diag(z, mu_e, log_var_e)\n",
    "\n",
    "    # Forward pass: either log-probability or sampling.\n",
    "    def forward(self, x, type='log_prob'):\n",
    "        assert type in ['encode', 'log_prob'], 'Type could be either encode or log_prob'\n",
    "        if type == 'log_prob':\n",
    "            return self.log_prob(x)\n",
    "        else:\n",
    "            return self.sample(x)\n",
    "## 5.3.5 VAE Code Implementation\n",
    "\n",
    "### Encoder Class\n",
    "\n",
    "The encoder class takes in an input $ x $, passes it through the encoder network, and generates the mean $ \\mu_\\phi(x) $ and log-variance $ \\log \\sigma_\\phi^2(x) $ of the variational posterior $ q_\\phi(z|x) $. We then use the **reparameterization trick** to sample from this posterior.\n",
    "\n",
    "```python\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, encoder_net):\n",
    "        super(Encoder, self).__init__()\n",
    "        # The init of the encoder network.\n",
    "        self.encoder = encoder_net\n",
    "\n",
    "    # Reparameterization trick for Gaussians.\n",
    "    @staticmethod\n",
    "    def reparameterization(mu, log_var):\n",
    "        # The formula is the following:\n",
    "        # z = mu + std * epsilon\n",
    "        # epsilon ~ Normal(0, 1)\n",
    "        \n",
    "        # Get the standard deviation from the log-variance.\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        \n",
    "        # Sample epsilon from Normal(0, 1).\n",
    "        eps = torch.randn_like(std)\n",
    "        \n",
    "        # Final output\n",
    "        return mu + std * eps\n",
    "\n",
    "    # Output of the encoder network (mean and log-variance).\n",
    "    def encode(self, x):\n",
    "        # Calculate the output of the encoder network of size 2M.\n",
    "        h_e = self.encoder(x)\n",
    "        \n",
    "        # Split the output into the mean and log-variance.\n",
    "        mu_e, log_var_e = torch.chunk(h_e, 2, dim=1)\n",
    "        \n",
    "        return mu_e, log_var_e\n",
    "\n",
    "    # Sampling procedure using reparameterization.\n",
    "    def sample(self, x=None, mu_e=None, log_var_e=None):\n",
    "        if mu_e is None and log_var_e is None:\n",
    "            # Calculate mean and log-variance from the encoder.\n",
    "            mu_e, log_var_e = self.encode(x)\n",
    "        \n",
    "        # Apply the reparameterization trick.\n",
    "        return self.reparameterization(mu_e, log_var_e)\n",
    "\n",
    "    # Log-probability of the sample used for ELBO calculation.\n",
    "    def log_prob(self, x=None, mu_e=None, log_var_e=None, z=None):\n",
    "        if x is not None:\n",
    "            # Calculate corresponding sample if only x is provided.\n",
    "            mu_e, log_var_e = self.encode(x)\n",
    "            z = self.sample(mu_e=mu_e, log_var_e=log_var_e)\n",
    "        \n",
    "        # Return the log-normal distribution.\n",
    "        return log_normal_diag(z, mu_e, log_var_e)\n",
    "\n",
    "    # Forward pass: either log-probability or sampling.\n",
    "    def forward(self, x, type='log_prob'):\n",
    "        assert type in ['encode', 'log_prob'], 'Type could be either encode or log_prob'\n",
    "        if type == 'log_prob':\n",
    "            return self.log_prob(x)\n",
    "        else:\n",
    "            return self.sample(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, decoder_net, distribution='categorical', num_vals=None):\n",
    "        super(Decoder, self).__init__()\n",
    "        # The decoder network.\n",
    "        self.decoder = decoder_net\n",
    "        \n",
    "        # The distribution used for the decoder.\n",
    "        self.distribution = distribution\n",
    "        \n",
    "        # The number of possible values (used for categorical distribution).\n",
    "        self.num_vals = num_vals\n",
    "\n",
    "    # This function calculates parameters of the likelihood function p(x|z).\n",
    "    def decode(self, z):\n",
    "        # Apply the decoder network.\n",
    "        h_d = self.decoder(z)\n",
    "        \n",
    "        if self.distribution == 'categorical':\n",
    "            # Reshape to (Batch size, Dimensionality, Number of Values).\n",
    "            b = h_d.shape[0]\n",
    "            d = h_d.shape[1] // self.num_vals\n",
    "            h_d = h_d.view(b, d, self.num_vals)\n",
    "            \n",
    "            # Apply softmax to get probabilities.\n",
    "            mu_d = torch.softmax(h_d, 2)\n",
    "            return [mu_d]\n",
    "        \n",
    "        elif self.distribution == 'bernoulli':\n",
    "            # Apply sigmoid for Bernoulli distribution.\n",
    "            mu_d = torch.sigmoid(h_d)\n",
    "            return [mu_d]\n",
    "        \n",
    "        else:\n",
    "            raise ValueError('Only: \"categorical\", \"bernoulli\"')\n",
    "\n",
    "    # Sampling from the decoder.\n",
    "    def sample(self, z):\n",
    "        outs = self.decode(z)\n",
    "        \n",
    "        if self.distribution == 'categorical':\n",
    "            # Use the output of the decoder.\n",
    "            mu_d = outs[0]\n",
    "            return mu_d\n",
    "\n",
    "### Summary:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695a31a7",
   "metadata": {},
   "source": [
    "- **Encoder**: Takes $ x $, computes the mean and log-variance, and applies the reparameterization trick to sample $ z $.\n",
    "- **Decoder**: Takes $ z $ and produces the parameters of the likelihood function, using a categorical or Bernoulli distribution.\n",
    "- **VAE Loss**: The ELBO is computed and used to train the model, where we sample from the variational posterior $ q_\\phi(z|x) $ using the reparameterization trick and maximize the likelihood of the observed data while regularizing the latent space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dca35c1",
   "metadata": {},
   "source": [
    "##  VAE Implementation\n",
    "\n",
    "### Prior Class\n",
    "\n",
    "In the current implementation, the prior is a simple standard Gaussian distribution. Although we could use a built-in PyTorch distribution, we chose not to for two reasons:\n",
    "1. It is important to think of the prior as a crucial component in VAEs.\n",
    "2. We can implement a learnable prior (e.g., a flow-based prior, VampPrior, a mixture of distributions).\n",
    "\n",
    "```python\n",
    "class Prior(nn.Module):\n",
    "    def __init__(self, L):\n",
    "        super(Prior, self).__init__()\n",
    "        self.L = L\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        # Sample from a standard Gaussian distribution.\n",
    "        z = torch.randn((batch_size, self.L))\n",
    "        return z\n",
    "\n",
    "    def log_prob(self, z):\n",
    "        # Log-probability of z from a standard normal distribution.\n",
    "        return log_standard_normal(z)\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, encoder_net, decoder_net, num_vals=256, L=16, likelihood_type='categorical'):\n",
    "        super(VAE, self).__init__()\n",
    "        print('VAE by JT.')\n",
    "\n",
    "        self.encoder = Encoder(encoder_net=encoder_net)\n",
    "        self.decoder = Decoder(distribution=likelihood_type, decoder_net=decoder_net, num_vals=num_vals)\n",
    "        self.prior = Prior(L=L)\n",
    "\n",
    "        self.num_vals = num_vals\n",
    "        self.likelihood_type = likelihood_type\n",
    "\n",
    "    def forward(self, x, reduction='avg'):\n",
    "        # Encoder\n",
    "        mu_e, log_var_e = self.encoder.encode(x)\n",
    "        z = self.encoder.sample(mu_e=mu_e, log_var_e=log_var_e)\n",
    "\n",
    "        # ELBO Calculation\n",
    "        RE = self.decoder.log_prob(x, z)\n",
    "        KL = (self.prior.log_prob(z) - self.encoder.log_prob(mu_e=mu_e, log_var_e=log_var_e, z=z)).sum(-1)\n",
    "\n",
    "        if reduction == 'sum':\n",
    "            return -(RE + KL).sum()\n",
    "        else:\n",
    "            return -(RE + KL).mean()\n",
    "\n",
    "    def sample(self, batch_size=64):\n",
    "        z = self.prior.sample(batch_size=batch_size)\n",
    "        return self.decoder.sample(z)\n",
    "# Example of Encoder Network\n",
    "encoder = nn.Sequential(\n",
    "    nn.Linear(D, M), nn.LeakyReLU(),\n",
    "    nn.Linear(M, M), nn.LeakyReLU(),\n",
    "    nn.Linear(M, 2 * L)  # Output mean and log-variance\n",
    ")\n",
    "\n",
    "# Example of Decoder Network\n",
    "decoder = nn.Sequential(\n",
    "    nn.Linear(L, M), nn.LeakyReLU(),\n",
    "    nn.Linear(M, M), nn.LeakyReLU(),\n",
    "    nn.Linear(M, num_vals * D)  # Output the probability distribution\n",
    ")class VAE(nn.Module):\n",
    "    def __init__(self, encoder_net, decoder_net, num_vals=256, L=16, likelihood_type='categorical'):\n",
    "        super(VAE, self).__init__()\n",
    "        print('VAE by JT.')\n",
    "\n",
    "        # Encoder, Decoder, and Prior initialization\n",
    "        self.encoder = Encoder(encoder_net=encoder_net)\n",
    "        self.decoder = Decoder(distribution=likelihood_type, decoder_net=decoder_net, num_vals=num_vals)\n",
    "        self.prior = Prior(L=L)\n",
    "\n",
    "        self.num_vals = num_vals\n",
    "        self.likelihood_type = likelihood_type\n",
    "\n",
    "    def forward(self, x, reduction='avg'):\n",
    "        # 1. Encoder: Calculate the mean (mu_e) and log variance (log_var_e) from the encoder\n",
    "        mu_e, log_var_e = self.encoder.encode(x)\n",
    "        \n",
    "        # 2. Sample the latent variable z using the reparameterization trick\n",
    "        z = self.encoder.sample(mu_e=mu_e, log_var_e=log_var_e)\n",
    "\n",
    "        # 3. ELBO Calculation\n",
    "        #   - RE (Reconstruction Error) is the negative log-likelihood of the data under the decoder\n",
    "        RE = self.decoder.log_prob(x, z)\n",
    "        \n",
    "        #   - KL Divergence between the posterior and the prior\n",
    "        KL = (self.prior.log_prob(z) - self.encoder.log_prob(mu_e=mu_e, log_var_e=log_var_e, z=z)).sum(-1)\n",
    "\n",
    "        # 4. Total Loss (Negative ELBO)\n",
    "        if reduction == 'sum':\n",
    "            total_loss = -(RE + KL).sum()\n",
    "        else:\n",
    "            total_loss = -(RE + KL).mean()\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def sample(self, batch_size=64):\n",
    "        # Sampling from the prior and passing through the decoder\n",
    "        z = self.prior.sample(batch_size=batch_size)\n",
    "        return self.decoder.sample(z)\n",
    "\n"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    },
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "8630772b",
   "metadata": {},
   "source": [
    "##  Typical Issues with VAEs\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Fig.4 An example of outcomes after the training: (a) Randomly selected real images. (b) Unconditional generations from the VAE. (c) The validation curve during training.\n",
    "\n",
    "\n",
    "VAEs constitute a very powerful class of models, mainly due to their flexibility. Unlike flow-based models, they do not require the invertibility of neural networks; thus, we can use any arbitrary architecture for encoders and decoders. In contrast to ARMs, they learn a low-dimensional data representation, and we can control the bottleneck (i.e., the dimensionality of the latent space). However, they also suffer from several issues. Except for the ones mentioned before (i.e., a necessity of an efficient integral estimation, a gap between the ELBO and the log-likelihood function for too simplistic variational posteriors), the potential problems are the following:\n",
    "\n",
    "- **Posterior Collapse**: \n",
    "  Let us take a look at the ELBO and the regularization term. For a non-trainable prior like the standard Gaussian, the regularization term will be minimized if: \n",
    "  $$ \\forall x, q_\\phi(z|x) = p(z) $$ \n",
    "  This may happen if the decoder is so powerful that it treats $z$ as noise, e.g., when a decoder is expressed by an AR model [10]. This issue is known as **posterior collapse** [11].\n",
    "\n",
    "- **Hole Problem**:\n",
    "  Another issue is associated with a mismatch between the aggregated posterior, \n",
    "  $$ 1/N \\sum_{n=1}^{N} q_\\phi(z|x_n), $$ \n",
    "  and the prior $p(z)$. Imagine that we have the standard Gaussian prior and the aggregated posterior (i.e., an average of variational posteriors over all training data). As a result, there are regions where the prior assigns a high probability, but the aggregated posterior assigns a low probability, or vice versa. Then, sampling from these \"holes\" provides unrealistic latent values, and the decoder produces images of very low quality. This problem is referred to as the **hole problem** [12].\n",
    "\n",
    "- **Out-of-Distribution Problem**:\n",
    "  The last problem we want to discuss is more general and affects all deep generative models. As it was noticed in [13], deep generative models (including VAEs) fail to properly detect out-of-distribution examples. Out-of-distribution datapoints are examples that follow a totally different distribution than the one a model was trained on. For instance, let us assume that our model is trained on MNIST, and then FashionMNIST examples are out-of-distribution. Intuition tells us that a properly trained deep generative model should assign a high probability to in-distribution examples and a low probability to out-of-distribution points. Unfortunately, as shown in [13], this is not the case. The **out-of-distribution problem** remains one of the main unsolved problems in deep generative modeling [14].\n",
    "\n",
    "##  There Is More!\n",
    "\n",
    "There are a plethora of papers that extend VAEs and apply them to many problems. Below, we will list out selected papers and only touch upon the vast literature on the topic!\n",
    "\n",
    "### Estimation of the Log-Likelihood Using Importance Weighting\n",
    "As we indicated multiple times, the ELBO is the lower bound to the log-likelihood, and it rather should not be used as a good estimate of the log-likelihood. In [7, 15], an importance weighting procedure is advocated to better approximate the log-likelihood, namely:\n",
    "$$ \\ln p(x) \\approx \\frac{1}{K} \\sum_{k=1}^{K} \\ln \\frac{p(x, z_k)}{q_\\phi(z_k|x)} $$ \n",
    "where $z_k \\sim q_\\phi(z_k|x)$. Notice that the logarithm is outside the expected value. As shown in [15], using importance weighting with sufficiently large $K$ gives a good estimate of the log-likelihood. In practice, $K$ is taken to be 512 or more if the computational budget allows.\n",
    "\n",
    "### Enhancing VAEs\n",
    "\n",
    "- **Better Encoders**: After introducing the idea of VAEs, many papers focused on proposing a flexible family of variational posteriors. The most prominent direction is based on utilizing conditional flow-based models [16–21].\n",
    "\n",
    "- **Better Decoders**: VAEs allow using any neural network to parameterize the decoder. Therefore, we can use fully connected networks, fully convolutional networks, ResNets, or ARMs. For instance, in [22], a PixelCNN-based decoder was used in a VAE.\n",
    "\n",
    "- **Better Priors**: If there is a big mismatch between the aggregated posterior and the prior, it can be a serious issue. To alleviate this, many papers use multimodal priors, such as the **VampPrior** [23], a flow-based prior [24, 25], an ARM-based prior [26], or using resampling ideas [27].\n",
    "\n",
    "### Extending VAEs\n",
    "\n",
    "- **Semi-Supervised VAEs**: In [28], a semi-supervised VAE was proposed, which was further extended to the concept of **fair representations** [29, 30].\n",
    "\n",
    "- **VAEs for Non-Image Data**: Although VAEs have mostly been used for image data, they can also be applied to other domains, such as sequential data (e.g., text) [11] or molecular graph generation [32].\n",
    "\n",
    "- **Hierarchical VAEs**: Recently, many VAEs with a deep, hierarchical structure of latent variables have been proposed, achieving remarkable results. Notable models include **BIVA** [45], **NVAE** [46], and very deep VAEs [47].\n",
    "\n",
    "- **Adversarial Auto-Encoders**: An interesting perspective on VAEs is presented in [48], where the prior is trained with an adversarial loss, allowing the model to benefit from adversarial learning.\n",
    "\n",
    "- **Adversarial Attacks**: VAEs are known to be susceptible to adversarial attacks. A possible remedy for this is to apply **MCMC** techniques at the inference time [50].\n",
    "## Improving Variational Auto-encoders\n",
    "\n",
    "###  Priors\n",
    "\n",
    "####  Insights from Rewriting the ELBO\n",
    "\n",
    "One of the crucial components of VAEs is the marginal distribution over $ z $'s. Now, we will take a closer look at this distribution, also called the prior. Before we start thinking about improving it, we inspect the ELBO one more time. We can write the ELBO as follows:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{x \\sim p_{\\text{data}}(x)}[\\ln p(x)] \\geq \\mathbb{E}_{x \\sim p_{\\text{data}}(x)} \\mathbb{E}_{q_\\phi(z|x)} \\left[ \\ln p_\\theta(x|z) + \\ln p_\\lambda(z) - \\ln q_\\phi(z|x) \\right]\n",
    "$$\n",
    "\n",
    "where we explicitly highlight the summation over training data, namely, the expected value with respect to $ x $'s from the empirical distribution $ p_{\\text{data}}(x) = \\frac{1}{N} \\sum_{n=1}^{N} \\delta(x - x_n) $, and $ \\delta(\\cdot) $ is the Dirac delta. The ELBO consists of two parts:\n",
    "\n",
    "- The **reconstruction error** $ \\Delta_{\\text{RE}} $:\n",
    "$$\n",
    "\\text{RE} = \\mathbb{E}_{x \\sim p_{\\text{data}}(x)} \\mathbb{E}_{q_\\phi(z|x)}[\\ln p_\\theta(x|z)]\n",
    "$$\n",
    "\n",
    "- The **regularization term** between the encoder and the prior $ \\Delta_{\\Omega} $:\n",
    "$$\n",
    "\\Omega = \\mathbb{E}_{x \\sim p_{\\text{data}}(x)} \\mathbb{E}_{q_\\phi(z|x)}[\\ln p_\\lambda(z) - \\ln q_\\phi(z|x)]\n",
    "$$\n",
    "\n",
    "Further, let us play a little bit with the regularization term $ \\Omega $:\n",
    "\n",
    "$$\n",
    "\\Omega = \\mathbb{E}_{x \\sim p_{\\text{data}}(x)} \\mathbb{E}_{q_\\phi(z|x)}[\\ln p_\\lambda(z) - \\ln q_\\phi(z|x)]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\int \\int \\ln p_\\lambda(z) - \\ln q_\\phi(z|x) \\, dz \\, dx\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sum_{n=1}^{N} \\int \\delta(x - x_n) q_\\phi(z|x) \\ln p_\\lambda(z) - \\ln q_\\phi(z|x) \\, dz \\, dx\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{N} \\sum_{n=1}^{N} \\int q_\\phi(z|x_n) \\left[\\ln p_\\lambda(z) - \\ln q_\\phi(z|x_n)\\right] \\, dz\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{N} \\sum_{n=1}^{N} \\left[ \\int q_\\phi(z|x_n) \\ln p_\\lambda(z) \\, dz - \\int q_\\phi(z|x_n) \\ln q_\\phi(z|x_n) \\, dz \\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= -\\frac{1}{N} \\sum_{n=1}^{N} \\left[ \\text{CE}(q_\\phi(z) || p_\\lambda(z)) + H(q_\\phi(z|x)) \\right]\n",
    "$$\n",
    "\n",
    "where we use the property of the Dirac delta: \n",
    "$$\n",
    "\\delta(a - a') f(a) da = f(a')\n",
    "$$\n",
    "and we use the notion of the **aggregated posterior** $ q(z) $ defined as:\n",
    "$$\n",
    "q(z) = \\frac{1}{N} \\sum_{n=1}^{N} q_\\phi(z|x_n)\n",
    "$$\n",
    "\n",
    "An example of the aggregated posterior is schematically depicted in Fig.5. Eventually, we obtain two terms:\n",
    "\n",
    "1. The first term, $ \\text{CE}(q_\\phi(z) || p_\\lambda(z)) $, is the **cross-entropy** between the aggregated posterior and the prior.\n",
    "2. The second term, $ H(q_\\phi(z|x)) $, is the **conditional entropy** of $ q_\\phi(z|x) $ with the empirical distribution $ p_{\\text{data}}(x) $.\n",
    "\n",
    "I highly recommend doing this derivation step by step, as it helps a lot in understanding what is going on here. Interestingly, there is another possibility to rewrite $ \\Omega $ using three terms, with the **total correlation** [51]. We will not use it here, so it is left as a \"homework.\" Anyway, one may ask, why is it useful to rewrite the ELBO? The answer is rather straightforward: We can analyze it from a different perspective! In this section, we will focus on the prior, an important component in the generative part that is very often neglected. Many Bayesianists argue that a prior should not be learned, but VAEs are not Bayesian models, so who says we cannot learn the prior? As we will see shortly, a non-learnable prior could be quite problematic, especially for the generation process.\n",
    "##  Improving Variational Auto-encoders\n",
    "\n",
    "###  Priors\n",
    "\n",
    "####  Insights from Rewriting the ELBO\n",
    "\n",
    "One of the crucial components of VAEs is the marginal distribution over $ z $'s. Now, we will take a closer look at this distribution, also called the prior. Before we start thinking about improving it, we inspect the ELBO one more time. We can write the ELBO as follows:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{x \\sim p_{\\text{data}}(x)}[\\ln p(x)] \\geq \\mathbb{E}_{x \\sim p_{\\text{data}}(x)} \\mathbb{E}_{q_\\phi(z|x)} \\left[ \\ln p_\\theta(x|z) + \\ln p_\\lambda(z) - \\ln q_\\phi(z|x) \\right]\n",
    "$$\n",
    "\n",
    "where we explicitly highlight the summation over training data, namely, the expected value with respect to $ x $'s from the empirical distribution $ p_{\\text{data}}(x) = \\frac{1}{N} \\sum_{n=1}^{N} \\delta(x - x_n) $, and $ \\delta(\\cdot) $ is the Dirac delta. The ELBO consists of two parts:\n",
    "\n",
    "- The **reconstruction error** $ \\Delta_{\\text{RE}} $:\n",
    "$$\n",
    "\\text{RE} = \\mathbb{E}_{x \\sim p_{\\text{data}}(x)} \\mathbb{E}_{q_\\phi(z|x)}[\\ln p_\\theta(x|z)]\n",
    "$$\n",
    "\n",
    "- The **regularization term** between the encoder and the prior $ \\Delta_{\\Omega} $:\n",
    "$$\n",
    "\\Omega = \\mathbb{E}_{x \\sim p_{\\text{data}}(x)} \\mathbb{E}_{q_\\phi(z|x)}[\\ln p_\\lambda(z) - \\ln q_\\phi(z|x)]\n",
    "$$\n",
    "\n",
    "Further, let us play a little bit with the regularization term $ \\Omega $:\n",
    "\n",
    "$$\n",
    "\\Omega = \\mathbb{E}_{x \\sim p_{\\text{data}}(x)} \\mathbb{E}_{q_\\phi(z|x)}[\\ln p_\\lambda(z) - \\ln q_\\phi(z|x)]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\int \\int \\ln p_\\lambda(z) - \\ln q_\\phi(z|x) \\, dz \\, dx\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sum_{n=1}^{N} \\int \\delta(x - x_n) q_\\phi(z|x) \\ln p_\\lambda(z) - \\ln q_\\phi(z|x) \\, dz \\, dx\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{N} \\sum_{n=1}^{N} \\int q_\\phi(z|x_n) \\left[\\ln p_\\lambda(z) - \\ln q_\\phi(z|x_n)\\right] \\, dz\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{N} \\sum_{n=1}^{N} \\left[ \\int q_\\phi(z|x_n) \\ln p_\\lambda(z) \\, dz - \\int q_\\phi(z|x_n) \\ln q_\\phi(z|x_n) \\, dz \\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= -\\frac{1}{N} \\sum_{n=1}^{N} \\left[ \\text{CE}(q_\\phi(z) || p_\\lambda(z)) + H(q_\\phi(z|x)) \\right]\n",
    "$$\n",
    "\n",
    "where we use the property of the Dirac delta: \n",
    "$$\n",
    "\\delta(a - a') f(a) da = f(a')\n",
    "$$\n",
    "and we use the notion of the **aggregated posterior** $ q(z) $ defined as:\n",
    "$$\n",
    "q(z) = \\frac{1}{N} \\sum_{n=1}^{N} q_\\phi(z|x_n)\n",
    "$$\n",
    "\n",
    "An example of the aggregated posterior is schematically depicted in Fig. 5.5. Eventually, we obtain two terms:\n",
    "\n",
    "1. The first term, $ \\text{CE}(q_\\phi(z) || p_\\lambda(z)) $, is the **cross-entropy** between the aggregated posterior and the prior.\n",
    "2. The second term, $ H(q_\\phi(z|x)) $, is the **conditional entropy** of $ q_\\phi(z|x) $ with the empirical distribution $ p_{\\text{data}}(x) $.\n",
    "\n",
    "I highly recommend doing this derivation step by step, as it helps a lot in understanding what is going on here. Interestingly, there is another possibility to rewrite $ \\Omega $ using three terms, with the **total correlation** [51]. We will not use it here, so it is left as a \"homework.\" Anyway, one may ask, why is it useful to rewrite the ELBO? The answer is rather straightforward: We can analyze it from a different perspective! In this section, we will focus on the prior, an important component in the generative part that is very often neglected. Many Bayesianists argue that a prior should not be learned, but VAEs are not Bayesian models, so who says we cannot learn the prior? As we will see shortly, a non-learnable prior could be quite problematic, especially for the generation process.\n",
    "\n",
    "\n",
    "\n",
    "![image-2.png](attachment:image-2.png)\n",
    "Fig.5 An example of the aggregated posterior. Individual points are encoded as Gaussians in the 2D latent space (magenta) and the mixture of variational posteriors (the aggregated posterior) is presented by contours.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2533b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal, MultivariateNormal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the encoder and decoder architectures (simplified for demonstration)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2_mean = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc2_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = torch.relu(self.fc1(x))\n",
    "        mean = self.fc2_mean(h)\n",
    "        logvar = self.fc2_logvar(h)\n",
    "        return mean, logvar\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = torch.relu(self.fc1(z))\n",
    "        output = torch.sigmoid(self.fc2(h))  # assuming binary data for simplicity\n",
    "        return output\n",
    "\n",
    "# Define the VAE model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(input_dim, hidden_dim, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim, input_dim)\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        return self.decoder(z), mean, logvar\n",
    "\n",
    "# Define the loss function (ELBO)\n",
    "def loss_function(recon_x, x, mean, logvar):\n",
    "    # Reconstruction term (RE)\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    \n",
    "    # Regularization term (Ω)\n",
    "    # Using a standard normal prior\n",
    "    p_lambda_z = Normal(torch.zeros_like(mean), torch.ones_like(mean))\n",
    "    q_phi_z = Normal(mean, torch.exp(0.5 * logvar))\n",
    "    \n",
    "    # Cross-entropy (CE) between the aggregated posterior and prior\n",
    "    # We use the KL divergence as a regularizer in VAEs\n",
    "    # The formula for the KL divergence is:\n",
    "    # D_KL(q(z|x) || p(z)) = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    # Where sigma is the std deviation and mu is the mean of the posterior q(z|x)\n",
    "    # In this case, we assume p(z) is a standard normal (mean 0, variance 1)\n",
    "    \n",
    "    # KL divergence term\n",
    "    KL_divergence = torch.sum(0.5 * (torch.exp(logvar) + mean**2 - 1 - logvar))\n",
    "    \n",
    "    # Total loss = Reconstruction + KL divergence\n",
    "    return BCE + KL_divergence\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = 784  # Example for MNIST (28x28)\n",
    "hidden_dim = 400\n",
    "latent_dim = 20\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "# Set up the model, optimizer\n",
    "model = VAE(input_dim, hidden_dim, latent_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Dummy data loader (Replace with actual dataset loading code)\n",
    "# For example, use MNIST data here\n",
    "# Assuming data is already flattened to (batch_size, 784) for MNIST\n",
    "# data_loader = ...\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(data_loader):  # Replace with actual data loader\n",
    "        data = data.view(-1, input_dim)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        recon_batch, mean, logvar = model(data)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_function(recon_batch, data, mean, logvar)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch}, Average Loss: {train_loss / len(data_loader.dataset)}\")\n",
    "\n",
    "# To visualize the aggregated posterior, we can plot the distribution\n",
    "# Example for 2D latent space visualization\n",
    "z_samples = mean.detach().cpu().numpy()\n",
    "plt.scatter(z_samples[:, 0], z_samples[:, 1], alpha=0.5)\n",
    "plt.title('Aggregated Posterior in 2D Latent Space')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0e8167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the architecture of the Encoder and Decoder networks\n",
    "def encoder(x, weights1, bias1, weights2, bias2, weights3, bias3):\n",
    "    \"\"\" Encoder Network: z_mean, z_logvar \"\"\"\n",
    "    h = np.tanh(np.dot(x, weights1) + bias1)  # First layer with tanh activation\n",
    "    z_mean = np.dot(h, weights2) + bias2     # Mean of the latent space\n",
    "    z_logvar = np.dot(h, weights3) + bias3   # Log variance of the latent space\n",
    "    return z_mean, z_logvar\n",
    "\n",
    "def decoder(z, weights1, bias1, weights2, bias2):\n",
    "    \"\"\" Decoder Network: Reconstruct the input data \"\"\"\n",
    "    h = np.tanh(np.dot(z, weights1) + bias1)  # First layer with tanh activation\n",
    "    x_reconstructed = np.dot(h, weights2) + bias2  # Output layer\n",
    "    return x_reconstructed\n",
    "\n",
    "def reparameterize(z_mean, z_logvar):\n",
    "    \"\"\" Reparameterization Trick to sample from q(z|x) \"\"\"\n",
    "    epsilon = np.random.randn(*z_mean.shape)  # Random noise from standard normal\n",
    "    z = z_mean + np.exp(0.5 * z_logvar) * epsilon  # Sample z\n",
    "    return z\n",
    "\n",
    "# Loss function (ELBO)\n",
    "def loss_function(x, x_reconstructed, z_mean, z_logvar):\n",
    "    \"\"\" Compute the ELBO loss: reconstruction error + KL divergence \"\"\"\n",
    "    # Reconstruction error (binary cross entropy)\n",
    "    recon_loss = np.sum((x - x_reconstructed) ** 2)\n",
    "\n",
    "    # KL divergence\n",
    "    kl_div = -0.5 * np.sum(1 + z_logvar - z_mean**2 - np.exp(z_logvar))\n",
    "\n",
    "    # Total loss\n",
    "    return recon_loss + kl_div\n",
    "\n",
    "# Initialize network weights and biases\n",
    "input_dim = 784  # Example: MNIST data (28x28 images flattened)\n",
    "hidden_dim = 400\n",
    "latent_dim = 20\n",
    "output_dim = input_dim\n",
    "\n",
    "# Encoder weights and biases\n",
    "weights1_enc = np.random.randn(input_dim, hidden_dim) * 0.01\n",
    "bias1_enc = np.zeros(hidden_dim)\n",
    "weights2_enc = np.random.randn(hidden_dim, latent_dim) * 0.01\n",
    "bias2_enc = np.zeros(latent_dim)\n",
    "weights3_enc = np.random.randn(hidden_dim, latent_dim) * 0.01\n",
    "bias3_enc = np.zeros(latent_dim)\n",
    "\n",
    "# Decoder weights and biases\n",
    "weights1_dec = np.random.randn(latent_dim, hidden_dim) * 0.01\n",
    "bias1_dec = np.zeros(hidden_dim)\n",
    "weights2_dec = np.random.randn(hidden_dim, output_dim) * 0.01\n",
    "bias2_dec = np.zeros(output_dim)\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 1e-3\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "# Dummy data (replace with real dataset, e.g., MNIST flattened images)\n",
    "# x_data is a batch of flattened images, size [batch_size, input_dim]\n",
    "x_data = np.random.randn(batch_size, input_dim)  # Random data for now\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(batch_size):  # Simulate a batch for training\n",
    "        x = x_data[i]  # Sample a single data point\n",
    "        \n",
    "        # Forward pass: Encoder\n",
    "        z_mean, z_logvar = encoder(x, weights1_enc, bias1_enc, weights2_enc, bias2_enc, weights3_enc, bias3_enc)\n",
    "        \n",
    "        # Reparameterization trick\n",
    "        z = reparameterize(z_mean, z_logvar)\n",
    "        \n",
    "        # Forward pass: Decoder\n",
    "        x_reconstructed = decoder(z, weights1_dec, bias1_dec, weights2_dec, bias2_dec)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_function(x, x_reconstructed, z_mean, z_logvar)\n",
    "        total_loss += loss\n",
    "        \n",
    "        # Backpropagation (gradient descent, simplified version)\n",
    "        # Compute gradients (manually for simplicity)\n",
    "        # Note: Here we would typically use backpropagation, but we'll skip it\n",
    "        # to keep things simple without frameworks like PyTorch\n",
    "\n",
    "        # Update parameters (gradient descent)\n",
    "        weights1_enc -= learning_rate * np.dot(x[:, None], (z_mean - x_reconstructed))  # Simplified update\n",
    "        bias1_enc -= learning_rate * np.sum(z_mean - x_reconstructed)\n",
    "        \n",
    "        # Add similar updates for the other weights and biases...\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Total Loss: {total_loss / batch_size}')\n",
    "    \n",
    "# Visualize aggregated posterior (assuming 2D latent space)\n",
    "# For demonstration, let's assume z_mean is 2D\n",
    "z_samples = np.random.randn(batch_size, 2)  # Placeholder, replace with real latents\n",
    "plt.scatter(z_samples[:, 0], z_samples[:, 1])\n",
    "plt.title('Aggregated Posterior in Latent Space')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4d09d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Basic activation function: Tanh\n",
    "def tanh(x):\n",
    "    return math.tanh(x)\n",
    "\n",
    "# Dot product function\n",
    "def dot_product(vec1, vec2):\n",
    "    return sum(a * b for a, b in zip(vec1, vec2))\n",
    "\n",
    "# Matrix multiplication (for layer transformation)\n",
    "def matmul(A, B):\n",
    "    # A is an m x n matrix, B is an n x p matrix\n",
    "    # Result will be an m x p matrix\n",
    "    result = []\n",
    "    for row in A:\n",
    "        result_row = []\n",
    "        for col in zip(*B):\n",
    "            result_row.append(dot_product(row, col))\n",
    "        result.append(result_row)\n",
    "    return result\n",
    "\n",
    "# Encoder Network: computes z_mean, z_logvar\n",
    "def encoder(x, weights1, bias1, weights2, bias2, weights3, bias3):\n",
    "    h = [tanh(dot_product(x, w) + b) for w, b in zip(weights1, bias1)]\n",
    "    z_mean = [dot_product(h, w) + b for w, b in zip(weights2, bias2)]\n",
    "    z_logvar = [dot_product(h, w) + b for w, b in zip(weights3, bias3)]\n",
    "    return z_mean, z_logvar\n",
    "\n",
    "# Decoder Network: reconstructs the input\n",
    "def decoder(z, weights1, bias1, weights2, bias2):\n",
    "    h = [tanh(dot_product(z, w) + b) for w, b in zip(weights1, bias1)]\n",
    "    x_reconstructed = [dot_product(h, w) + b for w, b in zip(weights2, bias2)]\n",
    "    return x_reconstructed\n",
    "\n",
    "# Reparameterization trick: sample from q(z|x)\n",
    "def reparameterize(z_mean, z_logvar):\n",
    "    epsilon = [random.gauss(0, 1) for _ in range(len(z_mean))]\n",
    "    return [z_m + math.exp(0.5 * z_lv) * e for z_m, z_lv, e in zip(z_mean, z_logvar, epsilon)]\n",
    "\n",
    "# Compute the loss function (ELBO)\n",
    "def loss_function(x, x_reconstructed, z_mean, z_logvar):\n",
    "    # Reconstruction error (mean squared error)\n",
    "    recon_loss = sum((xi - x_reconstructed[i]) ** 2 for i, xi in enumerate(x))\n",
    "    \n",
    "    # KL divergence\n",
    "    kl_div = -0.5 * sum(1 + z_lv - z_m**2 - math.exp(z_lv) for z_m, z_lv in zip(z_mean, z_logvar))\n",
    "    \n",
    "    # Total loss (ELBO)\n",
    "    return recon_loss + kl_div\n",
    "\n",
    "# Initialize network weights and biases\n",
    "input_dim = 784  # Example: MNIST data (28x28 images flattened)\n",
    "hidden_dim = 400\n",
    "latent_dim = 20\n",
    "output_dim = input_dim\n",
    "\n",
    "# Encoder weights and biases (random initialization)\n",
    "weights1_enc = [[random.gauss(0, 0.01) for _ in range(input_dim)] for _ in range(hidden_dim)]\n",
    "bias1_enc = [random.gauss(0, 0.01) for _ in range(hidden_dim)]\n",
    "weights2_enc = [[random.gauss(0, 0.01) for _ in range(hidden_dim)] for _ in range(latent_dim)]\n",
    "bias2_enc = [random.gauss(0, 0.01) for _ in range(latent_dim)]\n",
    "weights3_enc = [[random.gauss(0, 0.01) for _ in range(hidden_dim)] for _ in range(latent_dim)]\n",
    "bias3_enc = [random.gauss(0, 0.01) for _ in range(latent_dim)]\n",
    "\n",
    "# Decoder weights and biases (random initialization)\n",
    "weights1_dec = [[random.gauss(0, 0.01) for _ in range(latent_dim)] for _ in range(hidden_dim)]\n",
    "bias1_dec = [random.gauss(0, 0.01) for _ in range(hidden_dim)]\n",
    "weights2_dec = [[random.gauss(0, 0.01) for _ in range(hidden_dim)] for _ in range(output_dim)]\n",
    "bias2_dec = [random.gauss(0, 0.01) for _ in range(output_dim)]\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "# Dummy data (replace with real dataset)\n",
    "x_data = [[random.gauss(0, 1) for _ in range(input_dim)] for _ in range(batch_size)]\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(batch_size):\n",
    "        x = x_data[i]\n",
    "        \n",
    "        # Forward pass: Encoder\n",
    "        z_mean, z_logvar = encoder(x, weights1_enc, bias1_enc, weights2_enc, bias2_enc, weights3_enc, bias3_enc)\n",
    "        \n",
    "        # Reparameterization trick\n",
    "        z = reparameterize(z_mean, z_logvar)\n",
    "        \n",
    "        # Forward pass: Decoder\n",
    "        x_reconstructed = decoder(z, weights1_dec, bias1_dec, weights2_dec, bias2_dec)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_function(x, x_reconstructed, z_mean, z_logvar)\n",
    "        total_loss += loss\n",
    "        \n",
    "        # Backpropagation (manual gradient updates are omitted for simplicity)\n",
    "        # In a real implementation, we'd compute gradients and update weights here\n",
    "        \n",
    "    print(f'Epoch {epoch+1}, Total Loss: {total_loss / batch_size}')\n",
    "    \n",
    "# Example visualization: Random latent samples\n",
    "latent_samples = [[random.gauss(0, 1) for _ in range(2)] for _ in range(batch_size)]  # Example 2D latent space\n",
    "plt.scatter([z[0] for z in latent_samples], [z[1] for z in latent_samples])\n",
    "plt.title('Aggregated Posterior in Latent Space')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
