{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c8dd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright (c) 2004 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "52bc660c",
   "metadata": {},
   "source": [
    "## Autoregressive Models with Transformers\n",
    "\n",
    "###  Introduction\n",
    "\n",
    "As presented previously, the idea behind autoregressive models (ARMs) is relatively simple, as it utilizes the product rule multiple times to factorize the joint distribution:\n",
    "\n",
    "$$\n",
    "p(x) = \\prod_{d=1}^D p(x_d | x_{<d}),\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "x_{<d} = [x_1, x_2, \\ldots, x_{d-1}].\n",
    "$$\n",
    "\n",
    "The primary challenge in parameterizing such a model lies in creating a flexible model that can effectively handle the conditionals $ p(x_d | x_{<d}) $. By \"effectively,\" we mean having a single model that outputs probabilities for any given $ x_{<d} $, which is a nontrivial requirement.\n",
    "\n",
    "Neural networks provide a solution for parameterizing ARMs. Causal convolutions, in particular, simplify the implementation of conditional distributions and allow for efficient computation of conditional probabilities for a given $ x $ in a single forward pass. They also enable the modeling of long-range dependencies, which is crucial for complex objects like images.\n",
    "\n",
    "### Limitations and Advancements\n",
    "While autoregressive models with causal convolutions are powerful and have achieved state-of-the-art (SOTA) performance on various tasks, the emergence of the **attention mechanism** has introduced new possibilities. Initially developed for neural machine translation [23, 24], attention mechanisms have since been applied to image processing [25] and numerous other tasks. \n",
    "\n",
    "Attention mechanisms address the limitation of convolutional layers, which operate on a fixed grid of neighbors. For example, in NLP, languages like Dutch enforce specific word orders, whereas Polish allows more flexibility. Consequently, using pre-defined neighborhoods can be restrictive, making the attention mechanism a more suitable approach for capturing long-range dependencies.\n",
    "\n",
    "The seminal work [20] introduced **transformers**, an architecture composed of:\n",
    "- **Attention layers**\n",
    "- **Fully connected layers**\n",
    "- **Layer normalization**\n",
    "\n",
    "While dropout and other tricks are beneficial, these three components are the core of transformers, which have revolutionized various fields, particularly NLP.\n",
    "\n",
    "---\n",
    "\n",
    "## Transformers: \"Attention Is All You Need\"\n",
    "\n",
    "As the title of [20] claims, **attention is all you need**. The attention mechanism can be considered a generalization of convolutional layers. Assuming this holds, neural networks can be constructed using attention mechanisms alone.\n",
    "\n",
    "However, while attention is crucial, additional components are often required. Below, we explore the core mechanism of transformers: **self-attention**.\n",
    "\n",
    "---\n",
    "\n",
    "###  Self-Attention\n",
    "\n",
    "The self-attention mechanism is foundational to transformer architectures. To understand it, consider that convolutional kernels process an object (e.g., a sequence or image) by calculating responses based on a fixed neighborhood. In self-attention, the \"neighborhood\" is learned dynamically. Similar to multiple kernels in CNNs, self-attention can be extended to **multi-head self-attention**.\n",
    "\n",
    "#### Data Preparation and Tokenization\n",
    "\n",
    "In our discussion, we assume the data is well-prepared. A tokenizer converts input data (e.g., text, molecules, or images) into a tensor $ X \\in \\mathbb{R}^{B \\times T \\times D} $, where:\n",
    "- $ B $: Batch size\n",
    "- $ T $: Number of tokens (e.g., characters in a sentence)\n",
    "- $ D $: Dimensionality of token embeddings\n",
    "\n",
    "A robust tokenizer is critical for the success of transformer-based AI systems.\n",
    "\n",
    "#### Self-Attention Mechanism\n",
    "\n",
    "Self-attention requires basic knowledge of matrix calculus. The process involves transforming $ X $ into three new tensors: **values**, **keys**, and **queries**, using linear transformations:\n",
    "\n",
    "- Values: \n",
    "  $$\n",
    "  V = X \\otimes W_v + b_v\n",
    "  $$\n",
    "- Keys: \n",
    "  $$\n",
    "  K = X \\otimes W_k + b_k\n",
    "  $$\n",
    "- Queries: \n",
    "  $$\n",
    "  Q = X \\otimes W_q + b_q\n",
    "  $$\n",
    "\n",
    "Here, $ \\otimes $ denotes batch matrix multiplication, equivalent to the `bmm` function in PyTorch, which performs matrix multiplication for each element in a batch.\n",
    "\n",
    "## Self-Attention: A Deep Dive\n",
    "\n",
    "## Batch Matrix Multiplication ($\\otimes$)\n",
    "\n",
    "To better understand batch matrix multiplication $(\\otimes$), consider:\n",
    "- A tensor $ X $ of size $ B \\times T \\times D $ (in PyTorch or NumPy: `X.shape = (B, T, D)`).\n",
    "- A matrix $ W $ of size $ D \\times M $.\n",
    "\n",
    "The result of $ X \\otimes W $ will have a size $ B \\times T \\times M $. This operation computes the matrix multiplication $ X_b W $ for each batch $ b = 1, 2, \\ldots, B $, and concatenates all outputs. If $ W $ is a tensor $ B \\times D \\times M $, then $ X \\otimes W $ concatenates outputs $ X_b W_b $ for each $ b $.\n",
    "\n",
    "### Connection to Attention Mechanisms\n",
    "\n",
    "In self-attention, we transform the same input $ X $ to compute **queries (Q)**, **keys (K)**, and **values (V)** using linear transformations. This reflects the influence of information retrieval concepts:\n",
    "- **Queries**: Represent the search input.\n",
    "- **Keys**: Represent database information.\n",
    "- **Values**: Represent the retrieved information.\n",
    "\n",
    "Despite the conceptual analogy, these names serve primarily as labels for the linear transformations.\n",
    "\n",
    "---\n",
    "\n",
    "## Core of Self-Attention: Attention Weights\n",
    "\n",
    "The central idea of self-attention lies in computing **attention weights** $( A \\in [0, 1]^{B \\times T \\times T} $), which define the strengths of relationships among tokens. These weights can be interpreted as a soft adjacency matrix of a graph.\n",
    "\n",
    "### Calculating Attention Weights\n",
    "\n",
    "Attention weights are computed as:\n",
    "\n",
    "$$\n",
    "A = \\text{softmax}\\left(\\frac{Q \\otimes K^T}{\\sqrt{D}}\\right),\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ Q \\otimes K^T $: Matrix multiplication of queries and transposed keys.\n",
    "- $ \\sqrt{D} $: Scaling factor to counteract large magnitudes during multiplication.\n",
    "- $\\text{softmax}(\\cdot)$: Applied along the last dimension to ensure outputs sum to 1.\n",
    "\n",
    "---\n",
    "\n",
    "### Self-Attention Output\n",
    "\n",
    "The final output of self-attention is computed by applying the attention weights to the values:\n",
    "\n",
    "$$\n",
    "Y = \\text{softmax}\\left(\\frac{Q \\otimes K^T}{\\sqrt{D}}\\right) \\otimes V,\n",
    "$$\n",
    "\n",
    "where $ Y \\in \\mathbb{R}^{B \\times T \\times D} $.\n",
    "\n",
    "---\n",
    "\n",
    "## Extending to Multi-Head Self-Attention\n",
    "\n",
    "Multi-head self-attention involves running self-attention $ H $-times in parallel, each with its own set of weights:\n",
    "\n",
    "$$\n",
    "Y_h = \\text{softmax}\\left(\\frac{Q_h \\otimes K_h^T}{\\sqrt{D}}\\right) \\otimes V_h, \\quad h = 1, 2, \\ldots, H.\n",
    "$$\n",
    "\n",
    "The outputs $ Y_1, Y_2, \\ldots, Y_H $ are concatenated along the last dimension:\n",
    "\n",
    "$$\n",
    "Y = Y_1 \\oplus Y_2 \\oplus \\ldots \\oplus Y_H,\n",
    "$$\n",
    "\n",
    "resulting in $ Y \\in \\mathbb{R}^{B \\times T \\times (H \\cdot D)} $.\n",
    "\n",
    "Finally, the concatenated output is projected back to the original dimensionality using a linear transformation:\n",
    "\n",
    "$$\n",
    "M = Y \\otimes W_c,\n",
    "$$\n",
    "\n",
    "where $ W_c \\in \\mathbb{R}^{(H \\cdot D) \\times D} $.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Learning Relationships**: The attention weights $ A $ dynamically learn relationships among tokens, capturing long-range dependencies in a single layer.\n",
    "2. **Simplicity and Power**: Self-attention is conceptually simple, relying only on linear layers and softmax. Yet, it enables the model to discover complex patterns in data.\n",
    "3. **Stacking for Hierarchies**: By stacking multiple layers of multi-head self-attention, the model learns increasingly abstract representations of the data.\n",
    "\n",
    "Self-attention is akin to learning a knowledge graph, where each head identifies specific patterns or concepts, and multiple layers build high-level abstractions.\n",
    "\n",
    "Fantastic in its simplicity, isnâ€™t it? ðŸ˜Š\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Fig.7 A schematic representation of operations in self-attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "090b4c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-head output: torch.Size([2, 4, 8])\n",
      "Multi-head output: torch.Size([2, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        \"\"\"\n",
    "        Initialize the self-attention layer.\n",
    "        Args:\n",
    "            embed_dim (int): Dimensionality of the input embeddings (D).\n",
    "        \"\"\"\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Linear layers to compute Queries, Keys, and Values\n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Perform single-head self-attention.\n",
    "        Args:\n",
    "            X (torch.Tensor): Input tensor of shape (B, T, D).\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (B, T, D).\n",
    "        \"\"\"\n",
    "        # Compute Queries, Keys, and Values\n",
    "        Q = self.W_q(X)  # (B, T, D)\n",
    "        K = self.W_k(X)  # (B, T, D)\n",
    "        V = self.W_v(X)  # (B, T, D)\n",
    "\n",
    "        # Compute attention scores and apply softmax\n",
    "        scores = torch.bmm(Q, K.transpose(1, 2)) / (self.embed_dim ** 0.5)  # (B, T, T)\n",
    "        A = F.softmax(scores, dim=-1)  # (B, T, T)\n",
    "\n",
    "        # Compute the output\n",
    "        Y = torch.bmm(A, V)  # (B, T, D)\n",
    "        return Y\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        \"\"\"\n",
    "        Initialize the multi-head self-attention layer.\n",
    "        Args:\n",
    "            embed_dim (int): Dimensionality of the input embeddings (D).\n",
    "            num_heads (int): Number of attention heads (H).\n",
    "        \"\"\"\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by the number of heads.\"\n",
    "\n",
    "        # Dimensionality of each head\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # Linear layers for Queries, Keys, and Values\n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        # Output projection layer\n",
    "        self.W_out = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Perform multi-head self-attention.\n",
    "        Args:\n",
    "            X (torch.Tensor): Input tensor of shape (B, T, D).\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (B, T, D).\n",
    "        \"\"\"\n",
    "        B, T, D = X.size()\n",
    "\n",
    "        # Compute Queries, Keys, and Values and split into multiple heads\n",
    "        Q = self.W_q(X).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)  # (B, H, T, D_h)\n",
    "        K = self.W_k(X).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)  # (B, H, T, D_h)\n",
    "        V = self.W_v(X).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)  # (B, H, T, D_h)\n",
    "\n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)  # (B, H, T, T)\n",
    "        A = F.softmax(scores, dim=-1)  # (B, H, T, T)\n",
    "\n",
    "        # Compute attention outputs\n",
    "        Y = torch.matmul(A, V)  # (B, H, T, D_h)\n",
    "\n",
    "        # Concatenate heads and project back to original dimension\n",
    "        Y = Y.transpose(1, 2).contiguous().view(B, T, D)  # (B, T, D)\n",
    "        Y = self.W_out(Y)  # (B, T, D)\n",
    "        return Y\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    B, T, D, H = 2, 4, 8, 2  # Batch size, Tokens, Embedding Dim, Heads\n",
    "    X = torch.rand(B, T, D)\n",
    "\n",
    "    # Single-head self-attention\n",
    "    single_head = SelfAttention(embed_dim=D)\n",
    "    Y_single = single_head(X)\n",
    "    print(\"Single-head output:\", Y_single.shape)\n",
    "\n",
    "    # Multi-head self-attention\n",
    "    multi_head = MultiHeadSelfAttention(embed_dim=D, num_heads=H)\n",
    "    Y_multi = multi_head(X)\n",
    "    print(\"Multi-head output:\", Y_multi.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4242759c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-head output: [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "Multi-head output: [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "class SelfAttention:\n",
    "    def __init__(self, embed_dim):\n",
    "        \"\"\"\n",
    "        Initialize the self-attention layer.\n",
    "        Args:\n",
    "            embed_dim (int): Dimensionality of the input embeddings (D).\n",
    "        \"\"\"\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Weights for Queries, Keys, and Values\n",
    "        self.W_q = [[0] * embed_dim for _ in range(embed_dim)]\n",
    "        self.W_k = [[0] * embed_dim for _ in range(embed_dim)]\n",
    "        self.W_v = [[0] * embed_dim for _ in range(embed_dim)]\n",
    "\n",
    "    def matmul(self, A, B):\n",
    "        \"\"\"\n",
    "        Perform matrix multiplication.\n",
    "        Args:\n",
    "            A (list of list of floats): Matrix A.\n",
    "            B (list of list of floats): Matrix B.\n",
    "        Returns:\n",
    "            list of list of floats: Result of A * B.\n",
    "        \"\"\"\n",
    "        return [[sum(A[i][k] * B[k][j] for k in range(len(B))) for j in range(len(B[0]))] for i in range(len(A))]\n",
    "\n",
    "    def softmax(self, matrix):\n",
    "        \"\"\"\n",
    "        Apply softmax to the last dimension of a matrix.\n",
    "        Args:\n",
    "            matrix (list of list of floats): Input matrix.\n",
    "        Returns:\n",
    "            list of list of floats: Softmax applied matrix.\n",
    "        \"\"\"\n",
    "        import math\n",
    "        result = []\n",
    "        for row in matrix:\n",
    "            exp_row = [math.exp(x) for x in row]\n",
    "            sum_exp = sum(exp_row)\n",
    "            result.append([x / sum_exp for x in exp_row])\n",
    "        return result\n",
    "\n",
    "    def transpose(self, matrix):\n",
    "        \"\"\"\n",
    "        Transpose a matrix.\n",
    "        Args:\n",
    "            matrix (list of list of floats): Input matrix.\n",
    "        Returns:\n",
    "            list of list of floats: Transposed matrix.\n",
    "        \"\"\"\n",
    "        return [list(row) for row in zip(*matrix)]\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Perform single-head self-attention.\n",
    "        Args:\n",
    "            X (list of list of floats): Input tensor of shape (T, D).\n",
    "        Returns:\n",
    "            list of list of floats: Output tensor of shape (T, D).\n",
    "        \"\"\"\n",
    "        # Compute Queries, Keys, and Values\n",
    "        Q = self.matmul(X, self.W_q)  # (T, D)\n",
    "        K = self.matmul(X, self.W_k)  # (T, D)\n",
    "        V = self.matmul(X, self.W_v)  # (T, D)\n",
    "\n",
    "        # Compute attention scores and apply softmax\n",
    "        K_T = self.transpose(K)  # (D, T)\n",
    "        scores = self.matmul(Q, K_T)  # (T, T)\n",
    "        scaled_scores = [[val / (self.embed_dim ** 0.5) for val in row] for row in scores]  # Scale scores\n",
    "        A = self.softmax(scaled_scores)  # (T, T)\n",
    "\n",
    "        # Compute the output\n",
    "        Y = self.matmul(A, V)  # (T, D)\n",
    "        return Y\n",
    "\n",
    "class MultiHeadSelfAttention:\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        \"\"\"\n",
    "        Initialize the multi-head self-attention layer.\n",
    "        Args:\n",
    "            embed_dim (int): Dimensionality of the input embeddings (D).\n",
    "            num_heads (int): Number of attention heads (H).\n",
    "        \"\"\"\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by the number of heads.\"\n",
    "\n",
    "        # Dimensionality of each head\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # Weights for Queries, Keys, and Values for each head\n",
    "        self.W_q = [[[0] * self.head_dim for _ in range(embed_dim)] for _ in range(num_heads)]\n",
    "        self.W_k = [[[0] * self.head_dim for _ in range(embed_dim)] for _ in range(num_heads)]\n",
    "        self.W_v = [[[0] * self.head_dim for _ in range(embed_dim)] for _ in range(num_heads)]\n",
    "\n",
    "        # Output projection weights\n",
    "        self.W_out = [[0] * embed_dim for _ in range(embed_dim)]\n",
    "\n",
    "    def matmul(self, A, B):\n",
    "        return [[sum(A[i][k] * B[k][j] for k in range(len(B))) for j in range(len(B[0]))] for i in range(len(A))]\n",
    "\n",
    "    def softmax(self, matrix):\n",
    "        import math\n",
    "        result = []\n",
    "        for row in matrix:\n",
    "            exp_row = [math.exp(x) for x in row]\n",
    "            sum_exp = sum(exp_row)\n",
    "            result.append([x / sum_exp for x in exp_row])\n",
    "        return result\n",
    "\n",
    "    def transpose(self, matrix):\n",
    "        return [list(row) for row in zip(*matrix)]\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Perform multi-head self-attention.\n",
    "        Args:\n",
    "            X (list of list of floats): Input tensor of shape (T, D).\n",
    "        Returns:\n",
    "            list of list of floats: Output tensor of shape (T, D).\n",
    "        \"\"\"\n",
    "        T, D = len(X), len(X[0])\n",
    "\n",
    "        # Compute attention for each head\n",
    "        head_outputs = []\n",
    "        for h in range(self.num_heads):\n",
    "            W_q, W_k, W_v = self.W_q[h], self.W_k[h], self.W_v[h]\n",
    "\n",
    "            Q = self.matmul(X, W_q)  # (T, D_h)\n",
    "            K = self.matmul(X, W_k)  # (T, D_h)\n",
    "            V = self.matmul(X, W_v)  # (T, D_h)\n",
    "\n",
    "            K_T = self.transpose(K)  # (D_h, T)\n",
    "            scores = self.matmul(Q, K_T)  # (T, T)\n",
    "            scaled_scores = [[val / (self.head_dim ** 0.5) for val in row] for row in scores]  # Scale scores\n",
    "            A = self.softmax(scaled_scores)  # (T, T)\n",
    "\n",
    "            Y = self.matmul(A, V)  # (T, D_h)\n",
    "            head_outputs.append(Y)\n",
    "\n",
    "        # Concatenate heads\n",
    "        Y = [\n",
    "            [element for head_output in head_outputs for element in head_output[row_idx]]\n",
    "            for row_idx in range(T)\n",
    "        ]  # Combine outputs of all heads\n",
    "\n",
    "        # Output projection\n",
    "        Y = self.matmul(Y, self.W_out)  # (T, D)\n",
    "        return Y\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    T, D, H = 4, 8, 2  # Tokens, Embedding Dim, Heads\n",
    "    X = [[i + j for j in range(D)] for i in range(T)]\n",
    "\n",
    "    # Single-head self-attention\n",
    "    single_head = SelfAttention(embed_dim=D)\n",
    "    Y_single = single_head.forward(X)\n",
    "    print(\"Single-head output:\", Y_single)\n",
    "\n",
    "    # Multi-head self-attention\n",
    "    multi_head = MultiHeadSelfAttention(embed_dim=D, num_heads=H)\n",
    "    Y_multi = multi_head.forward(X)\n",
    "    print(\"Multi-head output:\", Y_multi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d6ef7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-head output: [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-head output: [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class SelfAttention:\n",
    "    def __init__(self, embed_dim):\n",
    "        \"\"\"\n",
    "        Initialize the self-attention layer.\n",
    "        Args:\n",
    "            embed_dim (int): Dimensionality of the input embeddings (D).\n",
    "        \"\"\"\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Weights for Queries, Keys, and Values\n",
    "        self.W_q = [[0] * embed_dim for _ in range(embed_dim)]\n",
    "        self.W_k = [[0] * embed_dim for _ in range(embed_dim)]\n",
    "        self.W_v = [[0] * embed_dim for _ in range(embed_dim)]\n",
    "\n",
    "    def matmul(self, A, B):\n",
    "        \"\"\"\n",
    "        Perform matrix multiplication.\n",
    "        Args:\n",
    "            A (list of list of floats): Matrix A.\n",
    "            B (list of list of floats): Matrix B.\n",
    "        Returns:\n",
    "            list of list of floats: Result of A * B.\n",
    "        \"\"\"\n",
    "        return [[sum(A[i][k] * B[k][j] for k in range(len(B))) for j in range(len(B[0]))] for i in range(len(A))]\n",
    "\n",
    "    def softmax(self, matrix):\n",
    "        \"\"\"\n",
    "        Apply softmax to the last dimension of a matrix.\n",
    "        Args:\n",
    "            matrix (list of list of floats): Input matrix.\n",
    "        Returns:\n",
    "            list of list of floats: Softmax applied matrix.\n",
    "        \"\"\"\n",
    "        import math\n",
    "        result = []\n",
    "        for row in matrix:\n",
    "            exp_row = [math.exp(x) for x in row]\n",
    "            sum_exp = sum(exp_row)\n",
    "            result.append([x / sum_exp for x in exp_row])\n",
    "        return result\n",
    "\n",
    "    def transpose(self, matrix):\n",
    "        \"\"\"\n",
    "        Transpose a matrix.\n",
    "        Args:\n",
    "            matrix (list of list of floats): Input matrix.\n",
    "        Returns:\n",
    "            list of list of floats: Transposed matrix.\n",
    "        \"\"\"\n",
    "        return [list(row) for row in zip(*matrix)]\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Perform single-head self-attention.\n",
    "        Args:\n",
    "            X (list of list of floats): Input tensor of shape (T, D).\n",
    "        Returns:\n",
    "            list of list of floats: Output tensor of shape (T, D).\n",
    "        \"\"\"\n",
    "        # Compute Queries, Keys, and Values\n",
    "        Q = self.matmul(X, self.W_q)  # (T, D)\n",
    "        K = self.matmul(X, self.W_k)  # (T, D)\n",
    "        V = self.matmul(X, self.W_v)  # (T, D)\n",
    "\n",
    "        # Compute attention scores and apply softmax\n",
    "        K_T = self.transpose(K)  # (D, T)\n",
    "        scores = self.matmul(Q, K_T)  # (T, T)\n",
    "        scaled_scores = [[val / (self.embed_dim ** 0.5) for val in row] for row in scores]  # Scale scores\n",
    "        A = self.softmax(scaled_scores)  # (T, T)\n",
    "\n",
    "        # Compute the output\n",
    "        Y = self.matmul(A, V)  # (T, D)\n",
    "        return Y, A\n",
    "\n",
    "class MultiHeadSelfAttention:\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        \"\"\"\n",
    "        Initialize the multi-head self-attention layer.\n",
    "        Args:\n",
    "            embed_dim (int): Dimensionality of the input embeddings (D).\n",
    "            num_heads (int): Number of attention heads (H).\n",
    "        \"\"\"\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by the number of heads.\"\n",
    "\n",
    "        # Dimensionality of each head\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # Weights for Queries, Keys, and Values for each head\n",
    "        self.W_q = [[[0] * self.head_dim for _ in range(embed_dim)] for _ in range(num_heads)]\n",
    "        self.W_k = [[[0] * self.head_dim for _ in range(embed_dim)] for _ in range(num_heads)]\n",
    "        self.W_v = [[[0] * self.head_dim for _ in range(embed_dim)] for _ in range(num_heads)]\n",
    "\n",
    "        # Output projection weights\n",
    "        self.W_out = [[0] * embed_dim for _ in range(embed_dim)]\n",
    "\n",
    "    def matmul(self, A, B):\n",
    "        return [[sum(A[i][k] * B[k][j] for k in range(len(B))) for j in range(len(B[0]))] for i in range(len(A))]\n",
    "\n",
    "    def softmax(self, matrix):\n",
    "        import math\n",
    "        result = []\n",
    "        for row in matrix:\n",
    "            exp_row = [math.exp(x) for x in row]\n",
    "            sum_exp = sum(exp_row)\n",
    "            result.append([x / sum_exp for x in exp_row])\n",
    "        return result\n",
    "\n",
    "    def transpose(self, matrix):\n",
    "        return [list(row) for row in zip(*matrix)]\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Perform multi-head self-attention.\n",
    "        Args:\n",
    "            X (list of list of floats): Input tensor of shape (T, D).\n",
    "        Returns:\n",
    "            list of list of floats: Output tensor of shape (T, D).\n",
    "        \"\"\"\n",
    "        T, D = len(X), len(X[0])\n",
    "\n",
    "        # Compute attention for each head\n",
    "        head_outputs = []\n",
    "        all_attention_weights = []\n",
    "        for h in range(self.num_heads):\n",
    "            W_q, W_k, W_v = self.W_q[h], self.W_k[h], self.W_v[h]\n",
    "\n",
    "            Q = self.matmul(X, W_q)  # (T, D_h)\n",
    "            K = self.matmul(X, W_k)  # (T, D_h)\n",
    "            V = self.matmul(X, W_v)  # (T, D_h)\n",
    "\n",
    "            K_T = self.transpose(K)  # (D_h, T)\n",
    "            scores = self.matmul(Q, K_T)  # (T, T)\n",
    "            scaled_scores = [[val / (self.head_dim ** 0.5) for val in row] for row in scores]  # Scale scores\n",
    "            A = self.softmax(scaled_scores)  # (T, T)\n",
    "\n",
    "            Y = self.matmul(A, V)  # (T, D_h)\n",
    "            head_outputs.append(Y)\n",
    "            all_attention_weights.append(A)\n",
    "\n",
    "        # Concatenate heads\n",
    "        Y = [\n",
    "            [element for head_output in head_outputs for element in head_output[row_idx]]\n",
    "            for row_idx in range(T)\n",
    "        ]  # Combine outputs of all heads\n",
    "\n",
    "        # Output projection\n",
    "        Y = self.matmul(Y, self.W_out)  # (T, D)\n",
    "        return Y, all_attention_weights\n",
    "\n",
    "# Example usage and plotting\n",
    "if __name__ == \"__main__\":\n",
    "    T, D, H = 4, 8, 2  # Tokens, Embedding Dim, Heads\n",
    "    X = [[i + j for j in range(D)] for i in range(T)]\n",
    "\n",
    "    # Single-head self-attention\n",
    "    single_head = SelfAttention(embed_dim=D)\n",
    "    Y_single, A_single = single_head.forward(X)\n",
    "    print(\"Single-head output:\", Y_single)\n",
    "\n",
    "    # Plot attention weights for single-head\n",
    "    plt.imshow(A_single, cmap=\"viridis\")\n",
    "    plt.title(\"Single-Head Attention Weights\")\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "    # Multi-head self-attention\n",
    "    multi_head = MultiHeadSelfAttention(embed_dim=D, num_heads=H)\n",
    "    Y_multi, A_multi = multi_head.forward(X)\n",
    "    print(\"Multi-head output:\", Y_multi)\n",
    "\n",
    "    # Plot attention weights for each head in multi-head attention\n",
    "    for h, A in enumerate(A_multi):\n",
    "        plt.imshow(A, cmap=\"viridis\")\n",
    "        plt.title(f\"Head {h + 1} Attention Weights\")\n",
    "        plt.colorbar()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e2edb7",
   "metadata": {},
   "source": [
    "##  Toward Implementing Transformers: Multi-head Self-Attention\n",
    "\n",
    "The way we presented self-attention was very close to the code. However, to implement self-attention, we can use a few simple tricks to speed up computations. The most important one involves calculating keys, queries, and values. Instead of defining separate single-head self-attentions, we can define linear layers that output $ D \\times H $ outputs, and then reshape tensors to $ B \\times T \\times H \\times D $ for batch matrix multiplication.\n",
    "\n",
    "The full code is presented in the listing below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0eac3c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, num_emb, num_heads=8):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.D = num_emb\n",
    "        self.H = num_heads\n",
    "        \n",
    "        # Weights for self-attention\n",
    "        self.w_k = nn.Linear(self.D, self.D * self.H)\n",
    "        self.w_q = nn.Linear(self.D, self.D * self.H)\n",
    "        self.w_v = nn.Linear(self.D, self.D * self.H)\n",
    "        \n",
    "        # Weights for a combination of multiple heads\n",
    "        self.w_c = nn.Linear(self.D * self.H, self.D)\n",
    "    \n",
    "    def forward(self, x, causal=True):\n",
    "        # Input: x (Batch $ B $ x Tokens $ T $ x Dimensionality $ D $)\n",
    "        B, T, D = x.size()\n",
    "        \n",
    "        # Keys, queries, and values\n",
    "        k = self.w_k(x).view(B, T, self.H, D)  # $ B \\times T \\times H \\times D $\n",
    "        q = self.w_q(x).view(B, T, self.H, D)  # $ B \\times T \\times H \\times D $\n",
    "        v = self.w_v(x).view(B, T, self.H, D)  # $ B \\times T \\times H \\times D $\n",
    "        \n",
    "        k = k.transpose(1, 2).contiguous().view(B * self.H, T, D)  # $ B \\times H \\to B*H $\n",
    "        q = q.transpose(1, 2).contiguous().view(B * self.H, T, D)  # $ B \\times H \\to B*H $\n",
    "        v = v.transpose(1, 2).contiguous().view(B * self.H, T, D)  # $ B \\times H \\to B*H $\n",
    "        \n",
    "        # Scaling\n",
    "        k = k / (D ** 0.25)\n",
    "        q = q / (D ** 0.25)\n",
    "        \n",
    "        # $ k \\cdot q^T $\n",
    "        kq = torch.bmm(q, k.transpose(1, 2))  # $ B*H \\times T \\times T $\n",
    "        \n",
    "        # Causal masking\n",
    "        if causal:\n",
    "            mask = torch.triu_indices(T, T, offset=1)\n",
    "            kq[..., mask[0], mask[1]] = float('-inf')\n",
    "        \n",
    "        # Softmax\n",
    "        skq = F.softmax(kq, dim=2)\n",
    "        \n",
    "        # Self-attention\n",
    "        sa = torch.bmm(skq, v)  # $ B*H \\times T \\times D $\n",
    "        sa = sa.view(B, self.H, T, D)  # $ B \\times H \\times T \\times D $\n",
    "        sa = sa.transpose(1, 2)   #$ B \\times T \\times H \\times D $\n",
    "        sa = sa.contiguous().view(B, T, D * self.H) #  $ B \\times T \\times D*H $\n",
    "        \n",
    "        # Output projection\n",
    "        out = self.w_c(sa)  # $ B \\times T \\times D $\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1a742e",
   "metadata": {},
   "source": [
    "## Causality and Positional Encodings in Transformers\n",
    "\n",
    "## Causality in Self-Attention\n",
    "\n",
    "To use self-attention as a replacement for causal convolutional layers in Autoregressive Models (ARMs), we need to ensure that the model does not \"look into the future.\" This is achieved by masking the multiplication of keys and queries to ensure causal calculations. \n",
    "\n",
    "For example, consider a sequence where the first token is a dummy token (i.e., Beginning Of Sequence, BOS). By masking values above the diagonal in the product of keys and queries, we ensure that no future tokens leak into the attention weights.\n",
    "\n",
    "## Positional Encodings\n",
    "\n",
    "Self-attention is permutation equivariant, meaning that permuting input tokens results in permuting output tokens in the same way. To address this, positional information is added to the input $ X $. \n",
    "\n",
    "The simplest approach is to encode absolute positions using an embedding layer, where each position integer is represented by a real-valued vector. The positional encoding $ P $ is then added to the input:\n",
    "$$\n",
    "X' = X + P\n",
    "$$\n",
    "This addition breaks permutation equivariance and ensures that the model understands token order.\n",
    "\n",
    "## Transformer Block: Putting It All Together\n",
    "\n",
    "Transformers consist of building blocks called **transformer blocks**. A single transformer block comprises the following components:\n",
    "\n",
    "1. **Multi-Head Self-Attention (MHSA):**\n",
    "   $$\n",
    "   \\text{MHSA}(X) = Y \\otimes W_c\n",
    "   $$\n",
    "   where:\n",
    "   $$\n",
    "   Y = Y_1 \\oplus \\cdots \\oplus Y_H, \\quad Y_h = \\text{softmax}\\left(\\frac{Q_h \\otimes K_h^\\top}{\\sqrt{D}}\\right) \\otimes V_h\n",
    "   $$\n",
    "\n",
    "2. **Layer Normalization (LN):**\n",
    "   $$\n",
    "   \\text{LN}(X) = \\gamma \\frac{X - m}{\\sqrt{s + \\epsilon}} + \\delta\n",
    "   $$\n",
    "   where $ m, s, \\gamma, \\delta $ are calculated per layer, not batch, and $ \\epsilon > 0 $ ensures numerical stability.\n",
    "\n",
    "3. **MLP (Feedforward Network):**\n",
    "   $$\n",
    "   \\text{MLP}(X) = \\text{Linear}(\\text{GELU}(\\text{Linear}(X)))\n",
    "   $$\n",
    "\n",
    "A single transformer block processes inputs as follows:\n",
    "1. **Multi-Head Self-Attention**:\n",
    "   $$\n",
    "   M = \\text{MHSA}(X)\n",
    "   $$\n",
    "2. **First Layer Normalization**:\n",
    "   $$\n",
    "   U = \\text{LN}_1(X + M)\n",
    "   $$\n",
    "3. **MLP**:\n",
    "   $$\n",
    "   Z = \\text{MLP}(U)\n",
    "   $$\n",
    "4. **Second Layer Normalization**:\n",
    "   $$\n",
    "   X' = \\text{LN}_2(Z + U)\n",
    "   $$\n",
    "\n",
    "### Transformer Block Implementation\n",
    "\n",
    "```python\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, num_emb, num_neurons, num_heads=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.D = num_emb\n",
    "        self.H = num_heads\n",
    "        self.neurons = num_neurons\n",
    "        \n",
    "        # Components\n",
    "        self.msha = MultiHeadSelfAttention(num_emb=self.D, num_heads=self.H)\n",
    "        self.layer_norm1 = nn.LayerNorm(self.D)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.D)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.D, self.neurons * self.D),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.neurons * self.D, self.D)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, causal=True):\n",
    "        # Multi-Head Self-Attention\n",
    "        x_attn = self.msha(x, causal)\n",
    "        \n",
    "        # First Layer Normalization\n",
    "        x = self.layer_norm1(x_attn + x)\n",
    "        \n",
    "        # MLP\n",
    "        x_mlp = self.mlp(x)\n",
    "        \n",
    "        # Second Layer Normalization\n",
    "        x = self.layer_norm2(x_mlp + x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1bd350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, num_emb, num_heads=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.D = num_emb\n",
    "        self.H = num_heads\n",
    "\n",
    "        # Linear layers for keys, queries, and values\n",
    "        self.w_k = nn.Linear(self.D, self.D * self.H)\n",
    "        self.w_q = nn.Linear(self.D, self.D * self.H)\n",
    "        self.w_v = nn.Linear(self.D, self.D * self.H)\n",
    "\n",
    "        # Linear layer for output projection\n",
    "        self.w_c = nn.Linear(self.D * self.H, self.D)\n",
    "\n",
    "    def forward(self, x, causal=True):\n",
    "        B, T, D = x.size()\n",
    "\n",
    "        # Generate keys, queries, and values\n",
    "        k = self.w_k(x).view(B, T, self.H, D).transpose(1, 2).contiguous().view(B * self.H, T, D)\n",
    "        q = self.w_q(x).view(B, T, self.H, D).transpose(1, 2).contiguous().view(B * self.H, T, D)\n",
    "        v = self.w_v(x).view(B, T, self.H, D).transpose(1, 2).contiguous().view(B * self.H, T, D)\n",
    "\n",
    "        # Scale queries and keys\n",
    "        k = k / (D ** 0.25)\n",
    "        q = q / (D ** 0.25)\n",
    "\n",
    "        # Compute attention scores\n",
    "        kq = torch.bmm(q, k.transpose(1, 2))  # B*H x T x T\n",
    "\n",
    "        # Apply causal masking if necessary\n",
    "        if causal:\n",
    "            mask = torch.triu(torch.ones(T, T), diagonal=1).to(kq.device)\n",
    "            mask = mask.bool()\n",
    "            kq.masked_fill_(mask, float('-inf'))\n",
    "\n",
    "        # Compute attention weights\n",
    "        skq = F.softmax(kq, dim=-1)\n",
    "\n",
    "        # Compute self-attention output\n",
    "        sa = torch.bmm(skq, v)  # B*H x T x D\n",
    "        sa = sa.view(B, self.H, T, D).transpose(1, 2).contiguous().view(B, T, D * self.H)\n",
    "\n",
    "        # Output projection\n",
    "        out = self.w_c(sa)\n",
    "\n",
    "        return out\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, num_emb, num_neurons, num_heads=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.D = num_emb\n",
    "        self.H = num_heads\n",
    "        self.neurons = num_neurons\n",
    "\n",
    "        # Components\n",
    "        self.msha = MultiHeadSelfAttention(num_emb=self.D, num_heads=self.H)\n",
    "        self.layer_norm1 = nn.LayerNorm(self.D)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.D)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.D, self.neurons * self.D),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.neurons * self.D, self.D)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, causal=True):\n",
    "        # Multi-Head Self-Attention\n",
    "        x_attn = self.msha(x, causal)\n",
    "\n",
    "        # First Layer Normalization\n",
    "        x = self.layer_norm1(x + x_attn)\n",
    "\n",
    "        # MLP\n",
    "        x_mlp = self.mlp(x)\n",
    "\n",
    "        # Second Layer Normalization\n",
    "        x = self.layer_norm2(x + x_mlp)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    B, T, D = 2, 5, 64  # Batch size, Sequence length, Embedding dimension\n",
    "    H = 4  # Number of heads\n",
    "    num_neurons = 2  # Neuron multiplier for MLP\n",
    "\n",
    "    x = torch.rand(B, T, D)  # Example input\n",
    "\n",
    "    transformer_block = TransformerBlock(num_emb=D, num_neurons=num_neurons, num_heads=H)\n",
    "    output = transformer_block(x, causal=True)\n",
    "\n",
    "    print(\"Transformer Block Output:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c89c4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer Block Output Shape: (2, 5, 64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MultiHeadSelfAttention:\n",
    "    def __init__(self, num_emb, num_heads):\n",
    "        self.D = num_emb\n",
    "        self.H = num_heads\n",
    "        self.W_k = np.random.randn(self.D, self.D * self.H)  # Weights for keys\n",
    "        self.W_q = np.random.randn(self.D, self.D * self.H)  # Weights for queries\n",
    "        self.W_v = np.random.randn(self.D, self.D * self.H)  # Weights for values\n",
    "        self.W_c = np.random.randn(self.D * self.H, self.D)  # Combine heads\n",
    "    \n",
    "    def softmax(self, x, axis=-1):\n",
    "        exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "    \n",
    "    def forward(self, x, causal=True):\n",
    "        B, T, D = x.shape\n",
    "\n",
    "        # Keys, Queries, Values\n",
    "        k = np.dot(x, self.W_k).reshape(B, T, self.H, D)\n",
    "        q = np.dot(x, self.W_q).reshape(B, T, self.H, D)\n",
    "        v = np.dot(x, self.W_v).reshape(B, T, self.H, D)\n",
    "\n",
    "        # Transpose and reshape for batch matrix multiplication\n",
    "        k = k.transpose(0, 2, 1, 3).reshape(B * self.H, T, D)\n",
    "        q = q.transpose(0, 2, 1, 3).reshape(B * self.H, T, D)\n",
    "        v = v.transpose(0, 2, 1, 3).reshape(B * self.H, T, D)\n",
    "\n",
    "        # Scaling\n",
    "        k /= np.sqrt(D)\n",
    "        q /= np.sqrt(D)\n",
    "\n",
    "        # Attention scores\n",
    "        kq = np.matmul(q, k.transpose(0, 2, 1))\n",
    "\n",
    "        # Apply causal masking\n",
    "        if causal:\n",
    "            mask = np.triu(np.ones((T, T)), k=1).astype(np.float32)\n",
    "            mask[mask == 1] = -np.inf\n",
    "            kq += mask\n",
    "\n",
    "        # Softmax and attention\n",
    "        skq = self.softmax(kq, axis=-1)\n",
    "        sa = np.matmul(skq, v)\n",
    "\n",
    "        # Reshape back\n",
    "        sa = sa.reshape(B, self.H, T, D).transpose(0, 2, 1, 3)\n",
    "        sa = sa.reshape(B, T, self.H * D)\n",
    "\n",
    "        # Combine heads\n",
    "        out = np.dot(sa, self.W_c)\n",
    "        return out\n",
    "\n",
    "\n",
    "class TransformerBlock:\n",
    "    def __init__(self, num_emb, num_neurons, num_heads):\n",
    "        self.D = num_emb\n",
    "        self.H = num_heads\n",
    "        self.neurons = num_neurons\n",
    "\n",
    "        # Components\n",
    "        self.msha = MultiHeadSelfAttention(num_emb=self.D, num_heads=self.H)\n",
    "        self.W_mlp1 = np.random.randn(self.D, self.neurons * self.D)\n",
    "        self.W_mlp2 = np.random.randn(self.neurons * self.D, self.D)\n",
    "        self.gamma1 = np.ones((self.D,))\n",
    "        self.beta1 = np.zeros((self.D,))\n",
    "        self.gamma2 = np.ones((self.D,))\n",
    "        self.beta2 = np.zeros((self.D,))\n",
    "    \n",
    "    def layer_norm(self, x, gamma, beta, eps=1e-5):\n",
    "        mean = np.mean(x, axis=-1, keepdims=True)\n",
    "        var = np.var(x, axis=-1, keepdims=True)\n",
    "        x_norm = (x - mean) / np.sqrt(var + eps)\n",
    "        return gamma * x_norm + beta\n",
    "\n",
    "    def forward(self, x, causal=True):\n",
    "        # Multi-Head Self-Attention\n",
    "        x_attn = self.msha.forward(x, causal)\n",
    "\n",
    "        # LayerNorm 1\n",
    "        x = self.layer_norm(x + x_attn, self.gamma1, self.beta1)\n",
    "\n",
    "        # MLP\n",
    "        x_mlp = np.dot(x, self.W_mlp1)\n",
    "        x_mlp = np.maximum(0, x_mlp)  # ReLU\n",
    "        x_mlp = np.dot(x_mlp, self.W_mlp2)\n",
    "\n",
    "        # LayerNorm 2\n",
    "        x = self.layer_norm(x + x_mlp, self.gamma2, self.beta2)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "B, T, D = 2, 5, 64  # Batch size, Sequence length, Embedding dimension\n",
    "H = 4                # Number of heads\n",
    "neurons = 2          # Multiplier for hidden layer neurons in MLP\n",
    "\n",
    "# Random input sequence\n",
    "x = np.random.randn(B, T, D)\n",
    "\n",
    "# Transformer Block\n",
    "transformer_block = TransformerBlock(num_emb=D, num_neurons=neurons, num_heads=H)\n",
    "output = transformer_block.forward(x, causal=True)\n",
    "\n",
    "print(\"Transformer Block Output Shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74e82f5",
   "metadata": {},
   "source": [
    "# Transformer-based ARM (Autoregressive Model)\n",
    "\n",
    "### Initial Remarks\n",
    "\n",
    "In this section, we implement a transformer-based architecture for autoregressive models (ARMs). Here are some key points:\n",
    "\n",
    "- We assume that inputs (i.e., tokens) are integers, but their order doesnâ€™t matter. The first token is a â€œdummyâ€ token to signal the start of a sequence.\n",
    "- The model uses two embedding layers: one for converting integer tokens into real-valued vectors, and another for positional encoding. Both embeddings are learnable.\n",
    "- A dropout layer is used before applying the transformer blocks to handle missing tokens, with a dropout probability of 0.1.\n",
    "- Since we assume that tokens are represented by integers, the conditional probabilities are modeled using categorical distributions, \\( p(x_t | x_{<t}) \\).\n",
    "- The first token is a \"dummy\" token, so we need to shift the outputs of the transformer and disregard the first input when calculating the loss function.\n",
    "\n",
    "### Sampling Procedure\n",
    "\n",
    "Since we are using transformers to parameterize ARMs, the sampling procedure is similar to that of ARMs with causal convolutions. Specifically, we start with a sequence containing the \"dummy\" token and sample one token at a time in a loop.\n",
    "\n",
    "### Full Code for Transformer-based ARM\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_tokens, num_token_vals, num_emb, num_neurons, num_heads=2, dropout_prob=0.1, num_blocks=10, device='cpu'):\n",
    "        super().__init__()\n",
    "\n",
    "        # hyperparameters\n",
    "        self.device = device\n",
    "        self.num_tokens = num_tokens\n",
    "        self.num_token_vals = num_token_vals\n",
    "        self.num_emb = num_emb\n",
    "        self.num_blocks = num_blocks\n",
    "\n",
    "        # embedding layers\n",
    "        self.embedding = torch.nn.Embedding(num_token_vals, num_emb)\n",
    "        self.positional_embedding = nn.Embedding(num_tokens, num_emb)\n",
    "\n",
    "        # transformer blocks\n",
    "        self.transformer_blocks = nn.ModuleList()\n",
    "        for _ in range(num_blocks):\n",
    "            self.transformer_blocks.append(TransformerBlock(num_emb=num_emb, num_neurons=num_neurons, num_heads=num_heads))\n",
    "\n",
    "        # output layer (logits + softmax)\n",
    "        self.logits = nn.Sequential(nn.Linear(num_emb, num_token_vals))\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # loss function\n",
    "        self.loss_fun = LossFun()\n",
    "\n",
    "    def transformer_forward(self, x, causal=True, temperature=1.0):\n",
    "        # x: B(atch) x T(okens)\n",
    "        x = self.embedding(x)  # B x T x D\n",
    "        pos = torch.arange(0, x.shape[1], dtype=torch.long).unsqueeze(0).to(self.device)\n",
    "        pos_emb = self.positional_embedding(pos)\n",
    "        x = self.dropout(x + pos_emb)\n",
    "\n",
    "        # transformer blocks\n",
    "        for i in range(self.num_blocks):\n",
    "            x = self.transformer_blocks[i](x)\n",
    "\n",
    "        # output logits\n",
    "        out = self.logits(x)\n",
    "        return F.log_softmax(out / temperature, 2)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, batch_size=4, temperature=1.0):\n",
    "        x_seq = np.asarray([[self.num_token_vals - 1] for i in range(batch_size)])\n",
    "\n",
    "        # sample next tokens\n",
    "        for i in range(self.num_tokens - 1):\n",
    "            xx = torch.tensor(x_seq, dtype=torch.long, device=self.device)\n",
    "            x_log_probs = self.transformer_forward(xx, temperature=temperature)\n",
    "            x_i_sample = torch.multinomial(torch.exp(x_log_probs[:, i]), 1).to(self.device)\n",
    "            x_seq = np.concatenate((x_seq, x_i_sample.to('cpu').detach().numpy()), 1)\n",
    "\n",
    "        return x_seq\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def top1_rec(self, x, causal=True):\n",
    "        x_prob = torch.exp(self.transformer_forward(x, causal=True))[:, :-1, :].contiguous()\n",
    "        _, x_rec_max = torch.max(x_prob, dim=2)\n",
    "        return torch.sum(torch.mean((x_rec_max.float() == x[:, 1:].float().to(self.device)).float(), 1).float())\n",
    "\n",
    "    def forward(self, x, causal=True, temperature=1.0, reduction='mean'):\n",
    "        # get log-probabilities\n",
    "        log_prob = self.transformer_forward(x, causal=causal, temperature=temperature)\n",
    "        return self.loss_fun(log_prob[:, :-1].contiguous(), x[:, 1:].contiguous(), reduction=reduction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fe8a56",
   "metadata": {},
   "source": [
    "# Transformer-based Molecule Generation and Transformer Research\n",
    "\n",
    "## 3.3.2.3 Results of Transformer-based ARM\n",
    "\n",
    "### Molecule Generation with SMILES\n",
    "\n",
    "In this experiment, we aim to learn a generative model for molecules. Molecules can be represented as strings of atoms and additional characters, like brackets, among others, known as SMILES [27]. After tokenizing SMILES using a specialized tokenizer (e.g., DeepChem [28]), we can represent molecules as sequences of integers. Each integer encodes either an atom or a character needed to represent a structure.\n",
    "\n",
    "The process of encoding molecules using SMILES is reversible: given a SMILES string, we can recover its graphical representation, and similarly, encoding SMILES using integers is also invertible.\n",
    "\n",
    "We use a simple dataset, the Tox21 dataset, containing about 7.4K molecules. This dataset is split into:\n",
    "- **Training set**: 6.6k molecules\n",
    "- **Validation set**: 400 molecules\n",
    "- **Test set**: a bit more than 400 molecules\n",
    "\n",
    "### Metrics Monitored\n",
    "\n",
    "In the experiments, we monitor two key metrics:\n",
    "1. **Negative log-likelihood (NLL)**: A measure of the likelihood of the model's predictions.\n",
    "2. **Reconstruction Accuracy (Rec)**: This is calculated by taking the most probable outputs from the transformer, instead of sampling from the distribution.\n",
    "\n",
    "Figures in **Fig. 3.8** show results:\n",
    "- **(a)** An example of a sample generated by the trained transformer.\n",
    "- **(b)** The negative log-likelihood.\n",
    "- **(c)** The reconstruction accuracy.\n",
    "\n",
    "After training with a relatively simple transformer model (about 1M weights), we observe the following:\n",
    "\n",
    "- **Generated Molecule Samples** (Fig. 3.8a)\n",
    "- **Validation Negative Log-Likelihood** (Fig. 3.8b)\n",
    "- **Validation Reconstruction Accuracy** (Fig. 3.8c)\n",
    "\n",
    "## 3.3.2.4 Transformers: A Dominant Field\n",
    "\n",
    "### Transformers in Generative AI\n",
    "\n",
    "Transformers are now at the forefront of many successes in generative AI, and you've likely heard of models like GPT (Generative Pretrained Transformer) and ChatGPT. These models use transformers, which have revolutionized the field of natural language processing (NLP) and beyond.\n",
    "\n",
    "#### Large Language Models (LLMs)\n",
    "Since the rise of transformers, many large-scale models based on transformers have emerged, known as **Large Language Models (LLMs)**. These models are typically pre-trained on massive datasets and then fine-tuned for downstream tasks. LLMs are also referred to as **foundation models**.\n",
    "\n",
    "Some important readings on transformers include:\n",
    "- ****: This provides an excellent introduction to transformers with numerous applications and extensions.\n",
    "  \n",
    "### Transformer Architectures and Applications\n",
    "Transformers are used for various NLP tasks, including:\n",
    "1. **Autoregressive Models (ARMs)**: Like GPT, which uses causal masks.\n",
    "2. **Autoencoding Models**: Like BERT, which does not use causal masks and learns a conditional distribution \\( p(x' | x) \\).\n",
    "\n",
    "#### Types of Transformer Models:\n",
    "- **Decoders**: These are ARMs that use causal masks. Example: GPT.\n",
    "- **Encoders**: These do not use causal masks. Example: BERT.\n",
    "- **Encoder-Decoder Models**: These models use cross-attention and are often used in machine translation tasks.\n",
    "\n",
    "### Zero-shot Learning and Prompt Engineering\n",
    "LLMs have shown the capability of **zero-shot learning**. This means they can perform tasks without explicit task-specific training by simply modifying the input prompt.\n",
    "\n",
    "For further reading on **zero-shot learning** and how to modify prompts to guide LLM behavior, check out this excellent overview by [33].\n",
    "\n",
    "### Transformer Applications in Molecule Generation\n",
    "Transformers have been used in **molecule generation** tasks, particularly in drug discovery. One notable resource is a paper (and accompanying code) on **drug discovery** [34], where transformers are used for molecule generation and property prediction. You can also explore papers on **de novo drug design** [36].\n",
    "\n",
    "### Transformers for Image Processing\n",
    "Transformers are not limited to text and molecules. They are also used for **image processing**. The approach, known as the **Visual Transformer (ViT)**, divides an image into small patches (e.g., 16x16 pixels) and processes them similarly to how sequences are processed. For more on this, check out the work on ViT [37].\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Transformers have truly changed the landscape of AI and machine learning, enabling breakthroughs in fields ranging from natural language processing to molecule generation. By utilizing transformer models with less than a thousand lines of code, we can generate new molecules and explore many possibilities in generative AI!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ba207d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 4.0689\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Define TransformerBlock (a simple transformer block)\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, num_emb, num_neurons, num_heads):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(num_emb, num_heads)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(num_emb, num_neurons),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_neurons, num_emb)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(num_emb)\n",
    "        self.norm2 = nn.LayerNorm(num_emb)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Self-attention layer\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + self.dropout(attn_out))\n",
    "        \n",
    "        # Feed-forward layer\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.dropout(ff_out))\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Define the Transformer model for molecule generation\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_tokens, num_token_vals, num_emb, num_neurons, num_heads=2, dropout_prob=0.1, num_blocks=10, device='cpu'):\n",
    "        super().__init__()\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.device = device\n",
    "        self.num_tokens = num_tokens\n",
    "        self.num_token_vals = num_token_vals\n",
    "        self.num_emb = num_emb\n",
    "        self.num_blocks = num_blocks\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(num_token_vals, num_emb)\n",
    "\n",
    "        # Positional encoding\n",
    "        self.positional_embedding = nn.Embedding(num_tokens, num_emb)\n",
    "\n",
    "        # Transformer blocks\n",
    "        self.transformer_blocks = nn.ModuleList([TransformerBlock(num_emb, num_neurons, num_heads) for _ in range(num_blocks)])\n",
    "\n",
    "        # Output layer (logits + softmax)\n",
    "        self.logits = nn.Linear(num_emb, num_token_vals)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def transformer_forward(self, x, causal=True, temperature=1.0):\n",
    "        # Embedding of tokens\n",
    "        x = self.embedding(x)  # B x T x D\n",
    "        pos = torch.arange(0, x.shape[1], dtype=torch.long).unsqueeze(0).to(self.device)\n",
    "        pos_emb = self.positional_embedding(pos)\n",
    "        x = self.dropout(x + pos_emb)\n",
    "\n",
    "        # Transformer blocks\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # Output logits\n",
    "        out = self.logits(x)\n",
    "        return F.log_softmax(out / temperature, dim=2)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, batch_size=4, temperature=1.0):\n",
    "        # Initial sequence with \"dummy\" token (last token in vocabulary)\n",
    "        x_seq = np.asarray([[self.num_token_vals - 1] for _ in range(batch_size)])\n",
    "\n",
    "        # Sample next tokens\n",
    "        for _ in range(self.num_tokens - 1):\n",
    "            xx = torch.tensor(x_seq, dtype=torch.long, device=self.device)\n",
    "            x_log_probs = self.transformer_forward(xx, temperature=temperature)\n",
    "            x_i_sample = torch.multinomial(torch.exp(x_log_probs[:, -1]), 1).to(self.device)\n",
    "            x_seq = np.concatenate((x_seq, x_i_sample.cpu().detach().numpy()), axis=1)\n",
    "\n",
    "        return x_seq\n",
    "\n",
    "    def forward(self, x, causal=True, temperature=1.0):\n",
    "        # Get log probabilities\n",
    "        log_prob = self.transformer_forward(x, causal=causal, temperature=temperature)\n",
    "        return log_prob\n",
    "\n",
    "# Loss function (negative log-likelihood)\n",
    "class LossFun(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, log_prob, target, reduction='mean'):\n",
    "        loss = F.nll_loss(log_prob.view(-1, log_prob.size(-1)), target.view(-1), reduction=reduction)\n",
    "        return loss\n",
    "\n",
    "# Example data (SMILES encoded as integers)\n",
    "# For simplicity, let's simulate some toy data.\n",
    "def create_toy_data(num_samples=1000, num_tokens=50, vocab_size=50):\n",
    "    # Random sequences of tokens (representing SMILES strings)\n",
    "    data = np.random.randint(0, vocab_size, size=(num_samples, num_tokens))\n",
    "    return torch.tensor(data, dtype=torch.long)\n",
    "\n",
    "# Training the model\n",
    "def train(model, data, epochs=10, batch_size=32, learning_rate=0.001):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = LossFun()\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            # Get batch data\n",
    "            batch = data[i:i+batch_size]\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            log_probs = model(batch)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(log_probs, batch, reduction='mean')\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_loss = total_loss / (len(data) // batch_size)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Hyperparameters\n",
    "num_tokens = 50\n",
    "num_token_vals = 50  # Vocabulary size\n",
    "num_emb = 128  # Embedding dimension\n",
    "num_neurons = 256  # Feed-forward layer size\n",
    "num_heads = 2  # Number of attention heads\n",
    "num_blocks = 6  # Number of transformer blocks\n",
    "device = 'cpu'  # Change to 'cuda' if using GPU\n",
    "\n",
    "# Instantiate the model\n",
    "model = Transformer(num_tokens, num_token_vals, num_emb, num_neurons, num_heads, device=device)\n",
    "\n",
    "# Generate toy data for training\n",
    "data = create_toy_data()\n",
    "\n",
    "# Train the model\n",
    "train(model, data, epochs=5)\n",
    "\n",
    "# Sampling new sequences (molecule generation)\n",
    "generated_samples = model.sample(batch_size=4, temperature=1.0)\n",
    "print(f\"Generated Samples:\\n{generated_samples}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc11e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize the essential transformer components\n",
    "class SimpleTransformer:\n",
    "    def __init__(self, num_tokens, num_token_vals, num_emb, num_neurons, num_heads, num_blocks):\n",
    "        self.num_tokens = num_tokens\n",
    "        self.num_token_vals = num_token_vals\n",
    "        self.num_emb = num_emb\n",
    "        self.num_neurons = num_neurons\n",
    "        self.num_heads = num_heads\n",
    "        self.num_blocks = num_blocks\n",
    "        \n",
    "        # Initialize embeddings for tokens and positions\n",
    "        self.token_embedding = np.random.randn(num_token_vals, num_emb)\n",
    "        self.positional_embedding = np.random.randn(num_tokens, num_emb)\n",
    "        \n",
    "        # Initialize transformer blocks\n",
    "        self.transformer_blocks = [self.create_transformer_block() for _ in range(num_blocks)]\n",
    "        \n",
    "        # Output weights (logits)\n",
    "        self.output_weights = np.random.randn(num_emb, num_token_vals)\n",
    "\n",
    "    def create_transformer_block(self):\n",
    "        # Each transformer block includes a simple multi-head attention and feed-forward network\n",
    "        return {\n",
    "            'attention_weights': np.random.randn(self.num_emb, self.num_emb),\n",
    "            'feed_forward_weights': np.random.randn(self.num_emb, self.num_neurons),\n",
    "            'feed_forward_output': np.random.randn(self.num_neurons, self.num_emb)\n",
    "        }\n",
    "\n",
    "    def self_attention(self, x):\n",
    "        # Basic attention mechanism\n",
    "        query = np.dot(x, self.transformer_blocks[0]['attention_weights'])\n",
    "        key = np.dot(x, self.transformer_blocks[0]['attention_weights'])\n",
    "        value = np.dot(x, self.transformer_blocks[0]['attention_weights'])\n",
    "        attention_output = np.dot(query, key.T) / np.sqrt(self.num_emb)  # scaled dot-product attention\n",
    "        attention_output = np.dot(attention_output, value)\n",
    "        return attention_output\n",
    "\n",
    "    def feed_forward(self, x):\n",
    "        # Basic feed-forward neural network (FFN)\n",
    "        x = np.dot(x, self.transformer_blocks[0]['feed_forward_weights'])\n",
    "        x = np.maximum(x, 0)  # ReLU activation\n",
    "        x = np.dot(x, self.transformer_blocks[0]['feed_forward_output'])\n",
    "        return x\n",
    "\n",
    "    def transformer_forward(self, x):\n",
    "        x = x + self.positional_embedding[:x.shape[1]]\n",
    "        for block in self.transformer_blocks:\n",
    "            x = self.self_attention(x)\n",
    "            x = self.feed_forward(x)\n",
    "        return x\n",
    "\n",
    "    def sample(self, batch_size=4):\n",
    "        x_seq = np.full((batch_size, 1), self.num_token_vals - 1)  # dummy token\n",
    "        for _ in range(self.num_tokens - 1):\n",
    "            x_log_probs = self.transformer_forward(x_seq)\n",
    "            next_token_probs = np.exp(x_log_probs[:, -1])  # convert logits to probabilities\n",
    "            next_token = np.array([np.random.choice(self.num_token_vals, p=prob / prob.sum()) for prob in next_token_probs])\n",
    "            x_seq = np.concatenate([x_seq, next_token[:, np.newaxis]], axis=1)\n",
    "        return x_seq\n",
    "\n",
    "# Sample Data\n",
    "def create_toy_data(num_samples=1000, num_tokens=50, vocab_size=50):\n",
    "    return np.random.randint(0, vocab_size, size=(num_samples, num_tokens))\n",
    "\n",
    "# Example hyperparameters\n",
    "num_tokens = 50\n",
    "num_token_vals = 50\n",
    "num_emb = 128\n",
    "num_neurons = 256\n",
    "num_heads = 2\n",
    "num_blocks = 6\n",
    "\n",
    "# Initialize the transformer model\n",
    "model = SimpleTransformer(num_tokens, num_token_vals, num_emb, num_neurons, num_heads, num_blocks)\n",
    "\n",
    "# Toy data creation\n",
    "data = create_toy_data()\n",
    "\n",
    "# Sampling new sequences (molecule generation)\n",
    "generated_samples = model.sample(batch_size=4)\n",
    "print(f\"Generated Samples:\\n{generated_samples}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8154e448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Helper function to initialize weights\n",
    "def init_weights(shape):\n",
    "    return [[random.uniform(-0.1, 0.1) for _ in range(shape[1])] for _ in range(shape[0])]\n",
    "\n",
    "# Helper function to apply softmax to a list of numbers\n",
    "def softmax(x):\n",
    "    max_x = max(x)\n",
    "    exp_x = [2.71828**(i - max_x) for i in x]  # Approximation of e^x\n",
    "    sum_exp_x = sum(exp_x)\n",
    "    return [i / sum_exp_x for i in exp_x]\n",
    "\n",
    "# Dot product of two matrices\n",
    "def matmul(A, B):\n",
    "    rows_A = len(A)\n",
    "    cols_A = len(A[0])\n",
    "    cols_B = len(B[0])\n",
    "    result = [[0] * cols_B for _ in range(rows_A)]\n",
    "    for i in range(rows_A):\n",
    "        for j in range(cols_B):\n",
    "            result[i][j] = sum(A[i][k] * B[k][j] for k in range(cols_A))\n",
    "    return result\n",
    "\n",
    "# Transpose a matrix\n",
    "def transpose(M):\n",
    "    return [[M[j][i] for j in range(len(M))] for i in range(len(M[0]))]\n",
    "\n",
    "# Add two matrices element-wise\n",
    "def add(A, B):\n",
    "    return [[A[i][j] + B[i][j] for j in range(len(A[0]))] for i in range(len(A))]\n",
    "\n",
    "# Multi-head attention\n",
    "class MultiHeadAttention:\n",
    "    def __init__(self, num_heads, embed_dim):\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.attention_weights = init_weights((embed_dim, embed_dim))  # Attention weight matrix\n",
    "        self.query_weights = init_weights((embed_dim, embed_dim))  # Query weight matrix\n",
    "        self.key_weights = init_weights((embed_dim, embed_dim))  # Key weight matrix\n",
    "        self.value_weights = init_weights((embed_dim, embed_dim))  # Value weight matrix\n",
    "\n",
    "    def scaled_dot_product_attention(self, query, key, value):\n",
    "        # Scaled Dot-Product Attention\n",
    "        # Calculate attention scores\n",
    "        scores = matmul(query, transpose(key))\n",
    "        scale = len(key) ** 0.5  # Scaling factor\n",
    "        scores = [[score / scale for score in row] for row in scores]\n",
    "        attention_weights = [softmax(row) for row in scores]  # Softmax over the rows\n",
    "\n",
    "        # Multiply attention weights by values\n",
    "        attention_output = matmul(attention_weights, value)\n",
    "        return attention_output\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply attention heads to the input `x`\n",
    "        query = matmul(x, self.query_weights)\n",
    "        key = matmul(x, self.key_weights)\n",
    "        value = matmul(x, self.value_weights)\n",
    "\n",
    "        # Split into heads\n",
    "        head_dim = self.embed_dim // self.num_heads\n",
    "        query = [query[i][:head_dim] for i in range(len(query))]\n",
    "        key = [key[i][:head_dim] for i in range(len(key))]\n",
    "        value = [value[i][:head_dim] for i in range(len(value))]\n",
    "\n",
    "        # Apply scaled dot product attention for each head\n",
    "        attention_outputs = [self.scaled_dot_product_attention(q, k, v) for q, k, v in zip(query, key, value)]\n",
    "\n",
    "        # Concatenate the heads and return the output\n",
    "        return [sum(head, []) for head in zip(*attention_outputs)]  # Merging the heads\n",
    "\n",
    "# Feed Forward Neural Network (FFN) layer\n",
    "class FeedForwardNN:\n",
    "    def __init__(self, embed_dim, hidden_dim):\n",
    "        self.weights1 = init_weights((embed_dim, hidden_dim))\n",
    "        self.weights2 = init_weights((hidden_dim, embed_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Linear layer 1\n",
    "        layer1_output = [sum(x[i][j] * self.weights1[j][k] for j in range(len(x[i]))) for i in range(len(x)) for k in range(len(self.weights1[0]))]\n",
    "        # Apply ReLU activation\n",
    "        relu_output = [max(0, value) for value in layer1_output]\n",
    "        # Linear layer 2\n",
    "        layer2_output = [sum(relu_output[i][j] * self.weights2[j][k] for j in range(len(relu_output[i]))) for i in range(len(relu_output)) for k in range(len(self.weights2[0]))]\n",
    "        return layer2_output\n",
    "\n",
    "# Transformer Block: Multi-Head Attention + FeedForward\n",
    "class TransformerBlock:\n",
    "    def __init__(self, num_heads, embed_dim, hidden_dim):\n",
    "        self.attn = MultiHeadAttention(num_heads, embed_dim)\n",
    "        self.ffn = FeedForwardNN(embed_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply multi-head attention\n",
    "        attention_output = self.attn.forward(x)\n",
    "        # Apply feedforward network\n",
    "        return self.ffn.forward(attention_output)\n",
    "\n",
    "# Example usage\n",
    "# Initialize a transformer block with 2 attention heads, embedding dimension of 4, and hidden dimension of 8\n",
    "transformer = TransformerBlock(num_heads=2, embed_dim=4, hidden_dim=8)\n",
    "\n",
    "# Example input (random matrix of size 3x4, where 3 is sequence length and 4 is embedding dimension)\n",
    "x = [[random.uniform(0, 1) for _ in range(4)] for _ in range(3)]\n",
    "\n",
    "# Forward pass through the transformer block\n",
    "output = transformer.forward(x)\n",
    "print(\"Output from the Transformer Block:\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ae242f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
