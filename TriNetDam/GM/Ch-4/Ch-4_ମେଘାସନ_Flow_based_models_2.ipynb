{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229881fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright (c) 2004 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "667ad062",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)\n",
    "\n",
    "Fig.6 An example of outcomes after the training: (a) Randomly selected real images. (b) Unconditional generations from the RealNVP. (c) The validation curve during training.\n",
    "\n",
    "## Is It All? Really?\n",
    "\n",
    "Yes and no. Yes, in the sense that it is the minimalistic example of an implementation of the RealNVP. No, because there are many improvements over the instance of the RealNVP presented here, namely:\n",
    "\n",
    "- **Factoring out** [7]: During the forward pass (from $ \\mathbf{x} $ to $ \\mathbf{z} $), we can split the variables and proceed with processing only a subset of them. This could help to parameterize the base distribution by using the outputs of intermediate layers. In other words, we can obtain an autoregressive base distribution.\n",
    "\n",
    "  - **Fig.6**: An example of outcomes after the training:\n",
    "    - (a) Randomly selected real images.\n",
    "    - (b) Unconditional generations from the RealNVP.\n",
    "    - (c) The validation curve during training.\n",
    "\n",
    "- **Rezero trick** [11]: Introducing additional parameters to the coupling layer, e.g., \n",
    "\n",
    "  $$\n",
    "  \\mathbf{y}_b = \\exp(\\alpha s(\\mathbf{x}_a)) \\odot \\mathbf{x}_b + \\beta t(\\mathbf{x}_a)\n",
    "  $$\n",
    "\n",
    "  where $ \\alpha $ and $ \\beta $ are initialized with 0’s. This helps to ensure that the transformations act as identity maps in the beginning. It is shown in [12] that this trick helps to learn better transformations by maintaining information about the input through all layers in the beginning of the training process.\n",
    "\n",
    "- **Masking or Checkerboard pattern** [7]: We can use a checkerboard pattern instead of dividing an input into two parts like:\n",
    "\n",
    "  $$\n",
    "  [\\mathbf{x}_{1:D/2}, \\mathbf{x}_{D/2+1:D}]\n",
    "  $$\n",
    "\n",
    "  This encourages learning local statistics better.\n",
    "\n",
    "- **Squeezing** [7]: We can also experiment with “squeezing” some dimensions. For instance, an image consists of $ C $ channels, width $ W $, and height $ H $, which could be turned into $ 4C $ channels, width $ W/2 $, and height $ H/2 $.\n",
    "\n",
    "- **Learnable base distributions**: Instead of using a standard Gaussian base distribution, we can consider another model for that, e.g., an autoregressive model.\n",
    "\n",
    "- **Invertible 1x1 convolution** [8]: A fixed permutation could be replaced with a (learned) invertible 1x1 convolution as in the GLOW model [8].\n",
    "\n",
    "- **Variational dequantization** [13]: We can also pick a different dequantization scheme, e.g., variational dequantization. This allows for much better scores. However, it is not for free, because it leads to a lower bound to the log-likelihood function.\n",
    "\n",
    "### Other Fascinating Research Directions\n",
    "\n",
    "- **Data compression with flows** [14]: Flow-based models are perfect candidates for compression since they allow us to calculate the exact likelihood. [14] proposed a scheme that allows using flows in the bit-back-like compression scheme.\n",
    "\n",
    "- **Conditional flows** [15–17]: Here, we present the unconditional RealNVP. However, we can use a flow-based model for conditional distributions. For instance, we can use the conditioning as an input to the scale network and the translation network.\n",
    "\n",
    "- **Variational inference with flows** [1, 3, 18–21]: Conditional flow-based models could be used to form a flexible family of variational posteriors. Then, the lower bound to the log-likelihood function could be tighter. We will come back to that in Chapter 5, Section 5.4.2.\n",
    "\n",
    "- **Integer discrete flows** [12, 22, 23]: Another interesting direction is a version of the RealNVP for integer-valued data. We will explain this idea in Section 4.2.\n",
    "\n",
    "- **Flows on manifolds** [24]: Typically, flow-based models are considered in the Euclidean space. However, they could be considered in non-Euclidean spaces, resulting in new properties of (partially) invertible transformations.\n",
    "\n",
    "- **Flows for ABC** [25]: Approximate Bayesian computation (ABC) assumes that the posterior over quantities of interest is intractable. One possible approach to mitigate this issue is to approximate it using flow-based models, e.g., masked autoregressive flows [26], as presented in [25].\n",
    "\n",
    "  Much other interesting information on flow-based models could be found in a fantastic review by [27].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9ebfe4",
   "metadata": {},
   "source": [
    "##  ResNet Flows and DenseNet Flows\n",
    "\n",
    "###  ResNet Flows [4, 5]\n",
    "\n",
    "In the previous sections, we discussed flow-based models with predesigned architectures (i.e., blocks consisting of coupling layers and permutation layers) that allow easy calculation of the Jacobian determinant. However, we can take a different approach and think of how we can approximate the Jacobian determinant for an almost arbitrary architecture. Additionally, we explore what kind of requirements must be imposed to make the architecture invertible.\n",
    "\n",
    "In [4], the authors consider widely used residual neural networks (ResNets) and construct an invertible ResNet layer which is only constrained by Lipschitz continuity. A ResNet is defined as:\n",
    "\n",
    "$$\n",
    "\\mathbf{F}(\\mathbf{x}) = \\mathbf{x} + g(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "where $ g $ is modeled by a (convolutional) neural network, and $ \\mathbf{F} $ represents a ResNet layer, which is in general not invertible. However, $ g $ is constructed in such a way that it satisfies the Lipschitz constant being strictly lower than 1:\n",
    "\n",
    "$$\n",
    "\\text{Lip}(g) < 1, \\quad \\text{if} \\quad \\| \\mathbf{W}_i \\|_2 < 1\n",
    "$$\n",
    "\n",
    "where $ \\| \\cdot \\|_2 $ is the $ \\ell_2 $-matrix norm. Then, we have:\n",
    "\n",
    "$$\n",
    "\\text{Lip}(g) = K < 1 \\quad \\text{and} \\quad \\text{Lip}(\\mathbf{F}) < 1 + K\n",
    "$$\n",
    "\n",
    "Only in this specific case, the Banach fixed-point theorem holds, and the ResNet layer $ \\mathbf{F} $ has a unique inverse. As a result, the inverse can be approximated by fixed-point iterations [4].\n",
    "\n",
    "Estimating the log determinant is, especially for high-dimensional spaces, computationally intractable due to expensive computations. Since ResNet blocks have a constrained Lipschitz constant, the logarithm of the Jacobian determinant is cheaper to compute, tractable, and can be approximated with guaranteed convergence:\n",
    "\n",
    "$$\n",
    "\\ln p(\\mathbf{x}) = \\ln p(\\mathbf{f}(\\mathbf{x})) + \\sum_{k=1}^{\\infty} (-1)^{k+1} \\, \\text{tr} \\left( \\mathbf{J}_g(\\mathbf{x})^k \\right)\n",
    "$$\n",
    "\n",
    "where $ \\mathbf{J}_g(\\mathbf{x}) $ is the Jacobian of $ g $ at $ \\mathbf{x} $, and $ \\| \\mathbf{J}_g \\| < 1 $. The Skilling-Hutchinson trace estimator [30, 31] is used to compute the trace at a lower cost than fully computing the trace of the Jacobian.\n",
    "\n",
    "Residual flows [5] use an improved method to estimate the power series at an even lower cost with an unbiased estimator based on “Russian roulette” [32]. Intuitively, the method estimates the infinite sum of the power series by evaluating a finite number of terms. In return, this leads to fewer computations compared to invertible residual networks.\n",
    "\n",
    "To avoid derivative saturation, which occurs when the second derivative is zero in large regions, the **LipSwish** activation is proposed [4].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b277fbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import spectral_norm\n",
    "\n",
    "# ResNet Block Definition\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "\n",
    "        # Define the layers of the ResNet block with spectral normalization applied\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        # Apply spectral normalization directly to the layers\n",
    "        self.fc1 = spectral_norm(self.fc1)\n",
    "        self.fc2 = spectral_norm(self.fc2)\n",
    "        \n",
    "        # Optional activation function (LipSwish in original work)\n",
    "        self.activation = nn.Softplus()  # Using Softplus as a non-linear activation function\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Define the forward pass\n",
    "        residual = x\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # Adding the residual (skip connection)\n",
    "        return x + residual\n",
    "\n",
    "# RealNVP with ResNet Flows\n",
    "class RealNVPResNetFlow(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_flows):\n",
    "        super(RealNVPResNetFlow, self).__init__()\n",
    "\n",
    "        self.num_flows = num_flows\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        # Define the ResNet blocks for the flow\n",
    "        self.resnet_blocks = nn.ModuleList(\n",
    "            [ResNetBlock(input_dim, hidden_dim, input_dim) for _ in range(num_flows)]\n",
    "        )\n",
    "    \n",
    "    def coupling(self, x, block, forward=True):\n",
    "        \"\"\" Coupling layer with ResNet transformation \"\"\"\n",
    "        mid = x.size(1) // 2  # Correct handling of dimensions\n",
    "        xa, xb = x[:, :mid], x[:, mid:]\n",
    "\n",
    "        # Apply ResNet transformation\n",
    "        s, t = block(xa), block(xa)\n",
    "\n",
    "        if forward:\n",
    "            yb = (xb - t) * torch.exp(-s)\n",
    "        else:\n",
    "            yb = torch.exp(s) * xb + t\n",
    "\n",
    "        return torch.cat([xa, yb], dim=-1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass through the normalizing flow model \"\"\"\n",
    "        log_det_J = 0\n",
    "        z = x\n",
    "        for i in range(self.num_flows):\n",
    "            z = self.coupling(z, self.resnet_blocks[i], forward=True)\n",
    "            log_det_J += torch.sum(z)  # A simple log determinant approximation\n",
    "            \n",
    "        return z, log_det_J\n",
    "\n",
    "    def inverse(self, z):\n",
    "        \"\"\" Inverse pass through the normalizing flow model \"\"\"\n",
    "        x = z\n",
    "        for i in reversed(range(self.num_flows)):\n",
    "            x = self.coupling(x, self.resnet_blocks[i], forward=False)\n",
    "        return x\n",
    "\n",
    "# Define a simple prior (standard normal distribution)\n",
    "class StandardNormalPrior(nn.Module):\n",
    "    def forward(self, z):\n",
    "        return -0.5 * torch.sum(z ** 2, dim=-1)\n",
    "\n",
    "# Define the model and optimizer\n",
    "input_dim = 4  # Adjust input dimension to match your data\n",
    "hidden_dim = 128\n",
    "num_flows = 8\n",
    "model = RealNVPResNetFlow(input_dim, hidden_dim, num_flows)\n",
    "prior = StandardNormalPrior()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop (dummy data)\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # Dummy data (batch size of 16 and 4-dimensional input to match input_dim)\n",
    "    x = torch.randn(16, input_dim)\n",
    "\n",
    "    # Forward pass through the model\n",
    "    z, log_det_J = model(x)\n",
    "\n",
    "    # Calculate the log likelihood (negative log likelihood)\n",
    "    log_prob = prior(z)\n",
    "    loss = -(log_prob + log_det_J)\n",
    "\n",
    "    # Backpropagate and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.mean().backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.mean().item():.4f}\")\n",
    "\n",
    "# Generate samples from the model\n",
    "with torch.no_grad():\n",
    "    z_samples = torch.randn(5, input_dim)  # 5 samples from the prior\n",
    "    generated_samples = model.inverse(z_samples)\n",
    "    print(\"Generated Samples:\")\n",
    "    print(generated_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d714860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ResNet Block Definition\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "\n",
    "        # Define the layers of the ResNet block\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        # Optional activation function (ReLU or other)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward pass\n",
    "        residual = x\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # Adding the residual (skip connection)\n",
    "        return x + residual\n",
    "\n",
    "# RealNVP with ResNet Flows\n",
    "class RealNVPResNetFlow(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_flows):\n",
    "        super(RealNVPResNetFlow, self).__init__()\n",
    "\n",
    "        self.num_flows = num_flows\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        # Define the ResNet blocks for the flow\n",
    "        self.resnet_blocks = nn.ModuleList(\n",
    "            [ResNetBlock(input_dim, hidden_dim, input_dim) for _ in range(num_flows)]\n",
    "        )\n",
    "    \n",
    "    def coupling(self, x, block, forward=True):\n",
    "        \"\"\" Coupling layer with ResNet transformation \"\"\"\n",
    "        mid = x.size(1) // 2  # Correct handling of dimensions\n",
    "        xa, xb = x[:, :mid], x[:, mid:]\n",
    "\n",
    "        # Apply ResNet transformation\n",
    "        s, t = block(xa), block(xa)\n",
    "\n",
    "        if forward:\n",
    "            yb = (xb - t) * torch.exp(-s)\n",
    "        else:\n",
    "            yb = torch.exp(s) * xb + t\n",
    "\n",
    "        return torch.cat([xa, yb], dim=-1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass through the normalizing flow model \"\"\"\n",
    "        log_det_J = 0\n",
    "        z = x\n",
    "        for i in range(self.num_flows):\n",
    "            z = self.coupling(z, self.resnet_blocks[i], forward=True)\n",
    "            log_det_J += torch.sum(z)  # A simple log determinant approximation\n",
    "            \n",
    "        return z, log_det_J\n",
    "\n",
    "    def inverse(self, z):\n",
    "        \"\"\" Inverse pass through the normalizing flow model \"\"\"\n",
    "        x = z\n",
    "        for i in reversed(range(self.num_flows)):\n",
    "            x = self.coupling(x, self.resnet_blocks[i], forward=False)\n",
    "        return x\n",
    "\n",
    "# Define a simple prior (standard normal distribution)\n",
    "class StandardNormalPrior(nn.Module):\n",
    "    def forward(self, z):\n",
    "        return -0.5 * torch.sum(z ** 2, dim=-1)\n",
    "\n",
    "# Define the model and optimizer\n",
    "input_dim = 4  # Adjust input dimension to match your data\n",
    "hidden_dim = 128\n",
    "num_flows = 8\n",
    "model = RealNVPResNetFlow(input_dim, hidden_dim, num_flows)\n",
    "prior = StandardNormalPrior()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop (dummy data)\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # Dummy data (batch size of 16 and 4-dimensional input to match input_dim)\n",
    "    x = torch.randn(16, input_dim)\n",
    "\n",
    "    # Forward pass through the model\n",
    "    z, log_det_J = model(x)\n",
    "\n",
    "    # Calculate the log likelihood (negative log likelihood)\n",
    "    log_prob = prior(z)\n",
    "    loss = -(log_prob + log_det_J)\n",
    "\n",
    "    # Backpropagate and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.mean().backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.mean().item():.4f}\")\n",
    "\n",
    "# Generate samples from the model\n",
    "with torch.no_grad():\n",
    "    z_samples = torch.randn(5, input_dim)  # 5 samples from the prior\n",
    "    generated_samples = model.inverse(z_samples)\n",
    "    print(\"Generated Samples:\")\n",
    "    print(generated_samples)\n",
    "#doable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f358a55",
   "metadata": {},
   "source": [
    "##  DenseNet Flows\n",
    "\n",
    "Since it is possible to formulate a flow for a ResNet architecture, a natural question is whether it could be accomplished for densely connected networks (DenseNets) $[33]$. In $[6]$, it was shown that indeed it is possible!\n",
    "\n",
    "The main component of DenseNet flows is a **DenseBlock** that is defined as a function $ F : \\mathbb{R}^d \\to \\mathbb{R}^d $ with:\n",
    "\n",
    "$$ F(x) = x + g(x), $$\n",
    "\n",
    "where $ g $ consists of dense layers:\n",
    "\n",
    "$$ g(x) = h_{n+1} \\circ h_n \\circ \\dots \\circ h_1(x), $$\n",
    "\n",
    "where $ h_{n+1} $ represents a $ 1 \\times 1 $ convolution to match the output size of $ \\mathbb{R}^d $. A layer $ h_i $ consists of two parts concatenated to each other. The upper part is a copy of the input signal, and the lower part consists of the transformed input, where the transformation is a multiplication of (convolutional) weights $ W_i $ with the input signal, followed by a nonlinearity $ \\phi $ having $ \\text{Lip}(\\phi) \\leq 1 $, such as ReLU, ELU, LipSwish, or tanh.\n",
    "\n",
    "As an example, a dense layer $ h_2 $ can be composed as follows:\n",
    "\n",
    "$$\n",
    "h_1(x) = \\left[ x, \\phi(W_1 x) \\right], \\quad h_2(h_1(x)) = \\left[ \\phi(W_2 h_1(x)), \\phi(W_3 h_2(h_1(x))) \\right].\n",
    "$$\n",
    "\n",
    "The DenseNet flows $[6]$ rely on the same techniques for approximating the Jacobian determinant as in the ResNet flows. The main difference between DenseNet flows and ResNet flows lies in normalizing weights, so that the Lipschitz constant of the transformation is smaller than 1, ensuring the transformation is invertible.\n",
    "\n",
    "Formally, to satisfy $ \\text{Lip}(g) < 1 $, we need to enforce $ \\text{Lip}(h_i) < 1 $ for all $ n $ layers, since $ \\text{Lip}(g) \\leq \\text{Lip}(h_{n+1}) \\cdot \\dots \\cdot \\text{Lip}(h_1) $. Therefore, we first need to determine the Lipschitz constant for a dense layer $ h_i $. We know that a function $ f $ is $ K $-Lipschitz if for all points $ v $ and $ w $, the following holds:\n",
    "\n",
    "$$\n",
    "d_Y(f(v), f(w)) \\leq K \\cdot d_X(v, w),\n",
    "$$\n",
    "\n",
    "where we assume that the distance metrics $ d_X = d_Y = d $ are chosen to be the $ \\ell_2 $-norm. Further, let two functions $ f_1 $ and $ f_2 $ be concatenated in $ h $:\n",
    "\n",
    "$$\n",
    "h_v = \\left[ f_1(v), f_2(v) \\right], \\quad h_w = \\left[ f_1(w), f_2(w) \\right].\n",
    "$$\n",
    "\n",
    "We can now find an analytical form to express a limit on $ K $ for the dense layer in the form of Equation (4.27):\n",
    "\n",
    "$$\n",
    "d(h_v, h_w)^2 = d(f_1(v), f_1(w))^2 + d(f_2(v), f_2(w))^2.\n",
    "$$\n",
    "\n",
    "Thus, the Lipschitz constant of layer $ h $ can be expressed as:\n",
    "\n",
    "$$\n",
    "\\text{Lip}(h) = \\sqrt{\\text{Lip}(f_1)^2 + \\text{Lip}(f_2)^2}.\n",
    "$$\n",
    "\n",
    "With spectral normalization of Equation (4.23), we know that we can enforce (convolutional) weights $ W_i $ to be at most 1-Lipschitz. Hence, for all $ n $ dense layers, we apply the spectral normalization on the lower part which locally enforces $ \\text{Lip}(f_2) = K_2 < 1 $. Further, since we enforce each layer $ h_i $ to be at most 1-Lipschitz and we start with $ f_1(x) = x $, we know that $ \\text{Lip}(f_1) = 1 $. Therefore, the Lipschitz constant of an entire layer can be at most:\n",
    "\n",
    "$$\n",
    "\\text{Lip}(h) = \\sqrt{1^2 + 1^2} = \\sqrt{2}.\n",
    "$$\n",
    "\n",
    "By dividing by this limit, we enforce each layer to be at most 1-Lipschitz.\n",
    "\n",
    "To read more about DenseNet flows and further improvements, please see the original paper $[6]$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0e6139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DenseLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A single dense layer which concatenates its input with the transformed input.\n",
    "    This layer includes a 1x1 convolution followed by a non-linearity (ReLU or similar).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(DenseLayer, self).__init__()\n",
    "        # First part of the layer: simply copy the input\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        # Second part of the layer: transform the input through a linear layer followed by non-linearity\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the first transformation (copying the input)\n",
    "        h1 = self.fc1(x)\n",
    "        h1 = self.activation(h1)\n",
    "        \n",
    "        # Apply the second transformation (non-linear transformation)\n",
    "        h2 = self.fc2(h1)\n",
    "        \n",
    "        # Concatenate the input with the transformed signal\n",
    "        return torch.cat([x, h2], dim=-1)  # Concatenate along the feature dimension\n",
    "\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A DenseBlock consisting of multiple dense layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [DenseLayer(input_dim + i * hidden_dim, hidden_dim, output_dim) for i in range(num_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through each DenseLayer and concatenate outputs\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DenseNetFlow(nn.Module):\n",
    "    \"\"\"\n",
    "    The main DenseNetFlow model.\n",
    "    Consists of a DenseBlock with an invertible transformation.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(DenseNetFlow, self).__init__()\n",
    "        self.dense_block = DenseBlock(input_dim, hidden_dim, num_layers, output_dim)\n",
    "        self.fc_out = nn.Linear(input_dim + num_layers * hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the DenseBlock to the input\n",
    "        x = self.dense_block(x)\n",
    "        # Final transformation to match the output dimension\n",
    "        return self.fc_out(x)\n",
    "\n",
    "\n",
    "# Example of using DenseNetFlow with random input\n",
    "input_dim = 4  # Example input dimension\n",
    "hidden_dim = 8  # Size of hidden layers\n",
    "num_layers = 3  # Number of dense layers\n",
    "output_dim = 4  # Output dimension (same as input for invertibility)\n",
    "\n",
    "# Initialize the DenseNet flow model\n",
    "model = DenseNetFlow(input_dim, hidden_dim, num_layers, output_dim)\n",
    "\n",
    "# Example input tensor (batch size of 2)\n",
    "x = torch.randn(2, input_dim)  # Batch size of 2 and input dimension of 4\n",
    "\n",
    "# Forward pass\n",
    "output = model(x)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b4424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        \"\"\"\n",
    "        A Dense Layer that applies a transformation to the input and concatenates it.\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # Initialize weights randomly\n",
    "        self.W1 = self.initialize_weights(input_dim, hidden_dim)\n",
    "        self.W2 = self.initialize_weights(hidden_dim, output_dim)\n",
    "\n",
    "    def initialize_weights(self, in_dim, out_dim):\n",
    "        \"\"\"Initialize weights randomly using a uniform distribution.\"\"\"\n",
    "        return [[random.uniform(-1, 1) for _ in range(out_dim)] for _ in range(in_dim)]\n",
    "\n",
    "    def activation(self, x):\n",
    "        \"\"\"Simple ReLU activation function.\"\"\"\n",
    "        return [max(0, xi) for xi in x]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply first transformation (copy the input)\n",
    "        h1 = self.matmul(x, self.W1)\n",
    "        h1 = self.activation(h1)\n",
    "        \n",
    "        # Apply second transformation\n",
    "        h2 = self.matmul(h1, self.W2)\n",
    "        \n",
    "        # Concatenate the input with the transformed signal\n",
    "        return x + h2  # Concatenate the input and the transformed output\n",
    "    \n",
    "    def matmul(self, A, B):\n",
    "        \"\"\"Matrix multiplication.\"\"\"\n",
    "        result = []\n",
    "        for i in range(len(A)):\n",
    "            result.append(sum(A[i][j] * B[j] for j in range(len(B))))\n",
    "        return result\n",
    "\n",
    "\n",
    "class DenseBlock:\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        \"\"\"\n",
    "        A Dense Block consisting of multiple Dense Layers.\n",
    "        \"\"\"\n",
    "        self.layers = [DenseLayer(input_dim + i * hidden_dim, hidden_dim, output_dim) for i in range(num_layers)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through each Dense Layer\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DenseNetFlow:\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        \"\"\"\n",
    "        The DenseNet Flow model, applying a DenseBlock.\n",
    "        \"\"\"\n",
    "        self.dense_block = DenseBlock(input_dim, hidden_dim, num_layers, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.dense_block.forward(x)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# Initialize the DenseNetFlow model with example dimensions\n",
    "input_dim = 4  # Input dimension\n",
    "hidden_dim = 8  # Hidden dimension\n",
    "num_layers = 3  # Number of layers in DenseBlock\n",
    "output_dim = 4  # Output dimension\n",
    "\n",
    "# Create a DenseNetFlow model\n",
    "model = DenseNetFlow(input_dim, hidden_dim, num_layers, output_dim)\n",
    "\n",
    "# Example input\n",
    "x = [random.uniform(-1, 1) for _ in range(input_dim)]  # Random input vector of size `input_dim`\n",
    "\n",
    "# Forward pass\n",
    "output = model.forward(x)\n",
    "print(\"Output:\", output)\n"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    },
    "image-3.png": {
     "image/png": "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"
    },
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "6f9b7801",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)\n",
    "\n",
    "Fig.7 Examples of: (a) homeomorphic spaces, and (b) non-homeomorphic spaces. The red cross indicates it is impossible invert the transformation.\n",
    "\n",
    "![image-2.png](attachment:image-2.png)\n",
    "Fig.8 An example of “replacing” a ring (in blue) with a ball (in magenta).\n",
    "\n",
    "##  Flows for Discrete Random Variables\n",
    "\n",
    "###  Introduction\n",
    "\n",
    "While discussing flow-based models in the previous section, we presented them as density estimators, namely, models that represent stochastic dependencies among continuous random variables. We introduced the change of variables formula that helps to express a random variable by transforming it using invertible maps (bijections) $ f $ to a random variable with a known probability density function. Formally, it is defined as follows:\n",
    "\n",
    "$$\n",
    "p(x) = p(z = f^{-1}(x)) \\left| J_f(z) \\right|\n",
    "$$\n",
    "\n",
    "where $ J_f(z) $ is the Jacobian of $ f $ at $ z $.\n",
    "\n",
    "However, there are potential issues with such an approach. First of all, in many problems (e.g., image processing), the considered random variables (objects) are discrete. For instance, images typically take values in $ \\{0, 1, \\dots, 255\\} \\subset \\mathbb{Z} $. In order to apply flows, we must apply dequantization [10] that results in a lower bound to the original probability distribution.\n",
    "\n",
    "A continuous space possesses various potential pitfalls. One of them is that if a transformation is a bijection (as in flows), not all continuous deformations are possible. It is tightly connected with topology and, more precisely, homeomorphisms, i.e., a continuous function between topological spaces that has a continuous inverse function, and diffeomorphisms, i.e., invertible functions that map one differentiable manifold to another such that both the function and its inverse are smooth.\n",
    "\n",
    "It is not crucial to know topology, but a curious reader may take a detour and read on that; it is definitely a fascinating field, and I wish to know more about it!\n",
    "\n",
    "Anyway, let us consider three examples. Imagine we want to transform a square into a circle (Fig.7a). It is possible to find a homeomorphism (i.e., a bijection) that turns the square into a circle and back. Imagine you have a hammer and an iron square. If you start hitting the square infinitely many times, you can get an iron circle. Then, you can do it \"backward\" to get the square back. I know, it is unrealistic but hey, we're talking about math here!\n",
    "\n",
    "However, if we consider a line segment and a circle (Fig. 4.7b), the situation is a bit more complicated. It is possible to transform the line segment into a circle, but not the other way around. Why? Because while transforming the circle to the line segment, it is unclear which point of the circle corresponds to the beginning (or the end) of the line segment. That is why we cannot invert the transformation!\n",
    "\n",
    "Another example that I really like, and which is closer to the potential issues of continuous flows, is transforming a ring into a ball as in Fig. 4.8. The goal is to replace the blue ring with the magenta ball. In order to make the transformation bijective, while transforming the blue ring in place of the magenta ball, we must ensure that the new magenta \"ring\" is in fact \"broken\" so that the new blue \"ball\" can get inside! Again, why? If the magenta ring is not broken, then we cannot say how the blue ball got inside, which destroys bijectivity! In the language of topology, it is impossible because the two spaces are non-homeomorphic.\n",
    "\n",
    "#### Fig.7 Examples of:\n",
    "- (a) homeomorphic spaces\n",
    "- (b) non-homeomorphic spaces.\n",
    "\n",
    "The red cross indicates it is impossible to invert the transformation.\n",
    "\n",
    "#### Fig.8 An example of \"replacing\" a ring (in blue) with a ball (in magenta).\n",
    "\n",
    "### Flows for Discrete Random Variables\n",
    "\n",
    "Now, how does this affect flow-based models? I hope that some of you asked this question, or maybe even imagined possible cases where this might hinder learning flows. In general, I would say it is fine, and we should not look for faults where there are none or almost none. However, if you work with flows that require dequantization, then you can spot cases like the one in Fig. 4.9.\n",
    "\n",
    "In this simple example, we have two discrete random variables that after uniform dequantization have two regions with equal probability mass and the remaining two regions with zero probability mass [10]. After training a flow-based model, we have a density estimator that assigns nonzero probability mass where the true distribution has zero density! Moreover, the transformation in the flow must be a bijection; therefore, there is a continuity between the two squares (see Fig. 4.9, right).\n",
    "\n",
    "Where did we see that? Yes, in Fig. 4.8! We must know how to invert the transformation; thus, there must be a \"trace\" of how the probability mass moves between the regions.\n",
    "\n",
    "Again, we can ask ourselves if it is bad. Well, I would say not really, but if we think of a case with more random variables, and there is always some little error here and there, this causes a probability mass leakage that could result in a far-from-perfect model. And overall, the model could err in proper probability assignment.\n",
    "\n",
    "![image-3.png](attachment:image-3.png)\n",
    "Fig.9 An example of uniformly dequantized discrete random variables (left) and a ﬂow-based model (right). Notice that in these examples, the true distribution assigns equal probability mass to the two regions in orange, and zero probability mass to the remaining two regions (in black). However, the ﬂow-based model assigns probability mass outside the original nonzero probability regions.\n",
    "\n",
    "\n",
    "#### Fig.9 An example of uniformly dequantized discrete random variables (left) and a flow-based model (right).\n",
    "Notice that in these examples, the true distribution assigns equal probability mass to the two regions in orange, and zero probability mass to the remaining two regions (in black). However, the flow-based model assigns probability mass outside the original nonzero probability regions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce7e528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class FlowModel(nn.Module):\n",
    "    def __init__(self, num_bins=256, hidden_dim=256):\n",
    "        super(FlowModel, self).__init__()\n",
    "        self.num_bins = num_bins\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Define the normalizing flow layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(num_bins, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, num_bins)\n",
    "            ) for _ in range(4)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        log_det_jacobian = 0\n",
    "        for layer in self.layers:\n",
    "            # Forward pass through the flow layers\n",
    "            x, ldj = layer(x)  # Log-determinant Jacobian (simplified)\n",
    "            log_det_jacobian += ldj\n",
    "        return x, log_det_jacobian\n",
    "    \n",
    "    def inverse(self, y):\n",
    "        # This would typically reverse the flow layers\n",
    "        for layer in reversed(self.layers):\n",
    "            y = layer(y)\n",
    "        return y\n",
    "\n",
    "def dequantize(x, noise_level=0.1):\n",
    "    \"\"\"\n",
    "    Dequantizes discrete random variables by adding uniform noise.\n",
    "    \"\"\"\n",
    "    return x + torch.rand_like(x).float() * noise_level\n",
    "\n",
    "def nll_loss(model, x):\n",
    "    # Dequantize the input data\n",
    "    x_dequantized = dequantize(x)\n",
    "    \n",
    "    # Forward pass through the model\n",
    "    x_transformed, log_det_jacobian = model(x_dequantized)\n",
    "    \n",
    "    # Compute the log-likelihood (negative log-likelihood is the loss)\n",
    "    log_prob = -0.5 * torch.sum(x_transformed ** 2, dim=1)  # Gaussian assumption\n",
    "    loss = -torch.mean(log_prob + log_det_jacobian)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "model = FlowModel(num_bins=256, hidden_dim=256)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Generate some synthetic discrete data\n",
    "num_samples = 10000\n",
    "data = torch.randint(0, 256, (num_samples, 1), dtype=torch.float32)  # Random integers between 0 and 255\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Calculate the negative log-likelihood loss\n",
    "    loss = nll_loss(model, data)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Sampling from the trained flow model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sampled_data = model.inverse(torch.randn(1000, 1))  # Sample from standard normal distribution\n",
    "    \n",
    "    # Plot the histogram of the sampled data\n",
    "    plt.hist(sampled_data.numpy(), bins=50, density=True)\n",
    "    plt.title(\"Histogram of Sampled Data\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48b86ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100, Loss: 29085626890.91003\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class FlowModel:\n",
    "    def __init__(self, num_bins=256, hidden_dim=256):\n",
    "        self.num_bins = num_bins\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layers = []\n",
    "        \n",
    "        # Initialize the model's layers\n",
    "        for _ in range(4):\n",
    "            self.layers.append(self.create_layer())\n",
    "    \n",
    "    def create_layer(self):\n",
    "        # Each layer will be represented by a random linear transformation matrix for simplicity\n",
    "        return {'W': [[random.uniform(-1, 1) for _ in range(self.hidden_dim)] for _ in range(self.num_bins)],\n",
    "                'b': [random.uniform(-1, 1) for _ in range(self.hidden_dim)]}\n",
    "    \n",
    "    def apply_layer(self, x, layer):\n",
    "        # Apply a simple linear transformation followed by ReLU activation\n",
    "        return [max(0, sum(x_i * layer['W'][i][j] for i, x_i in enumerate(x)) + layer['b'][j]) for j in range(self.hidden_dim)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        log_det_jacobian = 0\n",
    "        for layer in self.layers:\n",
    "            # Forward pass through the layers\n",
    "            x = self.apply_layer(x, layer)\n",
    "        return x, log_det_jacobian\n",
    "    \n",
    "    def inverse(self, y):\n",
    "        for layer in reversed(self.layers):\n",
    "            # Inverse pass through the layers (simplified for demonstration)\n",
    "            y = self.apply_layer(y, layer)\n",
    "        return y\n",
    "\n",
    "def dequantize(x, noise_level=0.1):\n",
    "    \"\"\"\n",
    "    Dequantizes discrete random variables by adding uniform noise.\n",
    "    \"\"\"\n",
    "    return [xi + random.uniform(0, noise_level) for xi in x]\n",
    "\n",
    "def nll_loss(model, x):\n",
    "    # Dequantize the input data\n",
    "    x_dequantized = dequantize(x)\n",
    "    \n",
    "    # Forward pass through the model\n",
    "    x_transformed, log_det_jacobian = model.forward(x_dequantized)\n",
    "    \n",
    "    # Compute the log-likelihood (negative log-likelihood is the loss)\n",
    "    log_prob = -0.5 * sum(xi ** 2 for xi in x_transformed)  # Gaussian assumption\n",
    "    loss = -log_prob + log_det_jacobian\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Initialize the model\n",
    "model = FlowModel(num_bins=256, hidden_dim=256)\n",
    "\n",
    "# Generate synthetic discrete data\n",
    "num_samples = 10000\n",
    "data = [[random.randint(0, 255)] for _ in range(num_samples)]  # Random integers between 0 and 255\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Calculate the negative log-likelihood loss for each sample\n",
    "    for x in data:\n",
    "        loss = nll_loss(model, x)\n",
    "        total_loss += loss\n",
    "    \n",
    "    average_loss = total_loss / len(data)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}/{epochs}, Loss: {average_loss}\")\n",
    "\n",
    "# Sampling from the trained flow model\n",
    "sampled_data = []\n",
    "for _ in range(1000):\n",
    "    sample = [random.gauss(0, 1)]  # Sample from standard normal distribution\n",
    "    sampled_data.append(model.inverse(sample))\n",
    "\n",
    "# Plot the histogram of the sampled data\n",
    "plt.hist([sample[0] for sample in sampled_data], bins=50, density=True)\n",
    "plt.title(\"Histogram of Sampled Data\")\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    },
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "c9f60646",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)\n",
    "Fig.10 An example of a discrete ﬂow for two binary random variables. Colors represent various probabilities (i.e., the sum of all squares is 1).\n",
    "\n",
    "![image-2.png](attachment:image-2.png)\n",
    "\n",
    "Fig.11 An example of a discrete ﬂow for two binary random variables but in the extended space. Colors represent various probabilities (i.e., the sum of all squares is 1).\n",
    "## Flows for Discrete Random Variables\n",
    "\n",
    "##  Flows in $ \\mathbb{R} $ or Maybe Rather in $ \\mathbb{Z} $?\n",
    "\n",
    "Before we consider any specific cases and discuss discrete flows, first we need to answer whether there is a change of variables formula for discrete random variables. The answer, fortunately, is **yes**! Let us consider $ x \\in X_D $, where $ X $ is a discrete space, e.g., $ X = \\{0, 1\\} $ or $ X = \\mathbb{Z} $. Then the change of variables takes the following form:\n",
    "\n",
    "$$\n",
    "p(x) = \\pi\\left( z_0 = f^{-1}(x) \\right)\n",
    "$$\n",
    "\n",
    "where $ f $ is an invertible transformation, and $ \\pi(\\cdot) $ is a base distribution. \n",
    "\n",
    "Immediately we can spot a “missing” Jacobian determinant. This is correct! Why? Because now we live in a discrete world where the probability mass is assigned to points that are “shapeless,” and the bijection cannot change the volume. Thus, the Jacobian determinant is always equal to 1! That seems to be good news, isn’t it?\n",
    "\n",
    "We can take any bijective transformations, and we do not need to bother about the Jacobian. That is obviously true; however, we need to remember that the output of the transformation must still be discrete, i.e., $ z \\in X_D $. As a result, we cannot use any arbitrary invertible neural network. \n",
    "\n",
    "Let us discuss the **expressivity** of discrete flows. Suppose we have an invertible transformation $ f: X_D \\to X_D $. Moreover, assume $ X = \\{0, 1\\} $. As noted by [27], a discrete flow can only permute probability masses. Since there is no Jacobian (or, rather, the Jacobian determinant is equal to 1), there is no chance to decrease or increase the probability for specific values.\n",
    "\n",
    "We can think of it like this: the space is the Rubik's cube, and your hands are the flows. If you record your moves, you can always play the video backward, so it is invertible. However, you can only shuffle the colors around! As a result, we do not gain anything by applying the discrete flow, and learning the discrete flow is equivalent to learning the base distribution $ \\pi $. \n",
    "\n",
    "But, as pointed out by [12], the situation looks different if we consider an **extended space** (or infinite space like $ \\mathbb{Z} $). The discrete flow can still only shuffle the probabilities, but now it can reorganize them in such a way that the probabilities can be **factorized**!\n",
    "\n",
    "In other words, it can help the base distribution to be a product of marginals:\n",
    "\n",
    "$$\n",
    "\\pi(z) = \\prod_{d=1}^D \\pi_d(z_d | \\theta_d)\n",
    "$$\n",
    "\n",
    "and the dependencies among variables are now encoded in the invertible transformations.\n",
    "\n",
    "An example of this case is presented below:\n",
    "\n",
    "\n",
    "\n",
    "We refer to [12] for a more thorough discussion with an appropriate lemma. \n",
    "\n",
    "This is amazing information! It means that building a flow-based model in the discrete space makes sense.\n",
    "\n",
    "## Integer Discrete Flows\n",
    "\n",
    "We now know that it makes sense to work with discrete flows, and they are flexible as long as we use extended spaces or infinite spaces like $ \\mathbb{Z} $. However, the question is: **How to formulate an invertible transformation (or rather an invertible neural network) that will output discrete values?**\n",
    "\n",
    "Hoogeboom et al. [22] proposed focusing on integers, since they can be seen as discretized continuous values. As such, we consider **coupling layers** [7] and modify them accordingly.\n",
    "\n",
    "Let us remind ourselves the definition of bipartite coupling layers for $ x \\in \\mathbb{R}^D $:\n",
    "\n",
    "$$\n",
    "y_a = x_a\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_b = \\exp(s(x_a)) \\odot x_b + t(x_a)\n",
    "$$\n",
    "\n",
    "where $ s(\\cdot) $ and $ t(\\cdot) $ are arbitrary neural networks called scaling and transition, respectively.\n",
    "\n",
    "Considering integer-valued variables, $ x \\in \\mathbb{Z}^D $ requires modifying this transformation. First, using scaling might be troublesome because multiplying by integers is still possible, but when we invert the transformation, we divide by integers. Dividing an integer by an integer does not necessarily result in an integer. Therefore, we must remove scaling just in case. \n",
    "\n",
    "Second, we use an arbitrary neural network for the transition. However, this network must return integers! [22] utilize a relatively simple trick: they say that we can **round** the output of $ t(\\cdot) $ to the closest integer. As a result, we add (in the forward pass) or subtract (in the inverse pass) integers from integers, which is perfectly fine (the outcome is still integer-valued).\n",
    "\n",
    "Eventually, we get the following bipartite coupling layer for integer-valued variables:\n",
    "\n",
    "$$\n",
    "y_a = x_a\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_b = \\text{round}(t(x_a)) + x_b\n",
    "$$\n",
    "\n",
    "This completes the modification for integer-valued transformations in discrete spaces. \n",
    "\n",
    "---\n",
    "**References:**\n",
    "- Hoogeboom, et al. [22]\n",
    "- [12] and [27] for further details on discrete flows in extended spaces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad986ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class DiscreteFlowModel:\n",
    "    def __init__(self, input_dim=1, hidden_dim=8):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layers = []\n",
    "        \n",
    "        # Initialize the model's layers (using random integer-valued weights and biases)\n",
    "        for _ in range(2):  # Two layers for simplicity\n",
    "            self.layers.append(self.create_layer())\n",
    "    \n",
    "    def create_layer(self):\n",
    "        # Each layer will consist of a random integer transformation (s and t)\n",
    "        return {\n",
    "            's': lambda x: [random.randint(-1, 1) for _ in range(self.input_dim)],  # scaling (just random integers for simplicity)\n",
    "            't': lambda x: [random.randint(-5, 5) for _ in range(self.input_dim)]  # transition (also random integers)\n",
    "        }\n",
    "    \n",
    "    def apply_layer(self, x, layer):\n",
    "        # Apply the bipartite coupling layer transformation\n",
    "        y_a = x[:len(x) // 2]  # Leave the first half unchanged\n",
    "        y_b = [xi + t for xi, t in zip(x[len(x) // 2:], layer['t'](x))]  # Apply the transition to the second half (with rounding)\n",
    "        \n",
    "        # Concatenate the transformed parts\n",
    "        y = y_a + y_b\n",
    "        return y\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Pass through the layers\n",
    "        for layer in self.layers:\n",
    "            x = self.apply_layer(x, layer)\n",
    "        return x\n",
    "    \n",
    "    def inverse(self, y):\n",
    "        # Inverse pass (applying the layers in reverse order)\n",
    "        for layer in reversed(self.layers):\n",
    "            y = self.apply_layer(y, layer)\n",
    "        return y\n",
    "    \n",
    "    def sample(self, num_samples=10):\n",
    "        # Sample from the model by generating random integers (simple approach)\n",
    "        return [[random.randint(0, 10) for _ in range(self.input_dim)] for _ in range(num_samples)]\n",
    "\n",
    "\n",
    "# Test the DiscreteFlowModel\n",
    "def test_discrete_flow():\n",
    "    model = DiscreteFlowModel(input_dim=4, hidden_dim=8)  # 4-dimensional input, 8 hidden units per layer\n",
    "\n",
    "    # Generate some synthetic data\n",
    "    data = [[random.randint(0, 10) for _ in range(4)] for _ in range(100)]\n",
    "\n",
    "    # Forward pass through the model\n",
    "    transformed_data = []\n",
    "    for sample in data:\n",
    "        transformed_data.append(model.forward(sample))\n",
    "\n",
    "    # Plot the original vs. transformed data (just for demonstration, plot first dimension)\n",
    "    plt.hist([sample[0] for sample in data], bins=10, alpha=0.5, label=\"Original Data\")\n",
    "    plt.hist([transformed[0] for transformed in transformed_data], bins=10, alpha=0.5, label=\"Transformed Data\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Histogram of Original vs. Transformed Data\")\n",
    "    plt.show()\n",
    "\n",
    "    # Sampling new data points\n",
    "    sampled_data = model.sample(num_samples=10)\n",
    "    print(f\"Sampled Data: {sampled_data}\")\n",
    "\n",
    "# Run the test\n",
    "test_discrete_flow()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4012ed",
   "metadata": {},
   "source": [
    "# Discrete Flow-based Model with Quadripartite Coupling Layer and STE\n",
    "\n",
    "In this section, we discuss the **quadripartite coupling layer** and its implementation in a discrete flow-based model. We also use the **straight-through estimator (STE)** to handle the rounding operation in the forward pass while maintaining differentiability in the backward pass.\n",
    "\n",
    "## Straight-Through Estimator (STE) for Rounding\n",
    "\n",
    "Since rounding is a non-differentiable operation, we use the **straight-through estimator (STE)**, which allows us to compute gradients even with a non-differentiable function like rounding. The STE works by applying the rounding operation during the forward pass and using the original input in the backward pass to compute gradients.\n",
    "\n",
    "### STE Implementation in PyTorch\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "class RoundStraightThrough(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Implements the rounding operation with the straight-through estimator.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        Forward pass: applies rounding in the forward pass.\n",
    "        \"\"\"\n",
    "        rounded = torch.round(input)  # Round the input\n",
    "        return rounded\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass: passes the gradient through as-is (STE).\n",
    "        \"\"\"\n",
    "        grad_input = grad_output.clone()  # Gradient is passed through without modification\n",
    "        return grad_input\n",
    "\n",
    "    \n",
    "    # Proposition 4.1 and Quadripartite Coupling Layer Transformation\n",
    "\n",
    "## Proposition 4.1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2021609c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a01121b8",
   "metadata": {},
   "source": [
    "Let $ x, y \\in X $. If binary transformations $ \\circ $ and $ \\sqsubset $ have inverses $ \\bullet $ and $ \\nabla $, respectively, and $ g_2, \\dots, g_D $ and $ f_1, \\dots, f_D $ are arbitrary functions, where $ g_i: X_{1:i-1} \\rightarrow X_i $, and $ f_i: X_{1:i-1} \\times X_{i+1:D} \\rightarrow X_i $, then the following transformation from $ x $ to $ y $ is invertible:\n",
    "\n",
    "$$\n",
    "y_1 = x_1 \\circ f_1(\\emptyset, x_{2:D})\n",
    "$$\n",
    "$$\n",
    "y_2 = (g_2(y_1) \\sqsubset x_2) \\circ f_2(y_1, x_{3:D})\n",
    "$$\n",
    "$$\n",
    "\\vdots\n",
    "$$\n",
    "$$\n",
    "y_d = (g_d(y_{1:d-1}) \\sqsubset x_d) \\circ f_d(y_{1:d-1}, x_{d+1:D})\n",
    "$$\n",
    "$$\n",
    "\\vdots\n",
    "$$\n",
    "$$\n",
    "y_D = (g_D(y_{1:D-1}) \\sqsubset x_D) \\circ f_D(y_{1:D-1}, \\emptyset)\n",
    "$$\n",
    "\n",
    "### Proof of Invertibility\n",
    "\n",
    "To reverse $ y $ to $ x $, we start from the last element $ y_D $ and obtain:\n",
    "\n",
    "$$\n",
    "x_D = g_D(y_{1:D-1}) \\nabla (y_D \\bullet f_D(y_{1:D-1}, \\emptyset))\n",
    "$$\n",
    "\n",
    "Then, we proceed with the next expressions in decreasing order (from $ D-1 $ to 1) to eventually obtain:\n",
    "\n",
    "$$\n",
    "x_{D-1} = g_{D-1}(y_{1:D-2}) \\nabla (y_{D-1} \\bullet f_{D-1}(y_{1:D-2}, x_D))\n",
    "$$\n",
    "$$\n",
    "\\vdots\n",
    "$$\n",
    "$$\n",
    "x_2 = g_2(y_1) \\nabla (y_2 \\bullet f_2(y_1, x_{3:D}))\n",
    "$$\n",
    "$$\n",
    "x_1 = y_1 \\bullet f_1(\\emptyset, x_{2:D})\n",
    "$$\n",
    "\n",
    "This completes the proof of the invertibility of the transformation. $ \\square $\n",
    "\n",
    "## Quadripartite Coupling Layer Transformation\n",
    "\n",
    "For instance, we can divide $ x $ into four parts, $ x = [x_a, x_b, x_c, x_d] $, and the following transformation (a quadripartite coupling layer) is invertible:\n",
    "\n",
    "$$\n",
    "y_a = x_a + \\lfloor t(x_b, x_c, x_d) \\rfloor\n",
    "$$\n",
    "$$\n",
    "y_b = x_b + \\lfloor t(y_a, x_c, x_d) \\rfloor\n",
    "$$\n",
    "$$\n",
    "y_c = x_c + \\lfloor t(y_a, y_b, x_d) \\rfloor\n",
    "$$\n",
    "$$\n",
    "y_d = x_d + \\lfloor t(y_a, y_b, y_c) \\rfloor\n",
    "$$\n",
    "\n",
    "This new invertible transformation can be seen as a form of **autoregressive processing**, since $ y_a $ is used to calculate $ y_b $, and then both $ y_a $ and $ y_b $ are used for calculating $ y_c $, and so on. Thus, each transformation builds upon the previous one, making this a **sequential transformation**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92fefaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Straight-Through Estimator (STE) for the Rounding Operator\n",
    "class RoundStraightThrough(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        rounded = torch.round(input)  # Round the input during the forward pass\n",
    "        return rounded\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_input = grad_output.clone()  # Pass the gradient through unchanged during the backward pass\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "# Transition Network (a simple MLP) for computing the transformation\n",
    "class TransitionNetwork(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(TransitionNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)  # Output one scalar for the transformation\n",
    "\n",
    "    def forward(self, *inputs):\n",
    "        # Concatenate inputs along the feature dimension\n",
    "        x = torch.cat(inputs, dim=-1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        t = self.fc3(x)\n",
    "        return t\n",
    "\n",
    "\n",
    "# Quadripartite Coupling Layer Implementation\n",
    "class QuadripartiteCouplingLayer(nn.Module):\n",
    "    def __init__(self, transition_network):\n",
    "        super(QuadripartiteCouplingLayer, self).__init__()\n",
    "        self.transition_network = transition_network  # Neural network for the transition\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Split x into four parts\n",
    "        xa, xb, xc, xd = torch.chunk(x, 4, dim=-1)\n",
    "\n",
    "        # Apply the transformations\n",
    "        ta = self.transition_network(xb, xc, xd)\n",
    "        ya = xa + RoundStraightThrough.apply(ta)  # Apply rounding with STE\n",
    "\n",
    "        tb = self.transition_network(ya, xc, xd)\n",
    "        yb = xb + RoundStraightThrough.apply(tb)\n",
    "\n",
    "        tc = self.transition_network(ya, yb, xd)\n",
    "        yc = xc + RoundStraightThrough.apply(tc)\n",
    "\n",
    "        td = self.transition_network(ya, yb, yc)\n",
    "        yd = xd + RoundStraightThrough.apply(td)\n",
    "\n",
    "        # Concatenate the transformed parts\n",
    "        y = torch.cat([ya, yb, yc, yd], dim=-1)\n",
    "        return y\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example input (batch_size=2, feature_dim=4)\n",
    "    x = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]], dtype=torch.float32)\n",
    "\n",
    "    # Define transition network\n",
    "    transition_network = TransitionNetwork(input_dim=3)  # 3 inputs for each transition function\n",
    "\n",
    "    # Define coupling layer\n",
    "    coupling_layer = QuadripartiteCouplingLayer(transition_network)\n",
    "\n",
    "    # Apply coupling layer\n",
    "    y = coupling_layer(x)\n",
    "    print(\"Transformed y:\", y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bdbecb",
   "metadata": {},
   "source": [
    "## Integer Discrete Flows (IDF) Model\n",
    "\n",
    "In this section, we define the Integer Discrete Flow (IDF) model using two types of coupling layers. The first option is a bipartite coupling layer, as introduced in Hoogeboom et al. (2019), and the second option is a new coupling layer with four parts, as proposed in Tomczak (2021). We will focus on the second option, which explicitly divides the transformation into four parts.\n",
    "\n",
    "### Discretized Logistic Distribution\n",
    "\n",
    "The base distribution in the model is based on a discretized logistic distribution, defined as follows:\n",
    "\n",
    "$$\n",
    "\\pi(z) = \\sigma\\left(\\frac{z + 0.5 - \\mu}{\\nu}\\right) - \\sigma\\left(\\frac{z - 0.5 - \\mu}{\\nu}\\right)\n",
    "$$\n",
    "\n",
    "where $\\mu \\in \\mathbb{R}$ is the mean and $\\nu > 0$ is the scale. The function $\\sigma(\\cdot)$ is the sigmoid function, and the discretized logistic distribution represents the probability mass in bins of length 1. This distribution is used to model integer-valued flows, and we can extend it to a mixture of discretized logistic distributions for better performance.\n",
    "\n",
    "### Logarithm of the Discretized Logistic Distribution\n",
    "\n",
    "We define the logarithm of the discretized logistic distribution as follows:\n",
    "\n",
    "$$\n",
    "\\log p(x) = \\log \\left( \\sigma\\left(\\frac{z + 0.5 - \\mu}{\\nu}\\right) - \\sigma\\left(\\frac{z - 0.5 - \\mu}{\\nu}\\right) \\right)\n",
    "$$\n",
    "\n",
    "This can be implemented in Python as:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def log_integer_probability(x, mean, logscale):\n",
    "    scale = torch.exp(logscale)\n",
    "    logp = log_min_exp(\n",
    "        F.logsigmoid((x + 0.5 - mean) / scale),\n",
    "        F.logsigmoid((x - 0.5 - mean) / scale)\n",
    "    )\n",
    "    return logp\n",
    "class IDF(nn.Module):\n",
    "    def __init__(self, netts, num_flows, D=2):\n",
    "        super(IDF, self).__init__()\n",
    "        print('IDF by JT.')\n",
    "        # Define the number of layers and the flow network\n",
    "        self.num_flows = num_flows\n",
    "        self.D = D  # Number of variables\n",
    "        self.netts = netts  # Network for the transformations\n",
    "        self.flows = nn.ModuleList([QuadripartiteCouplingLayer(self.netts) for _ in range(self.num_flows)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for flow in self.flows:\n",
    "            x = flow(x)\n",
    "        return x\n",
    "# Example of defining and using the IDF model\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the transition network\n",
    "    transition_network = TransitionNetwork(input_dim=3)\n",
    "    \n",
    "    # Define the IDF model\n",
    "    num_flows = 2  # Number of flows to apply\n",
    "    idf_model = IDF(netts=transition_network, num_flows=num_flows, D=4)\n",
    "    \n",
    "    # Example input (batch_size=2, feature_dim=4)\n",
    "    x = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]], dtype=torch.float32)\n",
    "    \n",
    "    # Apply IDF model\n",
    "    y = idf_model(x)\n",
    "    print(\"Transformed output y:\", y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4b3810",
   "metadata": {},
   "source": [
    "## Integer Discrete Flows (IDF): Full Implementation\n",
    "\n",
    "This section provides the complete implementation of the Integer Discrete Flows (IDF) model. Two options for coupling layers are considered:\n",
    "1. **Option 1**: The bipartite coupling layer as proposed by Hoogeboom et al. (2019).\n",
    "2. **Option 2**: The quadripartite coupling layer as introduced by Tomczak (2021), which splits the input into four parts.\n",
    "\n",
    "### IDF Class Implementation\n",
    "\n",
    "The class includes:\n",
    "1. Initialization of parameters and networks.\n",
    "2. Coupling layers for forward and inverse transformations.\n",
    "3. Permutation layers for variable reordering.\n",
    "4. Methods for sampling and calculating log-probability.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class IDF(nn.Module):\n",
    "    def __init__(self, netts, num_flows, D=2):\n",
    "        super(IDF, self).__init__()\n",
    "        print('IDF Initialized.')\n",
    "\n",
    "        # Choose between one translation network or four networks\n",
    "        if len(netts) == 1:\n",
    "            self.t = nn.ModuleList([netts[0]() for _ in range(num_flows)])\n",
    "            self.idf_git = 1\n",
    "        elif len(netts) == 4:\n",
    "            self.t_a = nn.ModuleList([netts[0]() for _ in range(num_flows)])\n",
    "            self.t_b = nn.ModuleList([netts[1]() for _ in range(num_flows)])\n",
    "            self.t_c = nn.ModuleList([netts[2]() for _ in range(num_flows)])\n",
    "            self.t_d = nn.ModuleList([netts[3]() for _ in range(num_flows)])\n",
    "            self.idf_git = 4\n",
    "        else:\n",
    "            raise ValueError('Provide either 1 or 4 translation networks.')\n",
    "\n",
    "        self.num_flows = num_flows\n",
    "        self.round = RoundStraightThrough.apply  # Rounding operator\n",
    "\n",
    "        # Parameters for base distribution (mean and log-scale)\n",
    "        self.mean = nn.Parameter(torch.zeros(1, D))\n",
    "        self.logscale = nn.Parameter(torch.ones(1, D))\n",
    "\n",
    "        self.D = D\n",
    "\n",
    "    def coupling(self, x, index, forward=True):\n",
    "        if self.idf_git == 1:\n",
    "            xa, xb = torch.chunk(x, 2, dim=1)\n",
    "            if forward:\n",
    "                yb = xb + self.round(self.t[index](xa))\n",
    "            else:\n",
    "                yb = xb - self.round(self.t[index](xa))\n",
    "            return torch.cat((xa, yb), dim=1)\n",
    "\n",
    "        elif self.idf_git == 4:\n",
    "            xa, xb, xc, xd = torch.chunk(x, 4, dim=1)\n",
    "            if forward:\n",
    "                ya = xa + self.round(self.t_a[index](torch.cat((xb, xc, xd), dim=1)))\n",
    "                yb = xb + self.round(self.t_b[index](torch.cat((ya, xc, xd), dim=1)))\n",
    "                yc = xc + self.round(self.t_c[index](torch.cat((ya, yb, xd), dim=1)))\n",
    "                yd = xd + self.round(self.t_d[index](torch.cat((ya, yb, yc), dim=1)))\n",
    "            else:\n",
    "                yd = xd - self.round(self.t_d[index](torch.cat((xa, xb, xc), dim=1)))\n",
    "                yc = xc - self.round(self.t_c[index](torch.cat((xa, xb, yd), dim=1)))\n",
    "                yb = xb - self.round(self.t_b[index](torch.cat((xa, yc, yd), dim=1)))\n",
    "                ya = xa - self.round(self.t_a[index](torch.cat((yb, yc, yd), dim=1)))\n",
    "            return torch.cat((ya, yb, yc, yd), dim=1)\n",
    "\n",
    "    def permute(self, x):\n",
    "        return x.flip(1)  # Simple flip operation for permutation\n",
    "\n",
    "    def f(self, x):\n",
    "        z = x\n",
    "        for i in range(self.num_flows):\n",
    "            z = self.coupling(z, i, forward=True)\n",
    "            z = self.permute(z)\n",
    "        return z\n",
    "\n",
    "    def f_inv(self, z):\n",
    "        x = z\n",
    "        for i in reversed(range(self.num_flows)):\n",
    "            x = self.permute(x)\n",
    "            x = self.coupling(x, i, forward=False)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, reduction='avg'):\n",
    "        z = self.f(x)\n",
    "        log_p = -self.log_prior(z)\n",
    "        return log_p.mean() if reduction == 'avg' else log_p.sum()\n",
    "\n",
    "    def sample(self, batchSize, intMax=100):\n",
    "        z = self.prior_sample(batchSize=batchSize, D=self.D, intMax=intMax)\n",
    "        x = self.f_inv(z)\n",
    "        return x.view(batchSize, 1, self.D)\n",
    "\n",
    "    def log_prior(self, x):\n",
    "        log_p = log_integer_probability(x, self.mean, self.logscale)\n",
    "        return log_p.sum(1)\n",
    "\n",
    "    def prior_sample(self, batchSize, D=2):\n",
    "        y = torch.rand(batchSize, self.D)\n",
    "        x = torch.exp(self.logscale) * torch.log(y / (1. - y)) + self.mean\n",
    "        return torch.round(x)\n",
    "D = 4  # Number of features\n",
    "M = 128  # Hidden layer size\n",
    "num_flows = 8  # Number of transformations\n",
    "\n",
    "# Option 1: Single network\n",
    "if idf_git == 1:\n",
    "    nett = lambda: nn.Sequential(\n",
    "        nn.Linear(D // 2, M), nn.LeakyReLU(),\n",
    "        nn.Linear(M, M), nn.LeakyReLU(),\n",
    "        nn.Linear(M, D // 2)\n",
    "    )\n",
    "    netts = [nett]\n",
    "\n",
    "# Option 2: Four networks\n",
    "elif idf_git == 4:\n",
    "    nett_a = lambda: nn.Sequential(\n",
    "        nn.Linear(3 * (D // 4), M), nn.LeakyReLU(),\n",
    "        nn.Linear(M, M), nn.LeakyReLU(),\n",
    "        nn.Linear(M, D // 4)\n",
    "    )\n",
    "    nett_b = lambda: nn.Sequential(\n",
    "        nn.Linear(3 * (D // 4), M), nn.LeakyReLU(),\n",
    "        nn.Linear(M, M), nn.LeakyReLU(),\n",
    "        nn.Linear(M, D // 4)\n",
    "    )\n",
    "    nett_c = lambda: nn.Sequential(\n",
    "        nn.Linear(3 * (D // 4), M), nn.LeakyReLU(),\n",
    "        nn.Linear(M, M), nn.LeakyReLU(),\n",
    "        nn.Linear(M, D // 4)\n",
    "    )\n",
    "    nett_d = lambda: nn.Sequential(\n",
    "        nn.Linear(3 * (D // 4), M), nn.LeakyReLU(),\n",
    "        nn.Linear(M, M), nn.LeakyReLU(),\n",
    "        nn.Linear(M, D // 4)\n",
    "    )\n",
    "    netts = [nett_a, nett_b, nett_c, nett_d]\n",
    "\n",
    "# Initialize IDF model\n",
    "model = IDF(netts, num_flows, D=D)\n",
    "from torchinfo import summary\n",
    "summary(model, input_data=torch.zeros(1, D), show_input=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4116e1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF Initialized.\n",
      "IDF Initialized.\n",
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "import torch\n",
    "\n",
    "def log_integer_probability(x, mean, logscale):\n",
    "    \"\"\"\n",
    "    Calculate the log probability of x under a logistic distribution.\n",
    "    Args:\n",
    "        x: Input tensor.\n",
    "        mean: Mean of the distribution.\n",
    "        logscale: Log-scale of the distribution.\n",
    "    Returns:\n",
    "        Log-probabilities for each element in x.\n",
    "    \"\"\"\n",
    "    scale = torch.exp(logscale)\n",
    "    z = (x - mean) / scale\n",
    "    log_p = -z - 2 * torch.log(1 + torch.exp(-z)) - logscale\n",
    "    return log_p\n",
    "\n",
    "# Define RoundStraightThrough\n",
    "class RoundStraightThrough(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        return torch.round(input)  # Forward pass uses the standard rounding function\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Straight-through estimator: pass gradients unchanged\n",
    "        return grad_output\n",
    "\n",
    "# Use it in your IDF model\n",
    "\n",
    "class IDF(nn.Module):\n",
    "    def __init__(self, netts, num_flows, D=2):\n",
    "        super(IDF, self).__init__()\n",
    "        print('IDF Initialized.')\n",
    "\n",
    "        # Choose between one translation network or four networks\n",
    "        if len(netts) == 1:\n",
    "            self.t = nn.ModuleList([netts[0]() for _ in range(num_flows)])\n",
    "            self.idf_git = 1\n",
    "        elif len(netts) == 4:\n",
    "            self.t_a = nn.ModuleList([netts[0]() for _ in range(num_flows)])\n",
    "            self.t_b = nn.ModuleList([netts[1]() for _ in range(num_flows)])\n",
    "            self.t_c = nn.ModuleList([netts[2]() for _ in range(num_flows)])\n",
    "            self.t_d = nn.ModuleList([netts[3]() for _ in range(num_flows)])\n",
    "            self.idf_git = 4\n",
    "        else:\n",
    "            raise ValueError('Provide either 1 or 4 translation networks.')\n",
    "\n",
    "        self.num_flows = num_flows\n",
    "        self.round = RoundStraightThrough.apply  # Rounding operator\n",
    "\n",
    "        # Parameters for base distribution (mean and log-scale)\n",
    "        self.mean = nn.Parameter(torch.zeros(1, D))\n",
    "        self.logscale = nn.Parameter(torch.ones(1, D))\n",
    "\n",
    "        self.D = D\n",
    "\n",
    "    def coupling(self, x, index, forward=True):\n",
    "        if self.idf_git == 1:\n",
    "            xa, xb = torch.chunk(x, 2, dim=1)\n",
    "            if forward:\n",
    "                yb = xb + self.round(self.t[index](xa))\n",
    "            else:\n",
    "                yb = xb - self.round(self.t[index](xa))\n",
    "            return torch.cat((xa, yb), dim=1)\n",
    "\n",
    "        elif self.idf_git == 4:\n",
    "            xa, xb, xc, xd = torch.chunk(x, 4, dim=1)\n",
    "            if forward:\n",
    "                ya = xa + self.round(self.t_a[index](torch.cat((xb, xc, xd), dim=1)))\n",
    "                yb = xb + self.round(self.t_b[index](torch.cat((ya, xc, xd), dim=1)))\n",
    "                yc = xc + self.round(self.t_c[index](torch.cat((ya, yb, xd), dim=1)))\n",
    "                yd = xd + self.round(self.t_d[index](torch.cat((ya, yb, yc), dim=1)))\n",
    "            else:\n",
    "                yd = xd - self.round(self.t_d[index](torch.cat((xa, xb, xc), dim=1)))\n",
    "                yc = xc - self.round(self.t_c[index](torch.cat((xa, xb, yd), dim=1)))\n",
    "                yb = xb - self.round(self.t_b[index](torch.cat((xa, yc, yd), dim=1)))\n",
    "                ya = xa - self.round(self.t_a[index](torch.cat((yb, yc, yd), dim=1)))\n",
    "            return torch.cat((ya, yb, yc, yd), dim=1)\n",
    "\n",
    "    def permute(self, x):\n",
    "        return x.flip(1)  # Simple flip operation for permutation\n",
    "\n",
    "    def f(self, x):\n",
    "        z = x\n",
    "        for i in range(self.num_flows):\n",
    "            z = self.coupling(z, i, forward=True)\n",
    "            z = self.permute(z)\n",
    "        return z\n",
    "\n",
    "    def f_inv(self, z):\n",
    "        x = z\n",
    "        for i in reversed(range(self.num_flows)):\n",
    "            x = self.permute(x)\n",
    "            x = self.coupling(x, i, forward=False)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, reduction='avg'):\n",
    "        z = self.f(x)\n",
    "        log_p = -self.log_prior(z)\n",
    "        return log_p.mean() if reduction == 'avg' else log_p.sum()\n",
    "\n",
    "    def sample(self, batchSize, intMax=100):\n",
    "        z = self.prior_sample(batchSize=batchSize, D=self.D, intMax=intMax)\n",
    "        x = self.f_inv(z)\n",
    "        return x.view(batchSize, 1, self.D)\n",
    "\n",
    "    def log_prior(self, x):\n",
    "        log_p = log_integer_probability(x, self.mean, self.logscale)\n",
    "        return log_p.sum(1)\n",
    "\n",
    "    def prior_sample(self, batchSize, D=2):\n",
    "        y = torch.rand(batchSize, self.D)\n",
    "        x = torch.exp(self.logscale) * torch.log(y / (1. - y)) + self.mean\n",
    "        return torch.round(x)\n",
    "D = 4  # Number of features\n",
    "M = 128  # Hidden layer size\n",
    "num_flows = 8  # Number of transformations\n",
    "\n",
    "# Define `idf_git` to choose the coupling layer configuration\n",
    "idf_git = 1  # Set to 1 for bipartite, 4 for quadripartite\n",
    "\n",
    "# Option 1: Single network\n",
    "if idf_git == 1:\n",
    "    nett = lambda: nn.Sequential(\n",
    "        nn.Linear(D // 2, M), nn.LeakyReLU(),\n",
    "        nn.Linear(M, M), nn.LeakyReLU(),\n",
    "        nn.Linear(M, D // 2)\n",
    "    )\n",
    "    netts = [nett]\n",
    "\n",
    "# Option 2: Four networks\n",
    "elif idf_git == 4:\n",
    "    nett_a = lambda: nn.Sequential(\n",
    "        nn.Linear(3 * (D // 4), M), nn.LeakyReLU(),\n",
    "        nn.Linear(M, M), nn.LeakyReLU(),\n",
    "        nn.Linear(M, D // 4)\n",
    "    )\n",
    "    nett_b = lambda: nn.Sequential(\n",
    "        nn.Linear(3 * (D // 4), M), nn.LeakyReLU(),\n",
    "        nn.Linear(M, M), nn.LeakyReLU(),\n",
    "        nn.Linear(M, D // 4)\n",
    "    )\n",
    "    nett_c = lambda: nn.Sequential(\n",
    "        nn.Linear(3 * (D // 4), M), nn.LeakyReLU(),\n",
    "        nn.Linear(M, M), nn.LeakyReLU(),\n",
    "        nn.Linear(M, D // 4)\n",
    "    )\n",
    "    nett_d = lambda: nn.Sequential(\n",
    "        nn.Linear(3 * (D // 4), M), nn.LeakyReLU(),\n",
    "        nn.Linear(M, M), nn.LeakyReLU(),\n",
    "        nn.Linear(M, D // 4)\n",
    "    )\n",
    "    netts = [nett_a, nett_b, nett_c, nett_d]\n",
    "\n",
    "# Initialize IDF model\n",
    "model = IDF(netts, num_flows, D=D)\n",
    "\n",
    "# Initialize IDF model\n",
    "model = IDF(netts, num_flows, D=D)\n",
    "from torchinfo import summary\n",
    "\n",
    "# Example usage\n",
    "#summary(model, input_data=torch.zeros(1, D), show_input=False)\n",
    "test_input = torch.zeros(1, D)  # Replace D with your input dimension\n",
    "output = model(test_input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d6b307",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
