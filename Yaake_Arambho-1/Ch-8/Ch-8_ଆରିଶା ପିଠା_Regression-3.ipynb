{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29533e93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94a2831f",
   "metadata": {},
   "source": [
    "# Dual Linear Regression\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Dual linear regression reparameterizes the standard linear regression model in terms of training examples rather than input dimensions. This is particularly efficient when:\n",
    "- Input dimensionality $D$ is high\n",
    "- Number of training examples $I$ is small ($I < D$)\n",
    "\n",
    "## 2. Mathematical Formulation\n",
    "\n",
    "### Standard Prediction Model\n",
    "The prediction model remains linear:\n",
    "\n",
    "$P r(w_i|x_i) = Norm_{x_i}[\\phi^T x_i, \\sigma^2]$\n",
    "\n",
    "### Dual Parameterization\n",
    "The key difference is representing the slope parameters $\\phi$ as:\n",
    "\n",
    "$\\phi = X\\psi$\n",
    "\n",
    "where:\n",
    "- $\\psi$ is an $I \\times 1$ vector of weights\n",
    "- $X$ is the data matrix\n",
    "- Each $\\phi$ is a weighted sum of training examples\n",
    "\n",
    "### Complete Model\n",
    "Substituting the dual parameterization:\n",
    "\n",
    "$P r(w_i|x_i, \\theta) = Norm_{x_i}[\\psi^T X^T x_i, \\sigma^2]$\n",
    "\n",
    "For all data points:\n",
    "\n",
    "$P r(w|X, \\theta) = Norm_w[X^T X\\psi, \\sigma^2I]$\n",
    "\n",
    "## 3. Maximum Likelihood Solution\n",
    "\n",
    "The log-likelihood function is:\n",
    "\n",
    "$\\log P(w|X,\\psi,\\sigma^2) = -\\frac{I}{2}\\log[2\\pi] - \\frac{I}{2}\\log[\\sigma] - \\frac{(w-X^TX\\psi)^T(w-X^TX\\psi)}{2\\sigma^2}$\n",
    "\n",
    "Maximizing with respect to $\\psi$ and $\\sigma^2$ gives:\n",
    "\n",
    "$\\hat{\\psi} = (X^TX)^{-1}w$\n",
    "\n",
    "$\\hat{\\sigma}^2 = \\frac{(w-X^TX\\psi)^T(w-X^TX\\psi)}{I}$\n",
    "\n",
    "## 4. Implementation\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "class DualLinearRegression:\n",
    "    def __init__(self):\n",
    "        self.psi = None\n",
    "        self.sigma2 = None\n",
    "        \n",
    "    def fit(self, X, w):\n",
    "        \"\"\"\n",
    "        Fit dual linear regression model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: array, shape (n_samples, n_features)\n",
    "            Training data\n",
    "        w: array, shape (n_samples,)\n",
    "            Target values\n",
    "        \"\"\"\n",
    "        # Compute dual parameters\n",
    "        XTX = X.T @ X\n",
    "        self.psi = np.linalg.solve(XTX, w)\n",
    "        \n",
    "        # Compute predictions\n",
    "        w_pred = X.T @ X @ self.psi\n",
    "        \n",
    "        # Estimate noise variance\n",
    "        residuals = w - w_pred\n",
    "        self.sigma2 = residuals.T @ residuals / len(w)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_new, return_std=False):\n",
    "        \"\"\"\n",
    "        Make predictions for new data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_new: array, shape (n_samples, n_features)\n",
    "            New data points\n",
    "        return_std: bool\n",
    "            Whether to return standard deviation\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        mean: array, shape (n_samples,)\n",
    "            Predicted mean\n",
    "        std: array, shape (n_samples,) (optional)\n",
    "            Predicted standard deviation\n",
    "        \"\"\"\n",
    "        mean = X_new.T @ X_new @ self.psi\n",
    "        \n",
    "        if return_std:\n",
    "            std = np.sqrt(self.sigma2) * np.ones_like(mean)\n",
    "            return mean, std\n",
    "        \n",
    "        return mean\n",
    "```\n",
    "\n",
    "## 5. Example Usage\n",
    "\n",
    "```python\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "n_samples, n_features = 100, 500  # High-dimensional case\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "true_phi = np.zeros(n_features)\n",
    "true_phi[:5] = np.random.randn(5)  # Only first 5 features matter\n",
    "w = X @ true_phi + np.random.randn(n_samples) * 0.1\n",
    "\n",
    "# Fit dual regression\n",
    "model = DualLinearRegression()\n",
    "model.fit(X, w)\n",
    "\n",
    "# Make predictions\n",
    "X_test = np.random.randn(10, n_features)\n",
    "mean_pred, std_pred = model.predict(X_test, return_std=True)\n",
    "```\n",
    "\n",
    "## 6. Relationship to Standard Linear Regression\n",
    "\n",
    "The dual formulation gives equivalent results to standard linear regression:\n",
    "\n",
    "$\\hat{\\phi} = X\\hat{\\psi} = X(X^TX)^{-1}w = (XX^T)^{-1}XX^TX(X^TX)^{-1}w = (XX^T)^{-1}Xw$\n",
    "\n",
    "Key differences:\n",
    "1. Computational efficiency when $I < D$\n",
    "2. Natural transition to kernel methods\n",
    "3. Basis for more advanced models like Relevance Vector Machines\n",
    "\n",
    "## 7. Advantages and Limitations\n",
    "\n",
    "### Advantages:\n",
    "1. Efficient for high-dimensional data ($D \\gg I$)\n",
    "2. Natural framework for kernel methods\n",
    "3. Same solution as standard linear regression\n",
    "\n",
    "### Limitations:\n",
    "1. Less efficient when $I > D$\n",
    "2. Can only represent gradients in span of training data\n",
    "3. Requires storing all training examples\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Fig.12 Dual variables. Two di- mensional training data {xi }Ii=1 and associated world state {wi }Ii=1 (indi- cated by marker color). The linear re- gression parameter φ determines the direction in this 2D space in which w changes most quickly. We can alter- nately represent the gradient direc- tion as a weighted sum of data ex- amples. Here we show the case φ = ψ1 x1 + ψ2 x2 . In practical problems the data dimensionality D is greater than the number of examples I so we take a weighted sum φ = Xψ of all of the data points. This is the dual parameterization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320e451f",
   "metadata": {},
   "source": [
    "# Bayesian Dual Linear Regression\n",
    "\n",
    "## 1. Bayesian Formulation\n",
    "\n",
    "### Prior Distribution\n",
    "We define a normal prior over the dual parameters $\\psi$:\n",
    "\n",
    "$P r(\\psi) = Norm_\\psi[0, \\sigma_p^2I]$\n",
    "\n",
    "where $\\sigma_p^2$ represents our prior uncertainty.\n",
    "\n",
    "### Posterior Distribution\n",
    "Using Bayes' rule:\n",
    "\n",
    "$P r(\\psi|X, w, \\sigma^2) = \\frac{P r(X|w, \\psi, \\sigma^2)P r(\\psi)}{P r(X|w, \\sigma^2)}$\n",
    "\n",
    "The posterior has a closed-form Gaussian distribution:\n",
    "\n",
    "$P r(\\psi|X, w, \\sigma^2) = Norm_\\psi[\\frac{1}{\\sigma^2}A^{-1}X^TXw, A^{-1}]$\n",
    "\n",
    "where:\n",
    "\n",
    "$A = \\frac{1}{\\sigma^2}X^TXX^TX + \\frac{1}{\\sigma_p^2}I$\n",
    "\n",
    "### Predictive Distribution\n",
    "The predictive distribution for a new input $x^*$ is:\n",
    "\n",
    "$P r(w^*|x^*, X, w) = \\int P r(w^*|x^*, \\psi)P r(\\psi|X, w) d\\psi$\n",
    "\n",
    "$= Norm_{w^*}[x^{*T}XA^{-1}X^TXw, x^{*T}XA^{-1}X^Tx^* + \\sigma^2]$\n",
    "\n",
    "## 2. Implementation\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.linalg import solve\n",
    "\n",
    "class BayesianDualRegression:\n",
    "    def __init__(self, sigma_p2=100.0):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        sigma_p2: float\n",
    "            Prior variance\n",
    "        \"\"\"\n",
    "        self.sigma_p2 = sigma_p2\n",
    "        self.sigma2 = None\n",
    "        self.A_inv = None\n",
    "        self.X = None\n",
    "        self.w = None\n",
    "        \n",
    "    def fit(self, X, w):\n",
    "        \"\"\"\n",
    "        Fit Bayesian dual regression model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: array, shape (n_samples, n_features)\n",
    "            Training data\n",
    "        w: array, shape (n_samples,)\n",
    "            Target values\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.w = w\n",
    "        \n",
    "        # Estimate noise variance using ML\n",
    "        self.sigma2 = self._estimate_sigma2()\n",
    "        \n",
    "        # Compute posterior parameters\n",
    "        XTX = X.T @ X\n",
    "        XTXXTX = XTX @ XTX\n",
    "        \n",
    "        # Compute A matrix\n",
    "        A = (1/self.sigma2) * XTXXTX + (1/self.sigma_p2) * np.eye(len(w))\n",
    "        \n",
    "        # Store inverse for predictions\n",
    "        self.A_inv = np.linalg.inv(A)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _estimate_sigma2(self):\n",
    "        \"\"\"Estimate noise variance using marginal likelihood\"\"\"\n",
    "        XTX = self.X.T @ self.X\n",
    "        XTXXTX = XTX @ XTX\n",
    "        \n",
    "        # Compute covariance matrix\n",
    "        cov = self.sigma_p2 * XTXXTX + np.eye(len(self.w))\n",
    "        \n",
    "        # Compute log marginal likelihood\n",
    "        log_ml = stats.multivariate_normal.logpdf(\n",
    "            self.w, \n",
    "            mean=np.zeros_like(self.w), \n",
    "            cov=cov\n",
    "        )\n",
    "        \n",
    "        # Use ML estimate for sigma2\n",
    "        return np.var(self.w - self.X.T @ self.X @ np.linalg.solve(XTX, self.w))\n",
    "    \n",
    "    def predict(self, X_new, return_std=False):\n",
    "        \"\"\"\n",
    "        Make predictions for new data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_new: array, shape (n_samples, n_features)\n",
    "            New data points\n",
    "        return_std: bool\n",
    "            Whether to return standard deviation\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        mean: array, shape (n_samples,)\n",
    "            Predicted mean\n",
    "        std: array, shape (n_samples,) (optional)\n",
    "            Predicted standard deviation\n",
    "        \"\"\"\n",
    "        # Compute mean prediction\n",
    "        XTX = self.X.T @ self.X\n",
    "        mean = X_new.T @ self.X @ self.A_inv @ XTX @ self.w / self.sigma2\n",
    "        \n",
    "        if return_std:\n",
    "            # Compute predictive variance\n",
    "            var = (X_new.T @ self.X @ self.A_inv @ self.X.T @ X_new + \n",
    "                  self.sigma2 * np.eye(X_new.shape[1]))\n",
    "            std = np.sqrt(np.diag(var))\n",
    "            return mean, std\n",
    "        \n",
    "        return mean\n",
    "```\n",
    "\n",
    "## 3. Example Usage\n",
    "\n",
    "```python\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "n_samples, n_features = 100, 500\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "true_psi = np.zeros(n_samples)\n",
    "true_psi[:5] = np.random.randn(5)\n",
    "w = X.T @ X @ true_psi + np.random.randn(n_samples) * 0.1\n",
    "\n",
    "# Fit Bayesian model\n",
    "model = BayesianDualRegression(sigma_p2=100.0)\n",
    "model.fit(X, w)\n",
    "\n",
    "# Make predictions with uncertainty\n",
    "X_test = np.random.randn(10, n_features)\n",
    "mean_pred, std_pred = model.predict(X_test, return_std=True)\n",
    "```\n",
    "\n",
    "## 4. Extension to Nonlinear Case\n",
    "\n",
    "To extend to nonlinear regression, replace:\n",
    "1. Training data $X$ with transformed data $Z = [z_1, z_2, ..., z_I]$\n",
    "2. Test data $x^*$ with transformed test data $z^*$\n",
    "\n",
    "The resulting expressions depend only on inner products:\n",
    "- $Z^TZ$\n",
    "- $Z^Tz^*$\n",
    "\n",
    "This makes the model amenable to kernelization.\n",
    "\n",
    "## 5. Marginal Likelihood\n",
    "\n",
    "The marginal likelihood is:\n",
    "\n",
    "$P r(w|X, \\sigma^2) = Norm_w[0, \\sigma_p^2X^TXX^TX + \\sigma^2I]$\n",
    "\n",
    "This can be used to:\n",
    "1. Estimate $\\sigma^2$\n",
    "2. Compare different models\n",
    "3. Select hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676259c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.linalg import solve\n",
    "\n",
    "class BayesianDualRegression:\n",
    "    def __init__(self, sigma_p2=100.0):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        sigma_p2: float\n",
    "            Prior variance\n",
    "        \"\"\"\n",
    "        self.sigma_p2 = sigma_p2\n",
    "        self.sigma2 = None\n",
    "        self.A_inv = None\n",
    "        self.X = None\n",
    "        self.w = None\n",
    "        \n",
    "    def fit(self, X, w):\n",
    "        \"\"\"\n",
    "        Fit Bayesian dual regression model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: array, shape (n_samples, n_features)\n",
    "            Training data\n",
    "        w: array, shape (n_samples,)\n",
    "            Target values\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.w = w\n",
    "        \n",
    "        # Estimate noise variance using ML\n",
    "        self.sigma2 = self._estimate_sigma2()\n",
    "        \n",
    "        # Compute posterior parameters\n",
    "        XTX = X.T @ X\n",
    "        XTXXTX = XTX @ XTX\n",
    "        \n",
    "        # Compute A matrix\n",
    "        A = (1/self.sigma2) * XTXXTX + (1/self.sigma_p2) * np.eye(len(w))\n",
    "        \n",
    "        # Store inverse for predictions\n",
    "        self.A_inv = np.linalg.inv(A)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _estimate_sigma2(self):\n",
    "        \"\"\"Estimate noise variance using marginal likelihood\"\"\"\n",
    "        XTX = self.X.T @ self.X\n",
    "        XTXXTX = XTX @ XTX\n",
    "        \n",
    "        # Compute covariance matrix\n",
    "        cov = self.sigma_p2 * XTXXTX + np.eye(len(self.w))\n",
    "        \n",
    "        # Compute log marginal likelihood\n",
    "        log_ml = stats.multivariate_normal.logpdf(\n",
    "            self.w, \n",
    "            mean=np.zeros_like(self.w), \n",
    "            cov=cov\n",
    "        )\n",
    "        \n",
    "        # Use ML estimate for sigma2\n",
    "        return np.var(self.w - self.X.T @ self.X @ np.linalg.solve(XTX, self.w))\n",
    "    \n",
    "    def predict(self, X_new, return_std=False):\n",
    "        \"\"\"\n",
    "        Make predictions for new data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_new: array, shape (n_samples, n_features)\n",
    "            New data points\n",
    "        return_std: bool\n",
    "            Whether to return standard deviation\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        mean: array, shape (n_samples,)\n",
    "            Predicted mean\n",
    "        std: array, shape (n_samples,) (optional)\n",
    "            Predicted standard deviation\n",
    "        \"\"\"\n",
    "        # Compute mean prediction\n",
    "        XTX = self.X.T @ self.X\n",
    "        mean = X_new.T @ self.X @ self.A_inv @ XTX @ self.w / self.sigma2\n",
    "        \n",
    "        if return_std:\n",
    "            # Compute predictive variance\n",
    "            var = (X_new.T @ self.X @ self.A_inv @ self.X.T @ X_new + \n",
    "                  self.sigma2 * np.eye(X_new.shape[1]))\n",
    "            std = np.sqrt(np.diag(var))\n",
    "            return mean, std\n",
    "        \n",
    "        return mean\n",
    "np.random.seed(42)\n",
    "n_samples, n_features = 100, 500\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "true_psi = np.zeros(n_samples)\n",
    "true_psi[:5] = np.random.randn(5)\n",
    "w = X.T @ X @ true_psi + np.random.randn(n_samples) * 0.1\n",
    "model = BayesianDualRegression(sigma_p2=100.0)\n",
    "model.fit(X, w)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "89852dab",
   "metadata": {},
   "source": [
    "## Relevance Vector Regression\n",
    "\n",
    "Having developed the dual approach to linear regression, we are now in a position to develop a model that depends only sparsely on the training data. This model is known as Relevance Vector Regression (RVR).\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "**(Fig.13: Relevance vector regression. A prior applying sparseness is applied to the dual parameters. This means that the final classifier only depends on a subset of the data points (indicated by the six larger points). The resulting regression function is considerably faster to evaluate and tends to be simpler: this means it is less likely to overfit to random statistical fluctuations in the training data and generalizes better to new data.)**\n",
    "\n",
    "### Sparseness Prior\n",
    "\n",
    "To achieve sparsity, we impose a penalty for every non-zero weighted training example.  This is done by replacing the normal prior over the dual parameters $\\psi$ with a product of one-dimensional t-distributions:\n",
    "\n",
    "$$\n",
    "\\prod_{i=1}^I Pr(\\psi_i) = \\prod_{i=1}^I \\text{Student-t}(\\psi_i | 0, 1, \\nu)  \\qquad (8.53)\n",
    "$$\n",
    "\n",
    "This model is known as relevance vector regression. This situation is analogous to the sparse linear regression model (Section 8.6), except that we are now working with dual variables.\n",
    "\n",
    "### Marginal Likelihood Approximation\n",
    "\n",
    "As in the sparse model, it is not possible to marginalize over the variables $\\psi$ with the t-distributed prior.  We approximate the t-distributions by maximizing with respect to their hidden variables rather than marginalizing (Equation 8.35). By analogy with Section 8.6, the marginal likelihood becomes:\n",
    "\n",
    "$$\n",
    "Pr(w|X, \\sigma^2) \\approx \\max_{H} \\text{Norm}_w[0, (X^T X + \\sigma^2 I)^{-1} X^T X] \\prod_{d=1}^D \\text{Gamma}(h_d | \\nu/2, \\nu/2) \\qquad (8.54)\n",
    "$$\n",
    "\n",
    "where the matrix $H$ contains the hidden variables $\\{h_i\\}_{i=1}^I$ associated with the t-distribution on its diagonal and zeros elsewhere. This expression is similar to Equation 8.52, except that instead of every data point having the same prior variance $\\sigma_p^2$, they now have individual variances determined by the hidden variables in the diagonal matrix $H$.\n",
    "\n",
    "### Optimization Procedure\n",
    "\n",
    "In relevance vector regression, we alternately:\n",
    "\n",
    "**(i)** Optimize the marginal likelihood with respect to the hidden variables:\n",
    "\n",
    "$$\n",
    "h_i^{\\text{new}} = \\frac{1 - h_i \\Sigma_{ii} + \\nu}{\\mu_i^2 + \\nu} \\qquad (8.55)\n",
    "$$\n",
    "\n",
    "**(ii)** Optimize the marginal likelihood with respect to the variance parameter $\\sigma^2$:\n",
    "\n",
    "$$\n",
    "(\\sigma^2)^{\\text{new}} = \\frac{1}{I - \\sum_i (1 - h_i \\Sigma_{ii})} (w - X^T X \\mu)^T (w - X^T X \\mu) \\qquad (8.56)\n",
    "$$\n",
    "\n",
    "Between each step, we update the mean $\\mu$ and variance $\\Sigma$ of the posterior distribution:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mu &= A^{-1} X^T X w \\\\\n",
    "\\Sigma &= A^{-1}\n",
    "\\end{aligned} \\qquad (8.57)\n",
    "$$\n",
    "\n",
    "where $A$ is defined as:\n",
    "\n",
    "$$\n",
    "A = \\frac{1}{\\sigma^2} X^T X + H \\qquad (8.58)\n",
    "$$\n",
    "\n",
    "### Sparsity\n",
    "\n",
    "At the end of training, data examples where the hidden variable $h_i$ is large (e.g., > 1000) are discarded, as the corresponding coefficients $\\psi_i$ will be very small and contribute almost nothing to the solution.\n",
    "\n",
    "### Nonlinear Version\n",
    "\n",
    "Since this algorithm depends only on inner products, a nonlinear version can be generated by replacing the inner products with a kernel function $k[x_i, x_j]$. If the kernel contains parameters, these may also be manipulated to improve the log marginal variance during fitting. Figure 8.13 shows an example fit using the RBF kernel. The final solution depends only on six data points but still captures the important aspects of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6882c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Original parameterization (x1, x2) - highly correlated\n",
    "mean = [0, 0]\n",
    "covariance = [[1, 0.99], [0.99, 1]]  # Strong correlation\n",
    "\n",
    "# Reparameterization (y1, y2) - less correlated\n",
    "eigenvalues, eigenvectors = np.linalg.eig(covariance)\n",
    "\n",
    "def transform_to_y(x):\n",
    "    return np.dot(eigenvectors.T, x)\n",
    "\n",
    "def transform_to_x(y):\n",
    "    return np.dot(eigenvectors, y)\n",
    "\n",
    "# Sampling (Metropolis-Hastings - simplified for illustration)\n",
    "n_samples = 1000\n",
    "x_samples = []\n",
    "y = [0, 0]  # Initial values in the y-space\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    y_proposal = [np.random.normal(0, 1), np.random.normal(0, 1)]  # Independent proposals\n",
    "    x = transform_to_x(y)\n",
    "    x_proposal = transform_to_x(y_proposal)\n",
    "\n",
    "    # Simplified acceptance probability (ignoring Jacobian for simplicity here)\n",
    "    # In a real example, you would need to include the Jacobian term\n",
    "    prob_current = np.exp(-0.5 * np.dot(x, np.linalg.inv(covariance), x))\n",
    "    prob_proposal = np.exp(-0.5 * np.dot(x_proposal, np.linalg.inv(covariance), x_proposal))\n",
    "    alpha = min(1, prob_proposal / prob_current)\n",
    "\n",
    "    if np.random.uniform() < alpha:\n",
    "        y = y_proposal\n",
    "    x_samples.append(transform_to_x(y))\n",
    "\n",
    "x_samples = np.array(x_samples)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x_samples[:, 0], x_samples[:, 1], s=10)\n",
    "plt.title(\"Samples in Original (x1, x2) Space\")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5d0cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt  # For plotting (no Seaborn without NumPy)\n",
    "\n",
    "def gibbs_metropolization(y_initial, n_iterations, g, g_prop, *args):\n",
    "    \"\"\"Gibbs sampling with Metropolis - No NumPy.\"\"\"\n",
    "    n_vars = len(y_initial)\n",
    "    y = list(y_initial)\n",
    "    samples = [list(y)]\n",
    "\n",
    "    for t in range(n_iterations):\n",
    "        for i in range(n_vars):\n",
    "            z_i = g_prop(y, i, *args)\n",
    "\n",
    "            log_alpha = math.log(1 - g(y, *args)) - math.log(1 - g(z_i, *args)) if min(g(y, *args), g(z_i, *args)) > 0 else -math.inf\n",
    "            alpha = math.exp(log_alpha) if log_alpha > -math.inf else 0\n",
    "            alpha = min(1, alpha)\n",
    "\n",
    "            u = random.random()\n",
    "            if u < alpha:\n",
    "                y[i] = z_i[i]\n",
    "        samples.append(list(y))\n",
    "\n",
    "    return samples\n",
    "\n",
    "\n",
    "# Example distributions (replace with your own)\n",
    "def g(y, *args):\n",
    "    \"\"\"Example joint distribution (replace).\"\"\"\n",
    "    a, b, x_max = args\n",
    "    prob = 1\n",
    "    for i in range(len(y)):\n",
    "        if i < 2:\n",
    "            prob *= binom_pmf(y[i], x_max[i], a[i] * 0.5 / (a[i] * 0.5 + b[i]))\n",
    "        else:\n",
    "            prob *= binom_pmf(y[i], x_max[i], a[i] * 0.5 / (a[i] * 0.5 + b[i]))\n",
    "    return prob\n",
    "\n",
    "def binom_pmf(k, n, p):\n",
    "    \"\"\"Binomial PMF.\"\"\"\n",
    "    if 0 <= k <= n:\n",
    "        coeff = math.factorial(n) // (math.factorial(k) * math.factorial(n - k))\n",
    "        return coeff * (p**k) * ((1 - p) ** (n - k))\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def g_prop(y, i, *args):\n",
    "    \"\"\"Example proposal distribution (replace).\"\"\"\n",
    "    a, b, x_max = args\n",
    "    z = list(y)\n",
    "    while True:\n",
    "        if i < 2:\n",
    "            z[i] = int(random.gauss(x_max[i] * a[i] * 0.5 / (a[i] * 0.5 + b[i]), 1)) # Example with mu=0.5\n",
    "        else:\n",
    "            z[i] = int(random.gauss(x_max[i] * a[i] * 0.5 / (a[i] * 0.5 + b[i]), 1)) # Example with eta=0.5\n",
    "        if 0 <= z[i] <= x_max[i] and z[i] != y[i]:\n",
    "            break\n",
    "    return z\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "a = [0.06, 0.14, 0.11, 0.09]\n",
    "b = [0.17, 0.24, 0.19, 0.20]\n",
    "x_max = [9, 15, 12, 7]\n",
    "y_initial = [0, 0, 0, 0]\n",
    "n_iterations = 10000\n",
    "\n",
    "samples = gibbs_metropolization(y_initial, n_iterations, g, g_prop, a, b, x_max)\n",
    "\n",
    "\n",
    "\n",
    "# --- Plotting with Matplotlib (no Seaborn) ---\n",
    "\n",
    "# 1. Trace Plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "fig.suptitle(\"Trace Plots of Samples\", fontsize=16)\n",
    "\n",
    "for i in range(4):\n",
    "    row = i // 2\n",
    "    col = i % 2\n",
    "    y_i_values = [sample[i] for sample in samples]  # Extract y_i values\n",
    "    axes[row, col].plot(range(n_iterations + 1), y_i_values)\n",
    "    axes[row, col].set_title(f\"Trace of y{i+1}\")\n",
    "    axes[row, col].set_xlabel(\"Iteration\")\n",
    "    axes[row, col].set_ylabel(f\"y{i+1} Value\")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 2. Distribution Plots (Histograms)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "fig.suptitle(\"Distribution of Samples\", fontsize=16)\n",
    "\n",
    "for i in range(4):\n",
    "    row = i // 2\n",
    "    col = i % 2\n",
    "    y_i_values = [sample[i] for sample in samples]\n",
    "    axes[row, col].hist(y_i_values, density=True)  # density=True for probabilities\n",
    "    axes[row, col].set_title(f\"Distribution of y{i+1}\")\n",
    "    axes[row, col].set_xlabel(f\"y{i+1} Value\")\n",
    "    axes[row, col].set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "# ... (Autocorrelation and Running Mean plots can be added using Matplotlib)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c2bfef4f",
   "metadata": {},
   "source": [
    "\n",
    "## Regression to Multivariate Data\n",
    "\n",
    "Throughout this chapter, we have discussed predicting a scalar value $w_i$ from multivariate data $x_i$. In real-world situations such as the pose regression problem, the world states $w_i$ are multivariate. It is trivial to extend the models in this chapter: we simply construct a separate regressor for each dimension. The exception to this rule is the relevance vector machine (RVM): here we might want to ensure that the sparse structure is common for each of these models so the efficiency gains are retained. To this end, we modify the model so that a single set of hidden variables is shared across the model for each world state dimension.\n",
    "\n",
    "## Applications\n",
    "\n",
    "Regression methods are used less frequently in vision than classification, but nonetheless, there are many useful applications. The majority of these involve estimating the position or pose of objects since the unknowns in such problems are naturally treated as continuous.\n",
    "\n",
    "### Human Body Pose Estimation\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Agarwal & Triggs (2006) developed a system based on the relevance vector machine to predict body pose $w$ from silhouette data $x$. To encode the silhouette, they computed a 60-dimensional shape context feature at each of 400-500 points on the boundary of the object. To reduce the data dimensionality, they computed the similarity of each shape context feature to each of 100 different prototypes. Finally, they formed a 100-dimensional histogram containing the aggregated 100-dimensional similarities for all of the boundary points. This histogram was used as the data vector $x$.\n",
    "\n",
    "The body pose was encoded by the 3 joint angles of each of the 18 major body joints and the overall azimuth (compass heading) of the body. The resulting 55-dimensional vector was used as the world state $w$.\n",
    "\n",
    "A relevance vector machine was trained using 2636 data vectors $x_i$ extracted from silhouettes that were rendered using the commercial program **POSER** from known motion capture data $w_i$. Using a radial basis function kernel, the relevance vector machine based its solution on just **6%** of these training examples. The body pose angles of test data could be predicted to within an average of **6°** error:\n",
    "\n",
    "$$ \\text{Mean test error} = 6.0^\\circ $$\n",
    "\n",
    "They also demonstrated that the system worked reasonably well on silhouettes from real images.\n",
    "\n",
    "Silhouette information is by its nature ambiguous: it is very hard to tell which leg is in front of the other based on a single silhouette. Agarwal & Triggs (2006) partially circumvented this system by tracking the body pose $w_i$ through a video sequence. Essentially, the ambiguity at a given frame is resolved by encouraging the estimated pose in adjacent frames in the sequence to be similar: information from frames where the pose vector is well-defined is propagated through the sequence to resolve ambiguities in other parts.\n",
    "\n",
    "However, the ambiguity of silhouette data is an argument for not using this type of classifier: the regression models in this chapter are designed to give a unimodal normal output. To effectively classify single frames of data, we should use a regression method that produces a **multi-modal prediction** that can effectively describe the ambiguity.\n",
    "\n",
    "![image-2.png](attachment:image-2.png)\n",
    "Fig.15 Tracking using displacement experts. The goal of the system i to predict a displacement vector indicating the motion of the object base on the pixel data at its last known position. a) The system is trained b perturbing the bounding box around the object to simulate the motion o the object. b) The system successfully tracks a face, even in the presenc c) of partial occlusions. d) If the system is trained using gradient vector rather than raw pixel values, it is also quite robust to changes in illumination Adapted from Williams et al. (2005) �2005 c IEEE.\n",
    "\n",
    "```markdown\n",
    "# Regression to Multivariate Data\n",
    "\n",
    "Throughout this chapter, we have discussed predicting a scalar value $w_i$ from multivariate data $x_i$. In real-world situations such as the pose regression problem, the world states $w_i$ are multivariate. It is trivial to extend the models in this chapter: we simply construct a separate regressor for each dimension. The exception to this rule is the relevance vector machine (RVM): here we might want to ensure that the sparse structure is common for each of these models so the efficiency gains are retained. To this end, we modify the model so that a single set of hidden variables is shared across the model for each world state dimension.\n",
    "\n",
    "## Applications\n",
    "\n",
    "Regression methods are used less frequently in vision than classification, but nonetheless, there are many useful applications. The majority of these involve estimating the position or pose of objects since the unknowns in such problems are naturally treated as continuous.\n",
    "\n",
    "### Human Body Pose Estimation\n",
    "\n",
    "Agarwal & Triggs (2006) developed a system based on the relevance vector machine to predict body pose $w$ from silhouette data $x$. To encode the silhouette, they computed a 60-dimensional shape context feature at each of 400-500 points on the boundary of the object. To reduce the data dimensionality, they computed the similarity of each shape context feature to each of 100 different prototypes. Finally, they formed a 100-dimensional histogram containing the aggregated 100-dimensional similarities for all of the boundary points. This histogram was used as the data vector $x$.\n",
    "\n",
    "The body pose was encoded by the 3 joint angles of each of the 18 major body joints and the overall azimuth (compass heading) of the body. The resulting 55-dimensional vector was used as the world state $w$.\n",
    "\n",
    "A relevance vector machine was trained using 2636 data vectors $x_i$ extracted from silhouettes that were rendered using the commercial program **POSER** from known motion capture data $w_i$. Using a radial basis function kernel, the relevance vector machine based its solution on just **6%** of these training examples. The body pose angles of test data could be predicted to within an average of **6°** error:\n",
    "\n",
    "$$ \\text{Mean test error} = 6.0^\\circ $$\n",
    "\n",
    "They also demonstrated that the system worked reasonably well on silhouettes from real images.\n",
    "\n",
    "Silhouette information is by its nature ambiguous: it is very hard to tell which leg is in front of the other based on a single silhouette. Agarwal & Triggs (2006) partially circumvented this system by tracking the body pose $w_i$ through a video sequence. Essentially, the ambiguity at a given frame is resolved by encouraging the estimated pose in adjacent frames in the sequence to be similar: information from frames where the pose vector is well-defined is propagated through the sequence to resolve ambiguities in other parts.\n",
    "\n",
    "However, the ambiguity of silhouette data is an argument for not using this type of classifier: the regression models in this chapter are designed to give a unimodal normal output. To effectively classify single frames of data, we should use a regression method that produces a **multi-modal prediction** that can effectively describe the ambiguity.\n",
    "\n",
    "### Displacement Experts\n",
    "\n",
    "Regression models are also used to form displacement experts in tracking applications. The goal is to take a region of the image $x$ and return a set of numbers $w$ that indicate the change in position of an object relative to the window. The world state $w$ might simply contain the horizontal and vertical translation vectors or might contain parameters of a more complex 2D transformation (chapter 15). For simplicity, we will describe the former situation.\n",
    "\n",
    "Training data are extracted as follows. A bounding box around the object of interest (car, face, etc.) is identified in a number of frames. For each of these frames, the bounding box is perturbed by a pre-determined set of translation vectors, to simulate the object moving in the opposite direction (figure 8.15a). In this way, we associate a translation vector $w_i$ with each perturbation. The data from the perturbed bounding box are extracted, resized to a standard shape, and histogram equalized (section 13.1.2) to induce a degree of invariance to illumination changes. The resulting values are then concatenated to form the data vector $x_i$.\n",
    "\n",
    "Williams et al. (2005) describe a system of this kind in which the elements of $w$ were learned by a set of independent relevance vector machines. They initialize the position of the object using a standard object detector (see chapter 9). In the subsequent frame, they compute a prediction for the displacement vector $w$ using the relevance vector machines on the data $x$ from the original position. This prediction is combined in a Kalman filter-like system (chapter 19) that imposes prior knowledge about the continuity of the motion to create a robust method for tracking known objects in scenes. Fig.15b-d show a series of tracking results from this system.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca98bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn_rvm import EMRVR  # Relevance Vector Machine for Regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate synthetic training data (for demonstration)\n",
    "def generate_training_data(num_samples=1000):\n",
    "    X = np.random.rand(num_samples, 64, 64)  # Simulated image patches (64x64)\n",
    "    w = np.random.rand(num_samples, 2) * 10 - 5  # Random translation vectors (-5 to 5)\n",
    "    return X, w\n",
    "\n",
    "# Preprocess data: Flatten images & normalize\n",
    "def preprocess_data(X):\n",
    "    return X.reshape(X.shape[0], -1) / 255.0  # Flatten and normalize\n",
    "\n",
    "# Train RVM for displacement estimation\n",
    "def train_displacement_expert(X, w):\n",
    "    model = EMRVR(kernel='rbf', gamma=0.1)  # Radial Basis Function kernel\n",
    "    model.fit(X, w)  # Train on input patches and translation vectors\n",
    "    return model\n",
    "\n",
    "# Predict displacement of an object in a new frame\n",
    "def predict_displacement(model, new_patch):\n",
    "    new_patch = preprocess_data(new_patch.reshape(1, 64, 64))\n",
    "    return model.predict(new_patch)[0]\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    X, w = generate_training_data()\n",
    "    X = preprocess_data(X)\n",
    "\n",
    "    # Split into train/test sets\n",
    "    X_train, X_test, w_train, w_test = train_test_split(X, w, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train model\n",
    "    model = train_displacement_expert(X_train, w_train)\n",
    "\n",
    "    # Simulate tracking with a new image patch\n",
    "    test_patch = np.random.rand(64, 64)  # Simulated new image patch\n",
    "    predicted_displacement = predict_displacement(model, test_patch)\n",
    "\n",
    "    print(f\"Predicted Displacement Vector: {predicted_displacement}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4fac0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
