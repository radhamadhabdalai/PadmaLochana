{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1166961",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright (c) 2004 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d053d9a",
   "metadata": {},
   "source": [
    "##  An Example of Score-Based Generative Models: Variance Exploding PF-ODE\n",
    "\n",
    "###  Model Formulation\n",
    "\n",
    "To define our own score-based generative model (SBGM), we need the following elements:\n",
    "- The drift $ f(x, t) $\n",
    "- The diffusion $ g(t) $\n",
    "- The form of $ p_0^t(x_t | x_0) $\n",
    "\n",
    "In [1] and [20], three examples of SBGM are provided: \n",
    "- Variance Exploding (VE) SDE\n",
    "- Variance Preserving (VP) SDE\n",
    "- Sub-VP SDE\n",
    "\n",
    "Here, we focus on the **VE SDE**, which assumes the following choices for the drift and diffusion:\n",
    "\n",
    "$$\n",
    "f(x, t) = 0, \\quad g(t) = \\sigma t,\n",
    "$$\n",
    "where \\( \\sigma > 0 \\) is a hyperparameter and $ t \\in [0, 1] $.\n",
    "\n",
    "By plugging in the choices for $ f(x, t) $ and $ g(t) $ into the general form of the PF-ODE, we get:\n",
    "\n",
    "$$\n",
    "\\frac{dx_t}{dt} = - \\frac{\\sigma^2 t}{2} \\nabla_{x_t} \\ln p_t(x_t), \\tag{9.31}\n",
    "$$\n",
    "\n",
    "where the term $ \\nabla_{x_t} \\ln p_t(x_t) $ represents the score function.\n",
    "\n",
    "Now, to learn the score model, we need to define the conditional distribution $ p_0^t(x_t | x_0) $. Fortunately, the theory of SDEs (e.g., see Chapter 5 of [19]) gives us a way to calculate $ p_0^t(x_t | x_0) $.\n",
    "\n",
    "The solution for $ p_0^t(x_t | x_0) $ is given by:\n",
    "\n",
    "$$\n",
    "p_0^t(x_t | x_0) = \\mathcal{N}(x_t | x_0, (\\sigma^2 t - 1)I), \\quad \\text{for} \\quad t \\in [0, 1]. \\tag{9.32}\n",
    "$$\n",
    "\n",
    "The variance function over time is:\n",
    "\n",
    "$$\n",
    "\\sigma_t^2 = (\\sigma^2 t - 1), \\tag{9.33}\n",
    "$$\n",
    "\n",
    "Thus, the final distribution $ p_1(x) $ (for sufficiently large $ \\sigma $) is approximately:\n",
    "\n",
    "$$\n",
    "p_1(x) = p_0(x_0) * \\mathcal{N}(x | x_0, (\\sigma^2 - 1)I). \\tag{9.34}\n",
    "$$\n",
    "\n",
    "For large $ \\sigma $, the distribution becomes:\n",
    "\n",
    "$$\n",
    "p_1(x) \\approx \\mathcal{N}(x | 0, (\\sigma^2 - 1)I). \\tag{9.35}\n",
    "$$\n",
    "\n",
    "###  The Choice of $ \\lambda_t $\n",
    "\n",
    "One important consideration is the choice of $ \\lambda_t $ in the definition of the loss function $ L_t(\\theta) $. \n",
    "\n",
    "Although Ho et al. [3] simply set $ \\lambda_t \\equiv 1 $, Song and Kingma [21] showed that setting $ \\lambda_t = \\sigma_t^2 $ is actually beneficial for the VE PF-ODE. This choice of $ \\lambda_t $ helps us use the sum over $ L_t(\\theta) $ as a proxy for the log-likelihood function, which is useful for early stopping during training.\n",
    "\n",
    "This leads to a simpler loss function:\n",
    "\n",
    "$$\n",
    "L_t(\\theta) = \\mathbb{E}_{x_0 \\sim p_d} \\left[ \\mathbb{E}_{x_t \\sim p_0^t(x_t | x_0)} \\left[ \\lambda_t \\| s_\\theta(x_t, t) - \\nabla_{x_t} \\ln p_0^t(x_t | x_0) \\|^2 \\right] \\right].\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0577e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the score model (using a simple linear model for demonstration)\n",
    "class ScoreModel:\n",
    "    def __init__(self, input_dim):\n",
    "        self.input_dim = input_dim\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        # A simple model that approximates the score function (for simplicity)\n",
    "        return np.array([x_i / (t + 1e-8) for x_i in x])\n",
    "\n",
    "# Drift and Diffusion Functions\n",
    "def f(x, t):\n",
    "    return 0.0  # Zero drift (no movement)\n",
    "\n",
    "def g(t, sigma):\n",
    "    return sigma * t  # Diffusion scales with time t\n",
    "\n",
    "# Noisy distribution function based on the VE-SDE model\n",
    "def noisy_distribution(x0, t, sigma):\n",
    "    noise_std = (sigma**2 * t - 1)**0.5\n",
    "    noise = np.random.normal(0, noise_std, size=x0.shape)\n",
    "    return x0 + noise\n",
    "\n",
    "# Denoising score matching loss function\n",
    "def denoising_score_matching_loss(model, x0, t, sigma):\n",
    "    x_t = noisy_distribution(x0, t, sigma)\n",
    "    \n",
    "    # True score function (gradient of log p_0^t(x_t | x_0))\n",
    "    true_score = (x_t - x0) / ((sigma**2 * t - 1)**0.5 + 1e-8)\n",
    "    \n",
    "    # Model prediction (score function)\n",
    "    predicted_score = model.forward(x_t, t)\n",
    "    \n",
    "    # Compute loss: Mean squared error between true and predicted score\n",
    "    loss = np.mean((predicted_score - true_score) ** 2)\n",
    "    return loss\n",
    "\n",
    "# Backward Euler's method for sampling\n",
    "def backward_euler_step(x_t, t, model, sigma, delta_t=0.1):\n",
    "    score = model.forward(x_t, t)\n",
    "    dx_t = -0.5 * g(t, sigma)**2 * score * delta_t\n",
    "    x_t_next = x_t + dx_t\n",
    "    return x_t_next\n",
    "\n",
    "# Training loop\n",
    "def train_score_model(model, data, sigma, num_epochs=100, learning_rate=1e-3):\n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for x0 in data:\n",
    "            t = random.random()  # Random time steps\n",
    "            \n",
    "            # Compute the denoising score matching loss\n",
    "            loss = denoising_score_matching_loss(model, x0, t, sigma)\n",
    "            epoch_loss += loss\n",
    "        \n",
    "        losses.append(epoch_loss / len(data))\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(data)}')\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Sampling from the trained model using backward Euler's method\n",
    "def sample_from_model(model, initial_condition, sigma, num_steps=100, delta_t=0.1):\n",
    "    x_t = initial_condition\n",
    "    for step in range(num_steps):\n",
    "        t = step / num_steps  # Linearly increasing t\n",
    "        x_t = backward_euler_step(x_t, t, model, sigma, delta_t)\n",
    "    return x_t\n",
    "\n",
    "# Plotting functions\n",
    "def plot_loss_curve(losses):\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss Curve\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_generated_samples(original_data, generated_sample):\n",
    "    original_data = np.array(original_data)\n",
    "    plt.scatter(original_data[:, 0], original_data[:, 1], label='Original Data', alpha=0.5)\n",
    "    generated_sample = np.array(generated_sample)\n",
    "    plt.scatter(generated_sample[:, 0], generated_sample[:, 1], label='Generated Sample', color='red', marker='x')\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.title(\"Original vs Generated Samples\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    # Generate synthetic data (e.g., from a Gaussian distribution)\n",
    "    initial_data = np.random.randn(100, 2)  # 100 samples, 2D data\n",
    "    \n",
    "    # Initialize the score model\n",
    "    model = ScoreModel(input_dim=2)\n",
    "    \n",
    "    # Hyperparameter\n",
    "    sigma = 2.0  # Variance parameter for the VE-SDE\n",
    "    \n",
    "    # Train the model and get loss values\n",
    "    losses = train_score_model(model, initial_data, sigma, num_epochs=100, learning_rate=1e-3)\n",
    "    \n",
    "    # Plot the loss curve\n",
    "    plot_loss_curve(losses)\n",
    "    \n",
    "    # Sample from the model after training\n",
    "    initial_condition = np.random.randn(2)  # Starting point for sampling\n",
    "    generated_sample = sample_from_model(model, initial_condition, sigma)\n",
    "    \n",
    "    # Plot original vs generated samples\n",
    "    plot_generated_samples(initial_data, [generated_sample])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a44823a",
   "metadata": {},
   "source": [
    "### 9.3.3.2 The Choice of \\( \\lambda_t \\)\n",
    "\n",
    "The last remark, before we move to the training procedure, is about the choice of \\( \\lambda_t \\) in the definition of \\( L_t(\\theta) \\). So far, I simply omitted that, but I had a good reason. Ho et al. [3] simply set \\( \\lambda_t \\equiv 1 \\). Done! Really though? Well, as you can imagine, but smart reader, that is not so easy. Song and Kingma [21] showed that it is actually beneficial to set \\( \\lambda_t = \\sigma_t^2 \\) in the case of VE PF-ODE. Then, we can even use the sum over \\( L_t(\\theta) \\) as a proxy to the log-likelihood function. We will take advantage of that for early stopping in our training procedure.\n",
    "\n",
    "### 9.3.3.3 Training\n",
    "\n",
    "We present a training procedure based on the chosen example of the VE SBGM. As we outlined earlier in the case of the score matching method, the procedure is relatively easy and straightforward. It consists of the following steps:\n",
    "\n",
    "#### Training Procedure for VE SBGM\n",
    "\n",
    "1. Pick a datapoint \\( x_0 \\).\n",
    "2. Sample \\( x_1 \\sim \\pi(x) = \\mathcal{N}(x | 0, I) \\).\n",
    "3. Sample \\( t \\sim \\text{Uniform}(0, 1) \\).\n",
    "4. Calculate \\( x_t = x_0 + 2 \\ln\\left( \\frac{1}{\\sigma} \\right) (\\sigma^2 t - 1) \\cdot x_1 \\). This is a sample from \\( p_0^t(x_t | x_0) \\).\n",
    "5. Evaluate the score model at \\( (x_t, t) \\), \\( s_\\theta(x_t, t) \\).\n",
    "6. Calculate the score matching loss for a single sample:\n",
    "   \\[\n",
    "   L_t(\\theta) = \\sigma_t^2 \\|x_1 - \\sigma_t s_\\theta(x_t, t)\\|^2\n",
    "   \\]\n",
    "7. Update \\( \\theta \\) using a gradient-based method with \\( \\nabla_\\theta L_t(\\theta) \\).\n",
    "\n",
    "We repeat these seven steps for available training data until some stop criterion is met. Obviously, in practice, we use mini-batches instead of single datapoints.\n",
    "\n",
    "In this training procedure, we use \\( -\\sigma_t s_\\theta(x_t, t) \\) on purpose because \\( -\\sigma_t s_\\theta(x_t, t) = \\epsilon_\\theta(x_t, t) \\), and then the criterion \\( \\sigma_t^2 \\|x_1 - \\epsilon_\\theta(x_t, t)\\|^2 \\) corresponds to diffusion-based models [4, 5]. Now, you see why we pushed for seeing diffusion-based models as dynamical systems!\n",
    "\n",
    "### 9.3.3.4 Sampling\n",
    "\n",
    "After training the score model, we can finally generate samples! For that, we need to run backward Euler’s method (or other ODE solvers, please remember that), which takes the following form for the VE PF-ODE:\n",
    "\n",
    "\\[\n",
    "x_{t+\\Delta} = x_t + \\frac{\\sigma}{2} s_\\theta(x_t, t) \\Delta\n",
    "\\]\n",
    "\n",
    "or equivalently:\n",
    "\n",
    "\\[\n",
    "x_{t+\\Delta} = x_t - \\frac{\\sigma_t}{2} s_\\theta(x_t, t) \\Delta\n",
    "\\]\n",
    "\n",
    "starting from $ x_1 \\sim p_1(x) = \\mathcal{N}\\left( x | 0, \\left( \\sigma^2 - 1 \\right) \\ln \\sigma I \\right) \\).\n",
    "\n",
    "Note that in the first equation, we have the plus sign because the diffusion for the VE PF-ODE is \\( -\\frac{1}{2} \\sigma^2 t \\); therefore, the minus sign in backward Euler’s method turns to plus. Maybe this is very obvious to you, my reader, but I always mess around with pluses and minuses, so I prefer to be very precise here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669ea6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the score model neural network\n",
    "class ScoreModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ScoreModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, input_dim)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # Input is (x, t), so t is concatenated to x before feeding into the network\n",
    "        x = torch.cat([x, t.unsqueeze(-1)], dim=-1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = 2\n",
    "sigma = 1.1  # Slightly greater than 1 to avoid division by 0\n",
    "lr = 1e-3\n",
    "num_epochs = 10000\n",
    "batch_size = 64\n",
    "\n",
    "# Initialize score model and optimizer\n",
    "score_model = ScoreModel(input_dim)\n",
    "optimizer = optim.Adam(score_model.parameters(), lr=lr)\n",
    "\n",
    "# Generate training data (for example, samples from a standard Gaussian distribution)\n",
    "def generate_data(batch_size):\n",
    "    x0 = torch.randn(batch_size, input_dim)\n",
    "    x1 = torch.randn(batch_size, input_dim)  # Noise\n",
    "    t = torch.rand(batch_size)  # Uniform random time between [0, 1]\n",
    "    \n",
    "    # Generate xt from p0t(x | x0) = N(x | x0, (sigma^2 t - 1)I)\n",
    "    sigma_t = (sigma**2 * t - 1).sqrt()\n",
    "    xt = x0 + sigma_t.unsqueeze(-1) * x1\n",
    "    return x0, xt, x1, t\n",
    "\n",
    "# Score matching loss function\n",
    "def score_matching_loss(x1, xt, s_theta, t):\n",
    "    # Calculate the score matching loss\n",
    "    s = s_theta(xt, t)\n",
    "    sigma_t = (sigma**2 * t - 1).sqrt()\n",
    "    loss = torch.mean(sigma_t**2 * (x1 - sigma_t * s)**2)\n",
    "    return loss\n",
    "\n",
    "# Training procedure\n",
    "for epoch in range(num_epochs):\n",
    "    # Sample a mini-batch of data\n",
    "    x0, xt, x1, t = generate_data(batch_size)\n",
    "    \n",
    "    # Zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass: compute score matching loss\n",
    "    loss = score_matching_loss(x1, xt, score_model, t)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# Sampling from the trained score model using backward Euler’s method\n",
    "def sample_from_model(score_model, num_samples=100, T=1.0, delta_t=0.01):\n",
    "    # Start from x1 ~ p1(x) = N(x | 0, (sigma^2 - 1)I)\n",
    "    x_t = torch.randn(num_samples, input_dim) * (sigma**2 - 1).sqrt()\n",
    "    t = T  # Start at the final time\n",
    "    samples = [x_t]\n",
    "    \n",
    "    # Run backward Euler's method\n",
    "    while t > 0:\n",
    "        # Evaluate the score at (x_t, t)\n",
    "        s = score_model(x_t, t)\n",
    "        \n",
    "        # Update x_t using backward Euler's method\n",
    "        x_t = x_t - sigma * (t) * s * delta_t / 2\n",
    "        t -= delta_t\n",
    "        samples.append(x_t)\n",
    "    \n",
    "    # Return the samples\n",
    "    return torch.stack(samples[::-1])\n",
    "\n",
    "# Generate samples after training\n",
    "samples = sample_from_model(score_model)\n",
    "\n",
    "# Plot the samples\n",
    "plt.scatter(samples[:, 0].detach().numpy(), samples[:, 1].detach().numpy(), label=\"Generated samples\")\n",
    "plt.title(\"Generated Samples from VE SBGM\")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7aa86ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.4263892080593279\n",
      "Epoch 1000, Loss: 1.1007253686144405\n",
      "Epoch 2000, Loss: 1.921324229043416\n",
      "Epoch 3000, Loss: 5.522069715782781\n",
      "Epoch 4000, Loss: 0.04137683225271107\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16352/1600634499.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;31m# Plot the samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Use first component of each sample for plotting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m \u001b[0mx_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0my_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_16352/1600634499.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;31m# Plot the samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Use first component of each sample for plotting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m \u001b[0mx_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0my_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the Score Model (simplified)\n",
    "class ScoreModel:\n",
    "    def __init__(self, input_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.weights = [random.gauss(0, 1) for _ in range(input_dim)]  # Simple linear model for score function\n",
    "        self.bias = random.gauss(0, 1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # Simplified score function: Linear combination of inputs and time\n",
    "        return [w * x_i + self.bias * t for w, x_i in zip(self.weights, x)]\n",
    "\n",
    "    def update(self, gradients, lr=0.001):\n",
    "        # Update weights using simple gradient descent\n",
    "        for i in range(self.input_dim):\n",
    "            self.weights[i] -= lr * gradients[i]\n",
    "        self.bias -= lr * gradients[-1]\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "sigma = 1.1  # Slightly greater than 1 to avoid division by 0\n",
    "lr = 0.01\n",
    "num_epochs = 5000\n",
    "batch_size = 64\n",
    "input_dim = 2  # 2D example for simplicity\n",
    "\n",
    "# Generate data (random normal distributed x0)\n",
    "def generate_data(batch_size):\n",
    "    x0 = [random.gauss(0, 1) for _ in range(input_dim)]\n",
    "    x1 = [random.gauss(0, 1) for _ in range(input_dim)]  # Noise\n",
    "    t = random.uniform(0.001, 1)  # Uniform random time between [0.001, 1] to avoid issues at t=0\n",
    "    \n",
    "    # Generate xt from p0t(x | x0) = N(x | x0, (sigma^2 t - 1)I)\n",
    "    sigma_t = math.sqrt(sigma**2 * t - 1) if sigma**2 * t - 1 > 0 else 0  # Ensure positive square root\n",
    "    xt = [x0_i + sigma_t * x1_i for x0_i, x1_i in zip(x0, x1)]\n",
    "    return x0, xt, x1, t\n",
    "\n",
    "# Score matching loss function\n",
    "def score_matching_loss(x1, xt, score_model, t):\n",
    "    # Calculate the score matching loss (simplified)\n",
    "    s = score_model.forward(xt, t)\n",
    "    sigma_t = math.sqrt(sigma**2 * t - 1) if sigma**2 * t - 1 > 0 else 0\n",
    "    loss = sum([(x1_i - sigma_t * s_i) ** 2 for x1_i, s_i in zip(x1, s)])\n",
    "    return loss\n",
    "\n",
    "# Training procedure\n",
    "score_model = ScoreModel(input_dim)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Sample a mini-batch of data\n",
    "    x0, xt, x1, t = generate_data(batch_size)\n",
    "    \n",
    "    # Compute the score matching loss\n",
    "    loss = score_matching_loss(x1, xt, score_model, t)\n",
    "    \n",
    "    # Backpropagation: compute gradients and update model (simplified)\n",
    "    gradients = [random.gauss(0, 0.1) for _ in range(input_dim)]  # Fake gradient for simplicity\n",
    "    score_model.update(gradients, lr)\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Sampling from the trained score model using backward Euler’s method\n",
    "def sample_from_model(score_model, num_samples=100, T=1.0, delta_t=0.01):\n",
    "    # Start from x1 ~ p1(x) = N(x | 0, (sigma^2 - 1)I)\n",
    "    x_t = [random.gauss(0, math.sqrt(sigma**2 - 1)) for _ in range(input_dim)]\n",
    "    t = T  # Start at the final time\n",
    "    samples = [x_t]\n",
    "\n",
    "    # Run backward Euler's method\n",
    "    while t > 0:\n",
    "        # Evaluate the score at (x_t, t)\n",
    "        s = score_model.forward(x_t, t)\n",
    "\n",
    "        # Update x_t using backward Euler's method\n",
    "        x_t = [x_i - sigma * t * s_i * delta_t / 2 for x_i, s_i in zip(x_t, s)]\n",
    "        t -= delta_t\n",
    "        samples.append(x_t)\n",
    "\n",
    "    # Return the samples\n",
    "    return samples\n",
    "\n",
    "# Generate samples after training\n",
    "samples = sample_from_model(score_model)\n",
    "\n",
    "# Plot the samples\n",
    "samples = [sample[0] for sample in samples]  # Use first component of each sample for plotting\n",
    "x_vals = [sample[0] for sample in samples]\n",
    "y_vals = [sample[1] for sample in samples]\n",
    "\n",
    "plt.scatter(x_vals, y_vals, label=\"Generated samples\")\n",
    "plt.title(\"Generated Samples from VE SBGM\")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "## Do it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9094ad81",
   "metadata": {},
   "source": [
    "# SBGM by JT\n",
    "\n",
    "The following code implements the SBGM class, which is conceptually similar to score matching.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SBGM(nn.Module):\n",
    "    def __init__(self, snet, sigma, D, T):\n",
    "        super(SBGM, self).__init__()\n",
    "        \n",
    "        print(\"SBGM by JT.\")\n",
    "        \n",
    "        # sigma parameter\n",
    "        self.sigma = torch.Tensor([sigma])\n",
    "        \n",
    "        # define the base distribution (multivariate Gaussian with the diagonal covariance)\n",
    "        var = (1. / (2. * torch.log(self.sigma))) * (self.sigma ** 2 - 1.)\n",
    "        self.base = torch.distributions.multivariate_normal.MultivariateNormal(\n",
    "            torch.zeros(D), var * torch.eye(D)\n",
    "        )\n",
    "        \n",
    "        # score model\n",
    "        self.snet = snet\n",
    "        \n",
    "        # time embedding (a single linear layer)\n",
    "        self.time_embedding = nn.Sequential(\n",
    "            nn.Linear(1, D),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        # other hyperparameters\n",
    "        self.D = D\n",
    "        self.T = T\n",
    "        self.EPS = 1.e-5\n",
    "        \n",
    "# SBGM by JT\n",
    "\n",
    "The following code implements the SBGM class, which is conceptually similar to score matching.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SBGM(nn.Module):\n",
    "    def __init__(self, snet, sigma, D, T):\n",
    "        super(SBGM, self).__init__()\n",
    "        \n",
    "        print(\"SBGM by JT.\")\n",
    "        \n",
    "        # sigma parameter\n",
    "        self.sigma = torch.Tensor([sigma])\n",
    "        \n",
    "        # define the base distribution (multivariate Gaussian with the diagonal covariance)\n",
    "        var = (1. / (2. * torch.log(self.sigma))) * (self.sigma ** 2 - 1.)\n",
    "        self.base = torch.distributions.multivariate_normal.MultivariateNormal(\n",
    "            torch.zeros(D), var * torch.eye(D)\n",
    "        )\n",
    "        \n",
    "        # score model\n",
    "        self.snet = snet\n",
    "        \n",
    "        # time embedding (a single linear layer)\n",
    "        self.time_embedding = nn.Sequential(\n",
    "            nn.Linear(1, D),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        # other hyperparameters\n",
    "        self.D = D\n",
    "        self.T = T\n",
    "        self.EPS = 1.e-5\n",
    "\n",
    "    def sigma_fun(self, t):\n",
    "        # the sigma function (dependent on t), it is the std of the distribution\n",
    "        return torch.sqrt((1. / (2. * torch.log(self.sigma))) * (self.sigma ** (2. * t) - 1.))\n",
    "\n",
    "    def log_p_base(self, x):\n",
    "        # the log-probability of the base distribution, p_1(x)\n",
    "        log_p = self.base.log_prob(x)\n",
    "        return log_p\n",
    "\n",
    "    def sample_base(self, x_0):\n",
    "        # sampling from the base distribution\n",
    "        return self.base.rsample(sample_shape=torch.Size([x_0.shape[0]]))\n",
    "\n",
    "    def sample_p_t(self, x_0, x_1, t):\n",
    "        # sampling from p_0t(x_t|x_0)\n",
    "        # x_0 ~ data, x_1 ~ noise\n",
    "        x = x_0 + self.sigma_fun(t) * x_1\n",
    "        return x\n",
    "\n",
    "    def lambda_t(self, t):\n",
    "        # the loss weighting\n",
    "        return self.sigma_fun(t) ** 2\n",
    "\n",
    "    def diffusion_coeff(self, t):\n",
    "        # the diffusion coefficient in the SDE\n",
    "        return self.sigma ** t\n",
    "\n",
    "    def forward(self, x_0, reduction='mean'):\n",
    "        # x_1 ~ the base distribution\n",
    "        x_1 = torch.randn_like(x_0)\n",
    "        # t ~ Uniform(0, 1)\n",
    "        t = torch.rand(size=(x_0.shape[0], 1)) * (1. - self.EPS) + self.EPS\n",
    "\n",
    "        # sample from p_0t(x|x_0)\n",
    "        x_t = self.sample_p_t(x_0, x_1, t)\n",
    "\n",
    "        # invert noise\n",
    "        # NOTE: here we use the correspondence eps_theta(x,t) = - sigma*t score_theta(x,t)\n",
    "        t_embd = self.time_embedding(t)\n",
    "        x_pred = -self.sigma_fun(t) * self.snet(x_t + t_embd)\n",
    "\n",
    "        # LOSS: Score Matching\n",
    "        # NOTE: since x_pred is the predicted noise, and x_1 is noise, this corresponds to Noise Matching\n",
    "        # (i.e., the loss used in diffusion-based models by Ho et al.)\n",
    "        SM_loss = 0.5 * self.lambda_t(t) * torch.pow(x_pred + x_1, 2).mean(-1)\n",
    "\n",
    "        if reduction == 'sum':\n",
    "            loss = SM_loss.sum()\n",
    "        else:\n",
    "            loss = SM_loss.mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def sample(self, batch_size=64):\n",
    "        # 1) sample x_0 ~ Normal(0, 1 / (2 log sigma) * (sigma ** 2 - 1))\n",
    "        x_t = self.sample_base(torch.empty(batch_size, self.D))\n",
    "\n",
    "        # Apply Euler’s method\n",
    "        # NOTE: x_0 - data, x_1 - noise\n",
    "        # Therefore, we must use BACKWARD Euler’s method! This results in the minus sign!\n",
    "        ts = torch.linspace(1., self.EPS, self.T)\n",
    "        delta_t = ts[0] - ts[1]\n",
    "\n",
    "        for t in ts[1:]:\n",
    "            tt = torch.Tensor([t])\n",
    "            u = 0.5 * self.diffusion_coeff(tt) * self.snet(x_t + self.time_embedding(tt))\n",
    "            x_t = x_t - delta_t * u\n",
    "\n",
    "        x_t = torch.tanh(x_t)\n",
    "        return x_t\n",
    "\n",
    "    def log_prob_proxy(self, x_0, reduction=\"mean\"):\n",
    "        # Calculate the proxy of the log-likelihood (see (Song et al., 2021))\n",
    "        # NOTE: Here, we use a single sample per time step (this is done only for simplicity and speed);\n",
    "        # To get a better estimate, we should sample more noise\n",
    "        ts = torch.linspace(self.EPS, 1., self.T)\n",
    "\n",
    "        for t in ts:\n",
    "            # Sample noise\n",
    "            x_1 = torch.randn_like(x_0)\n",
    "            # Sample from p_0t(x_t|x_0)\n",
    "            x_t = self.sample_p_t(x_0, x_1, t)\n",
    "            # Predict noise\n",
    "            t_embd = self.time_embedding(torch.Tensor([t]))\n",
    "            x_pred = -self.snet(x_t + t_embd) * self.sigma_fun(t)\n",
    "            # loss (proxy)\n",
    "            if t == self.EPS:\n",
    "                proxy = 0.5 * self.lambda_t(t) * torch.pow(x_pred + x_1, 2).mean(-1)\n",
    "            else:\n",
    "                proxy = proxy + 0.5 * self.lambda_t(t) * torch.pow(x_pred + x_1, 2).mean(-1)\n",
    "\n",
    "        if reduction == \"mean\":\n",
    "            return proxy.mean()\n",
    "        elif reduction == \"sum\":\n",
    "            return proxy.sum()        "
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "9b8b9241",
   "metadata": {},
   "source": [
    "## There Is a Fantastic World of Score-Based Generative Models Out There!\n",
    "\n",
    "### Other SBGMs\n",
    "\n",
    "There are other classes of SBGMs besides variance exploding, namely $[1]$:\n",
    "\n",
    "A B C\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "\n",
    "**Fig.6** Results for the VE PF-ODE model:\n",
    "- (a) A sample of real images.\n",
    "- (b) A sample of generated images.\n",
    "- (c) An example of the score matching loss calculated on the validation set.\n",
    "\n",
    "## Variance Preserving (VP) and Sub-VP Models\n",
    "\n",
    "- **Variance preserving (VP)**: The drift is $ f(x, t) = -\\frac{1}{2} \\beta_t x $, the diffusion is $ g(t) = \\beta_t $, and the loss weighting is $ \\lambda_t = 1 - \\exp\\left\\{-\\int_{0}^{t} \\beta_s ds\\right\\} $, where $ \\beta_t $ is some function of time $ t $.\n",
    "\n",
    "- **Sub-VP**: The drift is $ f(x, t) = -\\frac{1}{2} \\beta_t x $, the diffusion is $ g(t) = \\sqrt{\\int_{0}^{t} \\beta_s ds \\left(1 - \\exp\\left\\{-2 \\int_{0}^{t} \\beta_s ds\\right\\}\\right)} $, and the loss weighting is $ \\lambda_t = 1 - \\exp\\left\\{-\\int_{0}^{t} \\beta_s ds\\right\\} $, where $ \\beta_t $ is some function of time $ t $.\n",
    "\n",
    "There exist various versions of these models, especially there are different ways of defining $ \\lambda_t $ and other functions dependent on $ t $ like $ \\sigma_t $ in VE and $ \\beta_t $ in VP. See [5] for an overview.\n",
    "\n",
    "## Better Solvers\n",
    "\n",
    "As briefly mentioned earlier, one drawback of SBGMs is a large number of steps during sampling (i.e., the number of steps of an ODE solver). [22] presented specialized ODE solvers that could achieve great performance within $ T = 10 $ that was further improved to $ T = 5 $ in [23]! In general, a better-suited solver could be used to obtain better results, e.g., by using Heun’s method [24].\n",
    "\n",
    "## Other Improvements\n",
    "\n",
    "There are many ideas within the domain of score-based models! Here, I will name only a few:\n",
    "\n",
    "- Using SBGMs in the latent space [25].\n",
    "- It is possible to calculate the log-likelihood function for SBGMs in a similar manner to neural ODE [26]. Song et al. [20] showed that the log-likelihood function could be upper-bounded by some modification of the score matching loss.\n",
    "- Using various tricks to improve the log-likelihood estimation like dequantization and importance weighting [27].\n",
    "- In [28], a new class of models was proposed dubbed consistency models. The idea is to learn a model that could match noise to data in a single step.\n",
    "- An extension of SBGMs to Riemannian manifolds was proposed by [29].\n",
    "\n",
    "There is a lot of work done! There are many, many papers on SBGMs being published as we speak. Check out this webpage for an up-to-date overview of SBGMs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ff27887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions as dist\n",
    "\n",
    "class SBGM(nn.Module):\n",
    "    def __init__(self, snet, sigma, D, T):\n",
    "        super(SBGM, self).__init__()\n",
    "\n",
    "        print(\"SBGM by JT.\")\n",
    "\n",
    "        # sigma parameter\n",
    "        self.sigma = torch.Tensor([sigma])\n",
    "\n",
    "        # define the base distribution (multivariate Gaussian with the diagonal covariance)\n",
    "        var = (1. / (2. * torch.log(self.sigma))) * (self.sigma ** 2 - 1.)\n",
    "        self.base = dist.MultivariateNormal(torch.zeros(D), var * torch.eye(D))\n",
    "\n",
    "        # score model\n",
    "        self.snet = snet\n",
    "\n",
    "        # time embedding (a single linear layer)\n",
    "        self.time_embedding = nn.Sequential(\n",
    "            nn.Linear(1, D),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        # other hyperparameters\n",
    "        self.D = D\n",
    "        self.T = T\n",
    "        self.EPS = 1.e-5\n",
    "\n",
    "    def sigma_fun(self, t):\n",
    "        # the sigma function (dependent on t), it is the std of the distribution\n",
    "        return torch.sqrt((1. / (2. * torch.log(self.sigma))) * (self.sigma ** (2. * t) - 1.))\n",
    "\n",
    "    def log_p_base(self, x):\n",
    "        # the log-probability of the base distribution, p_1(x)\n",
    "        log_p = self.base.log_prob(x)\n",
    "        return log_p\n",
    "\n",
    "    def sample_base(self, x_0):\n",
    "        # sampling from the base distribution\n",
    "        return self.base.rsample(sample_shape=torch.Size([x_0.shape[0]]))\n",
    "\n",
    "    def sample_p_t(self, x_0, x_1, t):\n",
    "        # sampling from p_0t(x_t|x_0)\n",
    "        # x_0 ~ data, x_1 ~ noise\n",
    "        x = x_0 + self.sigma_fun(t) * x_1\n",
    "        return x\n",
    "\n",
    "    def lambda_t(self, t):\n",
    "        # the loss weighting\n",
    "        return self.sigma_fun(t) ** 2\n",
    "\n",
    "    def diffusion_coeff(self, t):\n",
    "        # the diffusion coefficient in the SDE\n",
    "        return self.sigma ** t\n",
    "\n",
    "    def forward(self, x_0, reduction='mean'):\n",
    "        # x_1 ~ the base distribution\n",
    "        x_1 = torch.randn_like(x_0)\n",
    "        # t ~ Uniform(0, 1)\n",
    "        t = torch.rand(size=(x_0.shape[0], 1)) * (1. - self.EPS) + self.EPS\n",
    "\n",
    "        # sample from p_0t(x|x_0)\n",
    "        x_t = self.sample_p_t(x_0, x_1, t)\n",
    "\n",
    "        # invert noise\n",
    "        # NOTE: here we use the correspondence eps_theta(x,t) = - sigma*t score_theta(x,t)\n",
    "        t_embd = self.time_embedding(t)\n",
    "        x_pred = -self.sigma_fun(t) * self.snet(x_t + t_embd)\n",
    "\n",
    "        # LOSS: Score Matching\n",
    "        # NOTE: since x_pred is the predicted noise, and x_1 is noise, this corresponds to Noise Matching\n",
    "        # (i.e., the loss used in diffusion-based models by Ho et al.)\n",
    "        SM_loss = 0.5 * self.lambda_t(t) * torch.pow(x_pred + x_1, 2).mean(-1)\n",
    "\n",
    "        if reduction == 'sum':\n",
    "            loss = SM_loss.sum()\n",
    "        else:\n",
    "            loss = SM_loss.mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def sample(self, batch_size=64):\n",
    "        # 1) sample x_0 ~ Normal(0, 1 / (2 log sigma) * (sigma ** 2 - 1))\n",
    "        x_t = self.sample_base(torch.empty(batch_size, self.D))\n",
    "\n",
    "        # Apply Euler’s method\n",
    "        # NOTE: x_0 - data, x_1 - noise\n",
    "        # Therefore, we must use BACKWARD Euler’s method! This results in the minus sign!\n",
    "        ts = torch.linspace(1., self.EPS, self.T)\n",
    "        delta_t = ts[0] - ts[1]\n",
    "\n",
    "        for t in ts[1:]:\n",
    "            tt = torch.Tensor([t])\n",
    "            u = 0.5 * self.diffusion_coeff(tt) * self.snet(x_t + self.time_embedding(tt))\n",
    "            x_t = x_t - delta_t * u\n",
    "\n",
    "        x_t = torch.tanh(x_t)\n",
    "        return x_t\n",
    "\n",
    "    def log_prob_proxy(self, x_0, reduction=\"mean\"):\n",
    "        # Calculate the proxy of the log-likelihood (see (Song et al., 2021))\n",
    "        # NOTE: Here, we use a single sample per time step (this is done only for simplicity and speed);\n",
    "        # To get a better estimate, we should sample more noise\n",
    "        ts = torch.linspace(self.EPS, 1., self.T)\n",
    "\n",
    "        for t in ts:\n",
    "            # Sample noise\n",
    "            x_1 = torch.randn_like(x_0)\n",
    "            # Sample from p_0t(x_t|x_0)\n",
    "            x_t = self.sample_p_t(x_0, x_1, t)\n",
    "            # Predict noise\n",
    "            t_embd = self.time_embedding(torch.Tensor([t]))\n",
    "            x_pred = -self.snet(x_t + t_embd) * self.sigma_fun(t)\n",
    "            # loss (proxy)\n",
    "            if t == self.EPS:\n",
    "                proxy = 0.5 * self.lambda_t(t) * torch.pow(x_pred + x_1, 2).mean(-1)\n",
    "            else:\n",
    "                proxy = proxy + 0.5 * self.lambda_t(t) * torch.pow(x_pred + x_1, 2).mean(-1)\n",
    "\n",
    "        if reduction == \"mean\":\n",
    "            return proxy.mean()\n",
    "        elif reduction == \"sum\":\n",
    "            return proxy.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cf0ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions as dist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class SBGM(nn.Module):\n",
    "    def __init__(self, snet, sigma, D, T):\n",
    "        super(SBGM, self).__init__()\n",
    "\n",
    "        print(\"SBGM by JT.\")\n",
    "\n",
    "        # sigma parameter\n",
    "        self.sigma = torch.Tensor([sigma])\n",
    "\n",
    "        # define the base distribution (multivariate Gaussian with the diagonal covariance)\n",
    "        var = (1. / (2. * torch.log(self.sigma))) * (self.sigma ** 2 - 1.)\n",
    "        self.base = dist.MultivariateNormal(torch.zeros(D), var * torch.eye(D))\n",
    "\n",
    "        # score model\n",
    "        self.snet = snet\n",
    "\n",
    "        # time embedding (a single linear layer)\n",
    "        self.time_embedding = nn.Sequential(\n",
    "            nn.Linear(1, D),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        # other hyperparameters\n",
    "        self.D = D\n",
    "        self.T = T\n",
    "        self.EPS = 1.e-5\n",
    "\n",
    "        # For storing training loss\n",
    "        self.training_losses = []\n",
    "\n",
    "    def sigma_fun(self, t):\n",
    "        # the sigma function (dependent on t), it is the std of the distribution\n",
    "        return torch.sqrt((1. / (2. * torch.log(self.sigma))) * (self.sigma ** (2. * t) - 1.))\n",
    "\n",
    "    def log_p_base(self, x):\n",
    "        # the log-probability of the base distribution, p_1(x)\n",
    "        log_p = self.base.log_prob(x)\n",
    "        return log_p\n",
    "\n",
    "    def sample_base(self, x_0):\n",
    "        # sampling from the base distribution\n",
    "        return self.base.rsample(sample_shape=torch.Size([x_0.shape[0]]))\n",
    "\n",
    "    def sample_p_t(self, x_0, x_1, t):\n",
    "        # sampling from p_0t(x_t|x_0)\n",
    "        # x_0 ~ data, x_1 ~ noise\n",
    "        x = x_0 + self.sigma_fun(t) * x_1\n",
    "        return x\n",
    "\n",
    "    def lambda_t(self, t):\n",
    "        # the loss weighting\n",
    "        return self.sigma_fun(t) ** 2\n",
    "\n",
    "    def diffusion_coeff(self, t):\n",
    "        # the diffusion coefficient in the SDE\n",
    "        return self.sigma ** t\n",
    "\n",
    "    def forward(self, x_0, reduction='mean'):\n",
    "        # x_1 ~ the base distribution\n",
    "        x_1 = torch.randn_like(x_0)\n",
    "        # t ~ Uniform(0, 1)\n",
    "        t = torch.rand(size=(x_0.shape[0], 1)) * (1. - self.EPS) + self.EPS\n",
    "\n",
    "        # sample from p_0t(x|x_0)\n",
    "        x_t = self.sample_p_t(x_0, x_1, t)\n",
    "\n",
    "        # invert noise\n",
    "        # NOTE: here we use the correspondence eps_theta(x,t) = - sigma*t score_theta(x,t)\n",
    "        t_embd = self.time_embedding(t)\n",
    "        x_pred = -self.sigma_fun(t) * self.snet(x_t + t_embd)\n",
    "\n",
    "        # LOSS: Score Matching\n",
    "        # NOTE: since x_pred is the predicted noise, and x_1 is noise, this corresponds to Noise Matching\n",
    "        # (i.e., the loss used in diffusion-based models by Ho et al.)\n",
    "        SM_loss = 0.5 * self.lambda_t(t) * torch.pow(x_pred + x_1, 2).mean(-1)\n",
    "\n",
    "        if reduction == 'sum':\n",
    "            loss = SM_loss.sum()\n",
    "        else:\n",
    "            loss = SM_loss.mean()\n",
    "\n",
    "        self.training_losses.append(loss.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def sample(self, batch_size=64):\n",
    "        # 1) sample x_0 ~ Normal(0, 1 / (2 log sigma) * (sigma ** 2 - 1))\n",
    "        x_t = self.sample_base(torch.empty(batch_size, self.D))\n",
    "\n",
    "        # Apply Euler’s method\n",
    "        # NOTE: x_0 - data, x_1 - noise\n",
    "        # Therefore, we must use BACKWARD Euler’s method! This results in the minus sign!\n",
    "        ts = torch.linspace(1., self.EPS, self.T)\n",
    "        delta_t = ts[0] - ts[1]\n",
    "\n",
    "        for t in ts[1:]:\n",
    "            tt = torch.Tensor([t])\n",
    "            u = 0.5 * self.diffusion_coeff(tt) * self.snet(x_t + self.time_embedding(tt))\n",
    "            x_t = x_t - delta_t * u\n",
    "\n",
    "        x_t = torch.tanh(x_t)\n",
    "        return x_t\n",
    "\n",
    "    def log_prob_proxy(self, x_0, reduction=\"mean\"):\n",
    "        # Calculate the proxy of the log-likelihood (see (Song et al., 2021))\n",
    "        # NOTE: Here, we use a single sample per time step (this is done only for simplicity and speed);\n",
    "        # To get a better estimate, we should sample more noise\n",
    "        ts = torch.linspace(self.EPS, 1., self.T)\n",
    "\n",
    "        for t in ts:\n",
    "            # Sample noise\n",
    "            x_1 = torch.randn_like(x_0)\n",
    "            # Sample from p_0t(x_t|x_0)\n",
    "            x_t = self.sample_p_t(x_0, x_1, t)\n",
    "            # Predict noise\n",
    "            t_embd = self.time_embedding(torch.Tensor([t]))\n",
    "            x_pred = -self.snet(x_t + t_embd) * self.sigma_fun(t)\n",
    "            # loss (proxy)\n",
    "            if t == self.EPS:\n",
    "                proxy = 0.5 * self.lambda_t(t) * torch.pow(x_pred + x_1, 2).mean(-1)\n",
    "            else:\n",
    "                proxy = proxy + 0.5 * self.lambda_t(t) * torch.pow(x_pred + x_1, 2).mean(-1)\n",
    "\n",
    "        if reduction == \"mean\":\n",
    "            return proxy.mean()\n",
    "        elif reduction == \"sum\":\n",
    "            return proxy.sum()\n",
    "\n",
    "    def plot_training_loss(self):\n",
    "        # Plot the training loss over epochs\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.training_losses, label='Training Loss')\n",
    "        plt.title('Training Loss Over Time')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_samples(self, num_samples=16):\n",
    "        # Plot sampled images\n",
    "        samples = self.sample(batch_size=num_samples).detach().cpu().numpy()\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        for i in range(num_samples):\n",
    "            plt.subplot(4, 4, i + 1)\n",
    "            plt.imshow(samples[i].reshape(28, 28), cmap='gray')\n",
    "            plt.axis('off')\n",
    "        plt.suptitle('Sampled Images')\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
