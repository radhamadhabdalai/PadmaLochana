{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8c9ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright (c) 2004 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41004f96",
   "metadata": {},
   "source": [
    "## Reparameterization Trick\n",
    "\n",
    "So far, we played around with the log-likelihood, and we ended up with the ELBO. However, there is still a problem with calculating the expected value, because it contains an integral! Therefore, the question is how we can calculate it and why it is better than the MC approximation of the log-likelihood without the variational posterior. In fact, we will use the MC approximation, but now, instead of sampling from the prior $ p(z) $, we will sample from the variational posterior $ q_\\phi(z|x) $.\n",
    "\n",
    "Is it better? Yes, because the variational posterior typically assigns more probability mass to a smaller region than the prior. If you examine the variance of the variational posterior, you will probably notice that the variational posteriors are almost deterministic (whether it is good or bad is an open question). As a result, we should get a better approximation!\n",
    "\n",
    "However, there is still an issue with the variance of the approximation. If we sample $ z $ from $ q_\\phi(z|x) $, plug them into the ELBO, and calculate gradients with respect to the parameters of a neural network $ \\phi $, the variance of the gradient may still be pretty large! A possible solution to that, first noticed by statisticians (e.g., see [8]), is the approach of reparameterizing the distribution.\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Fig.3 An example of reparameterizing a Gaussian distribution: We scale .ϵ distributed according to the standard Gaussian by .σ, and shift it by .μ.\n",
    "\n",
    "### Reparameterization Trick\n",
    "\n",
    "The idea is to express a random variable as a composition of primitive transformations (e.g., arithmetic operations, logarithm, etc.) of an independent random variable with a simple distribution. For example, if we consider a Gaussian random variable $ z $ with a mean $ \\mu $ and variance $ \\sigma^2 $, and an independent random variable $ \\epsilon \\sim N(\\epsilon | 0, 1) $, then the following holds:\n",
    "\n",
    "$$\n",
    "z = \\mu + \\sigma \\cdot \\epsilon\n",
    "$$\n",
    "\n",
    "Now, if we sample $ \\epsilon $ from the standard Gaussian, and apply the above transformation, then we get a sample from $ N(z | \\mu, \\sigma) $. This idea can be generalized to other distributions as well.\n",
    "\n",
    "The reparameterization trick can be used in the encoder $ q_\\phi(z|x) $. By using this trick, we can drastically reduce the variance of the gradient because the randomness comes from the independent source $ p(\\epsilon) $, and we calculate the gradient with respect to a deterministic function (i.e., a neural network), not random variables.\n",
    "\n",
    "### Application in VAEs\n",
    "\n",
    "In the VAE framework, we will apply the reparameterization trick to the latent variable $ z $. The encoder $ q_\\phi(z|x) $ outputs two values: the mean $ \\mu_\\phi(x) $ and log-variance $ \\log \\sigma^2_\\phi(x) $ for each input $ x $. Instead of sampling directly from $ q_\\phi(z|x) $, we sample an independent Gaussian $ \\epsilon \\sim N(\\epsilon | 0, 1) $ and apply the following transformation:\n",
    "\n",
    "$$\n",
    "z_\\phi = \\mu_\\phi(x) + \\sigma_\\phi(x) \\cdot \\epsilon\n",
    "$$\n",
    "\n",
    "Here, $ \\mu_\\phi(x) $ and $ \\sigma_\\phi(x) $ are outputs of the encoder neural network. This ensures that we can backpropagate through the reparameterization, reducing the variance of the gradient estimates.\n",
    "\n",
    "### Training the VAE\n",
    "\n",
    "The training objective for the VAE is to minimize the **negative ELBO**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\phi, \\theta; x) = - \\mathbb{E}_{q_\\phi(z|x)} \\left[\\ln p_\\theta(x|z)\\right] + \\text{KL}\\left(q_\\phi(z|x) || p(z)\\right)\n",
    "$$\n",
    "\n",
    "In practice, we approximate the expectation $ \\mathbb{E}_{q_\\phi(z|x)} $ by taking a single sample from the variational posterior:\n",
    "\n",
    "$$\n",
    "z_\\phi = \\mu_\\phi(x) + \\sigma_\\phi(x) \\cdot \\epsilon\n",
    "$$\n",
    "\n",
    "where $ \\epsilon \\sim N(0, I) $. This allows the model to be trained using stochastic gradient descent, where we only need a single sample of $ z $ during each training iteration.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- The reparameterization trick is used to express a random variable $ z $ as a deterministic transformation of an independent random variable $ \\epsilon $, making the gradient computation more stable.\n",
    "- For VAEs, we apply the reparameterization trick to sample $ z $ from the variational posterior $ q_\\phi(z|x) $.\n",
    "- The training objective is the negative ELBO, and the reparameterization trick allows us to sample $ z $ efficiently and compute gradients for optimization. \n",
    "\n",
    "##  VAE Code Implementation\n",
    "\n",
    "### Encoder Class\n",
    "\n",
    "The encoder class takes in an input $ x $, passes it through the encoder network, and generates the mean $ \\mu_\\phi(x) $ and log-variance $ \\log \\sigma_\\phi^2(x) $ of the variational posterior $ q_\\phi(z|x) $. We then use the **reparameterization trick** to sample from this posterior.\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, encoder_net):\n",
    "        super(Encoder, self).__init__()\n",
    "        # The init of the encoder network.\n",
    "        self.encoder = encoder_net\n",
    "\n",
    "    # Reparameterization trick for Gaussians.\n",
    "    @staticmethod\n",
    "    def reparameterization(mu, log_var):\n",
    "        # The formula is the following:\n",
    "        # z = mu + std * epsilon\n",
    "        # epsilon ~ Normal(0, 1)\n",
    "        \n",
    "        # Get the standard deviation from the log-variance.\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        \n",
    "        # Sample epsilon from Normal(0, 1).\n",
    "        eps = torch.randn_like(std)\n",
    "        \n",
    "        # Final output\n",
    "        return mu + std * eps\n",
    "\n",
    "    # Output of the encoder network (mean and log-variance).\n",
    "    def encode(self, x):\n",
    "        # Calculate the output of the encoder network of size 2M.\n",
    "        h_e = self.encoder(x)\n",
    "        \n",
    "        # Split the output into the mean and log-variance.\n",
    "        mu_e, log_var_e = torch.chunk(h_e, 2, dim=1)\n",
    "        \n",
    "        return mu_e, log_var_e\n",
    "\n",
    "    # Sampling procedure using reparameterization.\n",
    "    def sample(self, x=None, mu_e=None, log_var_e=None):\n",
    "        if mu_e is None and log_var_e is None:\n",
    "            # Calculate mean and log-variance from the encoder.\n",
    "            mu_e, log_var_e = self.encode(x)\n",
    "        \n",
    "        # Apply the reparameterization trick.\n",
    "        return self.reparameterization(mu_e, log_var_e)\n",
    "\n",
    "    # Log-probability of the sample used for ELBO calculation.\n",
    "    def log_prob(self, x=None, mu_e=None, log_var_e=None, z=None):\n",
    "        if x is not None:\n",
    "            # Calculate corresponding sample if only x is provided.\n",
    "            mu_e, log_var_e = self.encode(x)\n",
    "            z = self.sample(mu_e=mu_e, log_var_e=log_var_e)\n",
    "        \n",
    "        # Return the log-normal distribution.\n",
    "        return log_normal_diag(z, mu_e, log_var_e)\n",
    "\n",
    "    # Forward pass: either log-probability or sampling.\n",
    "    def forward(self, x, type='log_prob'):\n",
    "        assert type in ['encode', 'log_prob'], 'Type could be either encode or log_prob'\n",
    "        if type == 'log_prob':\n",
    "            return self.log_prob(x)\n",
    "        else:\n",
    "            return self.sample(x)\n",
    "## 5.3.5 VAE Code Implementation\n",
    "\n",
    "### Encoder Class\n",
    "\n",
    "The encoder class takes in an input $ x $, passes it through the encoder network, and generates the mean $ \\mu_\\phi(x) $ and log-variance $ \\log \\sigma_\\phi^2(x) $ of the variational posterior $ q_\\phi(z|x) $. We then use the **reparameterization trick** to sample from this posterior.\n",
    "\n",
    "```python\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, encoder_net):\n",
    "        super(Encoder, self).__init__()\n",
    "        # The init of the encoder network.\n",
    "        self.encoder = encoder_net\n",
    "\n",
    "    # Reparameterization trick for Gaussians.\n",
    "    @staticmethod\n",
    "    def reparameterization(mu, log_var):\n",
    "        # The formula is the following:\n",
    "        # z = mu + std * epsilon\n",
    "        # epsilon ~ Normal(0, 1)\n",
    "        \n",
    "        # Get the standard deviation from the log-variance.\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        \n",
    "        # Sample epsilon from Normal(0, 1).\n",
    "        eps = torch.randn_like(std)\n",
    "        \n",
    "        # Final output\n",
    "        return mu + std * eps\n",
    "\n",
    "    # Output of the encoder network (mean and log-variance).\n",
    "    def encode(self, x):\n",
    "        # Calculate the output of the encoder network of size 2M.\n",
    "        h_e = self.encoder(x)\n",
    "        \n",
    "        # Split the output into the mean and log-variance.\n",
    "        mu_e, log_var_e = torch.chunk(h_e, 2, dim=1)\n",
    "        \n",
    "        return mu_e, log_var_e\n",
    "\n",
    "    # Sampling procedure using reparameterization.\n",
    "    def sample(self, x=None, mu_e=None, log_var_e=None):\n",
    "        if mu_e is None and log_var_e is None:\n",
    "            # Calculate mean and log-variance from the encoder.\n",
    "            mu_e, log_var_e = self.encode(x)\n",
    "        \n",
    "        # Apply the reparameterization trick.\n",
    "        return self.reparameterization(mu_e, log_var_e)\n",
    "\n",
    "    # Log-probability of the sample used for ELBO calculation.\n",
    "    def log_prob(self, x=None, mu_e=None, log_var_e=None, z=None):\n",
    "        if x is not None:\n",
    "            # Calculate corresponding sample if only x is provided.\n",
    "            mu_e, log_var_e = self.encode(x)\n",
    "            z = self.sample(mu_e=mu_e, log_var_e=log_var_e)\n",
    "        \n",
    "        # Return the log-normal distribution.\n",
    "        return log_normal_diag(z, mu_e, log_var_e)\n",
    "\n",
    "    # Forward pass: either log-probability or sampling.\n",
    "    def forward(self, x, type='log_prob'):\n",
    "        assert type in ['encode', 'log_prob'], 'Type could be either encode or log_prob'\n",
    "        if type == 'log_prob':\n",
    "            return self.log_prob(x)\n",
    "        else:\n",
    "            return self.sample(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, decoder_net, distribution='categorical', num_vals=None):\n",
    "        super(Decoder, self).__init__()\n",
    "        # The decoder network.\n",
    "        self.decoder = decoder_net\n",
    "        \n",
    "        # The distribution used for the decoder.\n",
    "        self.distribution = distribution\n",
    "        \n",
    "        # The number of possible values (used for categorical distribution).\n",
    "        self.num_vals = num_vals\n",
    "\n",
    "    # This function calculates parameters of the likelihood function p(x|z).\n",
    "    def decode(self, z):\n",
    "        # Apply the decoder network.\n",
    "        h_d = self.decoder(z)\n",
    "        \n",
    "        if self.distribution == 'categorical':\n",
    "            # Reshape to (Batch size, Dimensionality, Number of Values).\n",
    "            b = h_d.shape[0]\n",
    "            d = h_d.shape[1] // self.num_vals\n",
    "            h_d = h_d.view(b, d, self.num_vals)\n",
    "            \n",
    "            # Apply softmax to get probabilities.\n",
    "            mu_d = torch.softmax(h_d, 2)\n",
    "            return [mu_d]\n",
    "        \n",
    "        elif self.distribution == 'bernoulli':\n",
    "            # Apply sigmoid for Bernoulli distribution.\n",
    "            mu_d = torch.sigmoid(h_d)\n",
    "            return [mu_d]\n",
    "        \n",
    "        else:\n",
    "            raise ValueError('Only: \"categorical\", \"bernoulli\"')\n",
    "\n",
    "    # Sampling from the decoder.\n",
    "    def sample(self, z):\n",
    "        outs = self.decode(z)\n",
    "        \n",
    "        if self.distribution == 'categorical':\n",
    "            # Use the output of the decoder.\n",
    "            mu_d = outs[0]\n",
    "            return mu_d\n",
    "\n",
    "### Summary:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e08343",
   "metadata": {},
   "source": [
    "- **Encoder**: Takes $ x $, computes the mean and log-variance, and applies the reparameterization trick to sample $ z $.\n",
    "- **Decoder**: Takes $ z $ and produces the parameters of the likelihood function, using a categorical or Bernoulli distribution.\n",
    "- **VAE Loss**: The ELBO is computed and used to train the model, where we sample from the variational posterior $ q_\\phi(z|x) $ using the reparameterization trick and maximize the likelihood of the observed data while regularizing the latent space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dca35c1",
   "metadata": {},
   "source": [
    "##  VAE Implementation\n",
    "\n",
    "### Prior Class\n",
    "\n",
    "In the current implementation, the prior is a simple standard Gaussian distribution. Although we could use a built-in PyTorch distribution, we chose not to for two reasons:\n",
    "1. It is important to think of the prior as a crucial component in VAEs.\n",
    "2. We can implement a learnable prior (e.g., a flow-based prior, VampPrior, a mixture of distributions).\n",
    "\n",
    "```python\n",
    "class Prior(nn.Module):\n",
    "    def __init__(self, L):\n",
    "        super(Prior, self).__init__()\n",
    "        self.L = L\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        # Sample from a standard Gaussian distribution.\n",
    "        z = torch.randn((batch_size, self.L))\n",
    "        return z\n",
    "\n",
    "    def log_prob(self, z):\n",
    "        # Log-probability of z from a standard normal distribution.\n",
    "        return log_standard_normal(z)\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, encoder_net, decoder_net, num_vals=256, L=16, likelihood_type='categorical'):\n",
    "        super(VAE, self).__init__()\n",
    "        print('VAE by JT.')\n",
    "\n",
    "        self.encoder = Encoder(encoder_net=encoder_net)\n",
    "        self.decoder = Decoder(distribution=likelihood_type, decoder_net=decoder_net, num_vals=num_vals)\n",
    "        self.prior = Prior(L=L)\n",
    "\n",
    "        self.num_vals = num_vals\n",
    "        self.likelihood_type = likelihood_type\n",
    "\n",
    "    def forward(self, x, reduction='avg'):\n",
    "        # Encoder\n",
    "        mu_e, log_var_e = self.encoder.encode(x)\n",
    "        z = self.encoder.sample(mu_e=mu_e, log_var_e=log_var_e)\n",
    "\n",
    "        # ELBO Calculation\n",
    "        RE = self.decoder.log_prob(x, z)\n",
    "        KL = (self.prior.log_prob(z) - self.encoder.log_prob(mu_e=mu_e, log_var_e=log_var_e, z=z)).sum(-1)\n",
    "\n",
    "        if reduction == 'sum':\n",
    "            return -(RE + KL).sum()\n",
    "        else:\n",
    "            return -(RE + KL).mean()\n",
    "\n",
    "    def sample(self, batch_size=64):\n",
    "        z = self.prior.sample(batch_size=batch_size)\n",
    "        return self.decoder.sample(z)\n",
    "# Example of Encoder Network\n",
    "encoder = nn.Sequential(\n",
    "    nn.Linear(D, M), nn.LeakyReLU(),\n",
    "    nn.Linear(M, M), nn.LeakyReLU(),\n",
    "    nn.Linear(M, 2 * L)  # Output mean and log-variance\n",
    ")\n",
    "\n",
    "# Example of Decoder Network\n",
    "decoder = nn.Sequential(\n",
    "    nn.Linear(L, M), nn.LeakyReLU(),\n",
    "    nn.Linear(M, M), nn.LeakyReLU(),\n",
    "    nn.Linear(M, num_vals * D)  # Output the probability distribution\n",
    ")class VAE(nn.Module):\n",
    "    def __init__(self, encoder_net, decoder_net, num_vals=256, L=16, likelihood_type='categorical'):\n",
    "        super(VAE, self).__init__()\n",
    "        print('VAE by JT.')\n",
    "\n",
    "        # Encoder, Decoder, and Prior initialization\n",
    "        self.encoder = Encoder(encoder_net=encoder_net)\n",
    "        self.decoder = Decoder(distribution=likelihood_type, decoder_net=decoder_net, num_vals=num_vals)\n",
    "        self.prior = Prior(L=L)\n",
    "\n",
    "        self.num_vals = num_vals\n",
    "        self.likelihood_type = likelihood_type\n",
    "\n",
    "    def forward(self, x, reduction='avg'):\n",
    "        # 1. Encoder: Calculate the mean (mu_e) and log variance (log_var_e) from the encoder\n",
    "        mu_e, log_var_e = self.encoder.encode(x)\n",
    "        \n",
    "        # 2. Sample the latent variable z using the reparameterization trick\n",
    "        z = self.encoder.sample(mu_e=mu_e, log_var_e=log_var_e)\n",
    "\n",
    "        # 3. ELBO Calculation\n",
    "        #   - RE (Reconstruction Error) is the negative log-likelihood of the data under the decoder\n",
    "        RE = self.decoder.log_prob(x, z)\n",
    "        \n",
    "        #   - KL Divergence between the posterior and the prior\n",
    "        KL = (self.prior.log_prob(z) - self.encoder.log_prob(mu_e=mu_e, log_var_e=log_var_e, z=z)).sum(-1)\n",
    "\n",
    "        # 4. Total Loss (Negative ELBO)\n",
    "        if reduction == 'sum':\n",
    "            total_loss = -(RE + KL).sum()\n",
    "        else:\n",
    "            total_loss = -(RE + KL).mean()\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def sample(self, batch_size=64):\n",
    "        # Sampling from the prior and passing through the decoder\n",
    "        z = self.prior.sample(batch_size=batch_size)\n",
    "        return self.decoder.sample(z)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2533b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal, MultivariateNormal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the encoder and decoder architectures (simplified for demonstration)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2_mean = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc2_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = torch.relu(self.fc1(x))\n",
    "        mean = self.fc2_mean(h)\n",
    "        logvar = self.fc2_logvar(h)\n",
    "        return mean, logvar\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = torch.relu(self.fc1(z))\n",
    "        output = torch.sigmoid(self.fc2(h))  # assuming binary data for simplicity\n",
    "        return output\n",
    "\n",
    "# Define the VAE model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(input_dim, hidden_dim, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim, input_dim)\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        return self.decoder(z), mean, logvar\n",
    "\n",
    "# Define the loss function (ELBO)\n",
    "def loss_function(recon_x, x, mean, logvar):\n",
    "    # Reconstruction term (RE)\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    \n",
    "    # Regularization term (Ω)\n",
    "    # Using a standard normal prior\n",
    "    p_lambda_z = Normal(torch.zeros_like(mean), torch.ones_like(mean))\n",
    "    q_phi_z = Normal(mean, torch.exp(0.5 * logvar))\n",
    "    \n",
    "    # Cross-entropy (CE) between the aggregated posterior and prior\n",
    "    # We use the KL divergence as a regularizer in VAEs\n",
    "    # The formula for the KL divergence is:\n",
    "    # D_KL(q(z|x) || p(z)) = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    # Where sigma is the std deviation and mu is the mean of the posterior q(z|x)\n",
    "    # In this case, we assume p(z) is a standard normal (mean 0, variance 1)\n",
    "    \n",
    "    # KL divergence term\n",
    "    KL_divergence = torch.sum(0.5 * (torch.exp(logvar) + mean**2 - 1 - logvar))\n",
    "    \n",
    "    # Total loss = Reconstruction + KL divergence\n",
    "    return BCE + KL_divergence\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = 784  # Example for MNIST (28x28)\n",
    "hidden_dim = 400\n",
    "latent_dim = 20\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "# Set up the model, optimizer\n",
    "model = VAE(input_dim, hidden_dim, latent_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Dummy data loader (Replace with actual dataset loading code)\n",
    "# For example, use MNIST data here\n",
    "# Assuming data is already flattened to (batch_size, 784) for MNIST\n",
    "# data_loader = ...\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(data_loader):  # Replace with actual data loader\n",
    "        data = data.view(-1, input_dim)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        recon_batch, mean, logvar = model(data)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_function(recon_batch, data, mean, logvar)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch}, Average Loss: {train_loss / len(data_loader.dataset)}\")\n",
    "\n",
    "# To visualize the aggregated posterior, we can plot the distribution\n",
    "# Example for 2D latent space visualization\n",
    "z_samples = mean.detach().cpu().numpy()\n",
    "plt.scatter(z_samples[:, 0], z_samples[:, 1], alpha=0.5)\n",
    "plt.title('Aggregated Posterior in 2D Latent Space')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0e8167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the architecture of the Encoder and Decoder networks\n",
    "def encoder(x, weights1, bias1, weights2, bias2, weights3, bias3):\n",
    "    \"\"\" Encoder Network: z_mean, z_logvar \"\"\"\n",
    "    h = np.tanh(np.dot(x, weights1) + bias1)  # First layer with tanh activation\n",
    "    z_mean = np.dot(h, weights2) + bias2     # Mean of the latent space\n",
    "    z_logvar = np.dot(h, weights3) + bias3   # Log variance of the latent space\n",
    "    return z_mean, z_logvar\n",
    "\n",
    "def decoder(z, weights1, bias1, weights2, bias2):\n",
    "    \"\"\" Decoder Network: Reconstruct the input data \"\"\"\n",
    "    h = np.tanh(np.dot(z, weights1) + bias1)  # First layer with tanh activation\n",
    "    x_reconstructed = np.dot(h, weights2) + bias2  # Output layer\n",
    "    return x_reconstructed\n",
    "\n",
    "def reparameterize(z_mean, z_logvar):\n",
    "    \"\"\" Reparameterization Trick to sample from q(z|x) \"\"\"\n",
    "    epsilon = np.random.randn(*z_mean.shape)  # Random noise from standard normal\n",
    "    z = z_mean + np.exp(0.5 * z_logvar) * epsilon  # Sample z\n",
    "    return z\n",
    "\n",
    "# Loss function (ELBO)\n",
    "def loss_function(x, x_reconstructed, z_mean, z_logvar):\n",
    "    \"\"\" Compute the ELBO loss: reconstruction error + KL divergence \"\"\"\n",
    "    # Reconstruction error (binary cross entropy)\n",
    "    recon_loss = np.sum((x - x_reconstructed) ** 2)\n",
    "\n",
    "    # KL divergence\n",
    "    kl_div = -0.5 * np.sum(1 + z_logvar - z_mean**2 - np.exp(z_logvar))\n",
    "\n",
    "    # Total loss\n",
    "    return recon_loss + kl_div\n",
    "\n",
    "# Initialize network weights and biases\n",
    "input_dim = 784  # Example: MNIST data (28x28 images flattened)\n",
    "hidden_dim = 400\n",
    "latent_dim = 20\n",
    "output_dim = input_dim\n",
    "\n",
    "# Encoder weights and biases\n",
    "weights1_enc = np.random.randn(input_dim, hidden_dim) * 0.01\n",
    "bias1_enc = np.zeros(hidden_dim)\n",
    "weights2_enc = np.random.randn(hidden_dim, latent_dim) * 0.01\n",
    "bias2_enc = np.zeros(latent_dim)\n",
    "weights3_enc = np.random.randn(hidden_dim, latent_dim) * 0.01\n",
    "bias3_enc = np.zeros(latent_dim)\n",
    "\n",
    "# Decoder weights and biases\n",
    "weights1_dec = np.random.randn(latent_dim, hidden_dim) * 0.01\n",
    "bias1_dec = np.zeros(hidden_dim)\n",
    "weights2_dec = np.random.randn(hidden_dim, output_dim) * 0.01\n",
    "bias2_dec = np.zeros(output_dim)\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 1e-3\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "# Dummy data (replace with real dataset, e.g., MNIST flattened images)\n",
    "# x_data is a batch of flattened images, size [batch_size, input_dim]\n",
    "x_data = np.random.randn(batch_size, input_dim)  # Random data for now\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(batch_size):  # Simulate a batch for training\n",
    "        x = x_data[i]  # Sample a single data point\n",
    "        \n",
    "        # Forward pass: Encoder\n",
    "        z_mean, z_logvar = encoder(x, weights1_enc, bias1_enc, weights2_enc, bias2_enc, weights3_enc, bias3_enc)\n",
    "        \n",
    "        # Reparameterization trick\n",
    "        z = reparameterize(z_mean, z_logvar)\n",
    "        \n",
    "        # Forward pass: Decoder\n",
    "        x_reconstructed = decoder(z, weights1_dec, bias1_dec, weights2_dec, bias2_dec)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_function(x, x_reconstructed, z_mean, z_logvar)\n",
    "        total_loss += loss\n",
    "        \n",
    "        # Backpropagation (gradient descent, simplified version)\n",
    "        # Compute gradients (manually for simplicity)\n",
    "        # Note: Here we would typically use backpropagation, but we'll skip it\n",
    "        # to keep things simple without frameworks like PyTorch\n",
    "\n",
    "        # Update parameters (gradient descent)\n",
    "        weights1_enc -= learning_rate * np.dot(x[:, None], (z_mean - x_reconstructed))  # Simplified update\n",
    "        bias1_enc -= learning_rate * np.sum(z_mean - x_reconstructed)\n",
    "        \n",
    "        # Add similar updates for the other weights and biases...\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Total Loss: {total_loss / batch_size}')\n",
    "    \n",
    "# Visualize aggregated posterior (assuming 2D latent space)\n",
    "# For demonstration, let's assume z_mean is 2D\n",
    "z_samples = np.random.randn(batch_size, 2)  # Placeholder, replace with real latents\n",
    "plt.scatter(z_samples[:, 0], z_samples[:, 1])\n",
    "plt.title('Aggregated Posterior in Latent Space')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4d09d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Basic activation function: Tanh\n",
    "def tanh(x):\n",
    "    return math.tanh(x)\n",
    "\n",
    "# Dot product function\n",
    "def dot_product(vec1, vec2):\n",
    "    return sum(a * b for a, b in zip(vec1, vec2))\n",
    "\n",
    "# Matrix multiplication (for layer transformation)\n",
    "def matmul(A, B):\n",
    "    # A is an m x n matrix, B is an n x p matrix\n",
    "    # Result will be an m x p matrix\n",
    "    result = []\n",
    "    for row in A:\n",
    "        result_row = []\n",
    "        for col in zip(*B):\n",
    "            result_row.append(dot_product(row, col))\n",
    "        result.append(result_row)\n",
    "    return result\n",
    "\n",
    "# Encoder Network: computes z_mean, z_logvar\n",
    "def encoder(x, weights1, bias1, weights2, bias2, weights3, bias3):\n",
    "    h = [tanh(dot_product(x, w) + b) for w, b in zip(weights1, bias1)]\n",
    "    z_mean = [dot_product(h, w) + b for w, b in zip(weights2, bias2)]\n",
    "    z_logvar = [dot_product(h, w) + b for w, b in zip(weights3, bias3)]\n",
    "    return z_mean, z_logvar\n",
    "\n",
    "# Decoder Network: reconstructs the input\n",
    "def decoder(z, weights1, bias1, weights2, bias2):\n",
    "    h = [tanh(dot_product(z, w) + b) for w, b in zip(weights1, bias1)]\n",
    "    x_reconstructed = [dot_product(h, w) + b for w, b in zip(weights2, bias2)]\n",
    "    return x_reconstructed\n",
    "\n",
    "# Reparameterization trick: sample from q(z|x)\n",
    "def reparameterize(z_mean, z_logvar):\n",
    "    epsilon = [random.gauss(0, 1) for _ in range(len(z_mean))]\n",
    "    return [z_m + math.exp(0.5 * z_lv) * e for z_m, z_lv, e in zip(z_mean, z_logvar, epsilon)]\n",
    "\n",
    "# Compute the loss function (ELBO)\n",
    "def loss_function(x, x_reconstructed, z_mean, z_logvar):\n",
    "    # Reconstruction error (mean squared error)\n",
    "    recon_loss = sum((xi - x_reconstructed[i]) ** 2 for i, xi in enumerate(x))\n",
    "    \n",
    "    # KL divergence\n",
    "    kl_div = -0.5 * sum(1 + z_lv - z_m**2 - math.exp(z_lv) for z_m, z_lv in zip(z_mean, z_logvar))\n",
    "    \n",
    "    # Total loss (ELBO)\n",
    "    return recon_loss + kl_div\n",
    "\n",
    "# Initialize network weights and biases\n",
    "input_dim = 784  # Example: MNIST data (28x28 images flattened)\n",
    "hidden_dim = 400\n",
    "latent_dim = 20\n",
    "output_dim = input_dim\n",
    "\n",
    "# Encoder weights and biases (random initialization)\n",
    "weights1_enc = [[random.gauss(0, 0.01) for _ in range(input_dim)] for _ in range(hidden_dim)]\n",
    "bias1_enc = [random.gauss(0, 0.01) for _ in range(hidden_dim)]\n",
    "weights2_enc = [[random.gauss(0, 0.01) for _ in range(hidden_dim)] for _ in range(latent_dim)]\n",
    "bias2_enc = [random.gauss(0, 0.01) for _ in range(latent_dim)]\n",
    "weights3_enc = [[random.gauss(0, 0.01) for _ in range(hidden_dim)] for _ in range(latent_dim)]\n",
    "bias3_enc = [random.gauss(0, 0.01) for _ in range(latent_dim)]\n",
    "\n",
    "# Decoder weights and biases (random initialization)\n",
    "weights1_dec = [[random.gauss(0, 0.01) for _ in range(latent_dim)] for _ in range(hidden_dim)]\n",
    "bias1_dec = [random.gauss(0, 0.01) for _ in range(hidden_dim)]\n",
    "weights2_dec = [[random.gauss(0, 0.01) for _ in range(hidden_dim)] for _ in range(output_dim)]\n",
    "bias2_dec = [random.gauss(0, 0.01) for _ in range(output_dim)]\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "# Dummy data (replace with real dataset)\n",
    "x_data = [[random.gauss(0, 1) for _ in range(input_dim)] for _ in range(batch_size)]\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(batch_size):\n",
    "        x = x_data[i]\n",
    "        \n",
    "        # Forward pass: Encoder\n",
    "        z_mean, z_logvar = encoder(x, weights1_enc, bias1_enc, weights2_enc, bias2_enc, weights3_enc, bias3_enc)\n",
    "        \n",
    "        # Reparameterization trick\n",
    "        z = reparameterize(z_mean, z_logvar)\n",
    "        \n",
    "        # Forward pass: Decoder\n",
    "        x_reconstructed = decoder(z, weights1_dec, bias1_dec, weights2_dec, bias2_dec)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_function(x, x_reconstructed, z_mean, z_logvar)\n",
    "        total_loss += loss\n",
    "        \n",
    "        # Backpropagation (manual gradient updates are omitted for simplicity)\n",
    "        # In a real implementation, we'd compute gradients and update weights here\n",
    "        \n",
    "    print(f'Epoch {epoch+1}, Total Loss: {total_loss / batch_size}')\n",
    "    \n",
    "# Example visualization: Random latent samples\n",
    "latent_samples = [[random.gauss(0, 1) for _ in range(2)] for _ in range(batch_size)]  # Example 2D latent space\n",
    "plt.scatter([z[0] for z in latent_samples], [z[1] for z in latent_samples])\n",
    "plt.title('Aggregated Posterior in Latent Space')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
