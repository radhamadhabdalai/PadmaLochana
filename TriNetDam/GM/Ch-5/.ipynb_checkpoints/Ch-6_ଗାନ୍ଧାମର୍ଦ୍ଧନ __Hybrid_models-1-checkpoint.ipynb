{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2041cea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright (c) 2004 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c7c010f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELBO Value: -153.7712\n",
      "Original x: [2.0, 3.0, 1.5, 4.0]\n",
      "Noisy x (after forward diffusion): [1.908332929757319, 2.4953427413903957, 1.4653547302261312, 4.76221706352085]\n",
      "Cleaned x (after backward diffusion): [1.6470191610098692, 2.4406025147198394, 1.2681118824335853, 4.69274003685799]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "from typing import List\n",
    "\n",
    "# Gaussian distribution helper functions\n",
    "def gaussian_sample(mean: float, stddev: float) -> float:\n",
    "    \"\"\"Generate a random sample from a Gaussian distribution.\"\"\"\n",
    "    return random.gauss(mean, stddev)\n",
    "\n",
    "def gaussian_pdf(x: float, mean: float, stddev: float) -> float:\n",
    "    \"\"\"Calculate the probability density function of a Gaussian distribution.\"\"\"\n",
    "    return (1.0 / (stddev * math.sqrt(2 * math.pi))) * math.exp(-0.5 * ((x - mean) / stddev) ** 2)\n",
    "\n",
    "# Diffusion process functions\n",
    "def forward_diffusion(x: List[float], beta_t: float) -> List[float]:\n",
    "    \"\"\"Forward diffusion process: Adds Gaussian noise to the input.\"\"\"\n",
    "    noisy_x = [gaussian_sample(x_i, math.sqrt(beta_t)) for x_i in x]\n",
    "    return noisy_x\n",
    "\n",
    "def backward_diffusion(z_t: List[float], beta_t: float, model_params: List[float]) -> List[float]:\n",
    "    \"\"\"Backward diffusion process: Removes Gaussian noise based on model parameters.\"\"\"\n",
    "    cleaned_z = [z_i - model_params[i] * math.sqrt(beta_t) for i, z_i in enumerate(z_t)]\n",
    "    return cleaned_z\n",
    "\n",
    "# Variational posteriors for forward and backward diffusion\n",
    "def q_phi(z_t_given_z_t_minus_1: List[float], beta_t: float) -> List[float]:\n",
    "    \"\"\"Variational posterior qφ(zt | zt-1) for the forward diffusion.\"\"\"\n",
    "    return [gaussian_sample(z_t_minus_1, math.sqrt(beta_t)) for z_t_minus_1 in z_t_given_z_t_minus_1]\n",
    "\n",
    "# Kullback-Leibler divergence for Gaussian distributions\n",
    "def kl_divergence(mu1: float, std1: float, mu2: float, std2: float) -> float:\n",
    "    \"\"\"Calculate the Kullback-Leibler divergence between two Gaussian distributions.\"\"\"\n",
    "    return math.log(std2 / std1) + (std1 ** 2 + (mu1 - mu2) ** 2) / (2 * std2 ** 2) - 0.5\n",
    "\n",
    "# ELBO calculation (simplified version)\n",
    "def elbo(x: List[float], z_t: List[float], model_params: List[float], beta_t: float) -> float:\n",
    "    \"\"\"Calculate a simplified ELBO (Evidence Lower Bound) for the diffusion model.\"\"\"\n",
    "    reconstruction_error = sum((x_i - z_i) ** 2 for x_i, z_i in zip(x, z_t))  # L2 loss (simplified)\n",
    "    kl_term = sum(kl_divergence(z_i, 1.0, model_params[i], math.sqrt(beta_t)) for i, z_i in enumerate(z_t))\n",
    "    return -reconstruction_error - kl_term\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example input data (x)\n",
    "    x = [2.0, 3.0, 1.5, 4.0]  # Some example data (e.g., an image vector or feature vector)\n",
    "    T = 10  # Number of diffusion steps\n",
    "    beta_t = 0.1  # Diffusion parameter at step t\n",
    "\n",
    "    # Diffusion model parameters (just an example; in practice, these would be learned)\n",
    "    model_params = [random.uniform(0, 1) for _ in range(len(x))]  # Random parameters for backward diffusion\n",
    "\n",
    "    # Forward diffusion: add noise\n",
    "    noisy_x = forward_diffusion(x, beta_t)\n",
    "\n",
    "    # Backward diffusion: remove noise using model parameters\n",
    "    cleaned_x = backward_diffusion(noisy_x, beta_t, model_params)\n",
    "\n",
    "    # Calculate ELBO for the model (simplified)\n",
    "    elbo_value = elbo(x, noisy_x, model_params, beta_t)\n",
    "    print(f\"ELBO Value: {elbo_value:.4f}\")\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Original x: {x}\")\n",
    "    print(f\"Noisy x (after forward diffusion): {noisy_x}\")\n",
    "    print(f\"Cleaned x (after backward diffusion): {cleaned_x}\")\n"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    },
    "image-3.png": {
     "image/png": "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"
    },
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "5c1b7bc7",
   "metadata": {},
   "source": [
    "## Hybrid Modeling\n",
    "\n",
    "###  Introduction\n",
    "\n",
    "In Chapter 1, I tried to convince you that learning the conditional distribution $ p(y|x) $ is not enough, and instead, we should focus on the joint distribution $ p(x, y) $ factorized as follows:\n",
    "\n",
    "$$\n",
    "p(x, y) = p(y|x)p(x)\n",
    "$$\n",
    "\n",
    "Why? Let me remind you of my reasoning. The conditional $ p(y|x) $ does not allow us to say anything about $ x $, but, instead, it will do its best to provide a decision. As a result, I can provide an object that has never been observed so far, and $ p(y|x) $ could still be pretty certain about its decision (i.e., assigning high probability to one class). On the other hand, once we have trained $ p(x) $, we should be able to, at least in theory, access the probability of the given object. And, eventually, determine whether our decision is reliable or not.\n",
    "\n",
    "In the previous chapters, we completely focused on answering the question of how to learn $ p(x) $ alone. Since we had in mind the necessity of using it for evaluating the probability, we discussed only the likelihood-based models, namely, the autoregressive models (ARMs), the flow-based models (flows), and the variational autoencoders (VAEs).\n",
    "\n",
    "Now, the naturally arising question is how to use a deep generative model together with a classifier (or a regressor). Let us focus on a classification task for simplicity and think of possible approaches.\n",
    "\n",
    "##  Approach 1: Let’s Be Naive!\n",
    "\n",
    "Let us start with some easy, naive, almost trivial approaches. In the most straightforward way, we can train $ p(y|x) $ and $ p(x) $ separately. And that is it, we have a classifier and a marginal distribution over objects. This approach is schematically presented in Figure 6.1 where we use different colors (purple and blue) to highlight that we use two different neural networks to parameterize the two distributions.\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "*Fig.1: A naive approach to learning the joint distribution by considering both distributions separately.*\n",
    "\n",
    "Taking the logarithm of the joint distribution yields:\n",
    "\n",
    "$$\n",
    "\\ln p(x, y) = \\ln p_\\alpha (y|x) + \\ln p_\\beta (x)\n",
    "$$\n",
    "\n",
    "where $ \\alpha $ and $ \\beta $ denote parameterizations of both distributions (i.e., neural networks). Once we start training and calculate gradients with respect to $ \\alpha $ and $ \\beta $, we clearly see that we get:\n",
    "\n",
    "$$\n",
    "\\nabla_\\alpha \\ln p(x, y) = \\nabla_\\alpha \\ln p_\\alpha (y|x) + \\nabla_\\alpha \\ln p_\\beta (x)\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\nabla_\\beta \\ln p(x, y) = \\nabla_\\beta \\ln p_\\alpha (y|x) + \\nabla_\\beta \\ln p_\\beta (x)\n",
    "$$\n",
    "\n",
    "Now, let's break these down:\n",
    "\n",
    "- $ \\nabla_\\alpha \\ln p_\\beta (x) = 0 $ because $ \\ln p_\\beta (x) $ is not dependent on $ \\alpha $,\n",
    "- $ \\nabla_\\beta \\ln p_\\alpha (y|x) = 0 $ because $ \\ln p_\\alpha (y|x) $ does not depend on $ \\beta $.\n",
    "\n",
    "In other words, we can simply first train $ p_\\alpha (y|x) $ using all data with labels and then train $ p_\\beta (x) $ using all available data.\n",
    "\n",
    "### What is a potential pitfall with this approach?\n",
    "\n",
    "Intuitively, we can say that there is no guarantee that both distributions treat $ x $ in the same manner and, thus, could introduce some errors. Moreover, due to the stochasticity during training, there is no information flow between random variables $ x $ and $ y $, and as a result, the neural networks seek for their own (local) minima. To use a metaphor, they are like two wings of a bird that move in total separation, completely asynchronously. \n",
    "\n",
    "Moreover, training both models separately is also inefficient. We must use two different neural networks, with no weight sharing. Since training is stochastic, we really could worry about potential bad local optima, and our worries are even doubled now.\n",
    "\n",
    "### Would such an approach fail?\n",
    "\n",
    "Well, there is no simple answer to this question. Probably, it could work pretty well even, but it might lead to models far from optimal ones. Either way, who does like being unclear about training models? At least not me.\n",
    "\n",
    "\n",
    "##  Approach 2: Shared Parameterization!\n",
    "\n",
    "Alright, so since I whine about sharing the parameterization, it is obvious that the second approach uses (drums here) a shared parameterization! To be more precise, a partially shared parameterization assumes that there is a neural network that processes $ x $ and then its output is fed to two neural networks: one for the classifier and one for the marginal distribution over $ x $'s. An example of this approach is depicted in Figure 6.2 (the shared neural network is shown in purple).\n",
    "\n",
    "Now, taking the logarithm of the joint distribution gives:\n",
    "\n",
    "$$\n",
    "\\ln p(x, y) = \\ln p_{\\alpha, \\gamma}(y|x) + \\ln p_{\\beta, \\gamma}(x)\n",
    "$$\n",
    "\n",
    "where it is worth highlighting that both distributions partially share the parameterization $ \\gamma $ (i.e., the purple neural network in Fig.2). As a result, during training, there is a piece of obvious information sharing between $ x $ and $ y $! Intuitively, both distributions operate on a processed $ x $ in the same manner, and then this representation is specialized to give probabilities for classes and objects. \n",
    "\n",
    "Again, one might ask: what is all this fuss about? Well, first of all, now, the two distributions are tightly connected. Like in the metaphor of a bird used before, both wings can move together, in a synchronized fashion. Second, from the optimization perspective, the gradients flow through the $ \\gamma $ network, and thus, it contains information about both $ x $ and $ y $. This may greatly help in finding a better solution.\n",
    "\n",
    "![image-3.png](attachment:image-3.png)\n",
    "\n",
    "*Fig.2: An approach to learning the joint distribution by using a partially shared parameterization.*\n",
    "\n",
    "### Hybrid Modeling\n",
    "\n",
    "At first glance, there is nothing wrong with learning using the training objective expressed as:\n",
    "\n",
    "$$\n",
    "\\ln p(x, y) = \\ln p_{\\alpha, \\gamma}(y|x) + \\ln p_{\\beta, \\gamma}(x)\n",
    "$$\n",
    "\n",
    "However, let us think about the dimensionalities of \\( y \\) and \\( x \\). For instance, if \\( y \\) is binary, then we have one single bit representing a class label. For a binary vector of \\( x \\), we have \\( D \\) bits. Hence, there is a clear discrepancy in scales! Let us take a look at the gradient with respect to \\( \\gamma \\) first, namely:\n",
    "\n",
    "$$\n",
    "\\nabla_\\gamma \\ln p(x, y) = \\nabla_\\gamma \\ln p_{\\alpha, \\gamma}(y|x) + \\nabla_\\gamma \\ln p_{\\beta, \\gamma}(x)\n",
    "$$\n",
    "\n",
    "If we think about it, during training, the \\( \\gamma \\) network obtains a much stronger signal from \\( \\ln p_{\\beta, \\gamma}(x) \\). Following our example of binary variables, let us assume that our neural nets return all probabilities equal to 0.5. So, for the independent Bernoulli variables, we get:\n",
    "\n",
    "$$\n",
    "\\ln \\text{Bern}(y|0.5) = y \\ln 0.5 + (1 − y) \\ln 0.5 = -\\ln 2\n",
    "$$\n",
    "\n",
    "where we use the property of the logarithm (\\( \\ln 0.5 = \\ln 2^{-1} = -\\ln 2 \\)) and it does not matter what the value of \\( y \\) is because the neural network returns \\( 0.5 \\) for \\( y = 0 \\) and \\( y = 1 \\). Similarly, for \\( x \\), we get:\n",
    "\n",
    "$$\n",
    "\\prod_{d=1}^D \\ln \\text{Bern}(x_d|0.5) = \\sum_{d=1}^D \\ln \\text{Bern}(x_d|0.5) = -D \\ln 2\n",
    "$$\n",
    "\n",
    "Therefore, we see that the \\( \\ln p_{\\beta, \\gamma}(x) \\) part is \\( D \\)-times stronger than the \\( \\ln p_{\\alpha, \\gamma}(y|x) \\) part! How does it influence the final gradients during training? \n",
    "\n",
    "Try to visualize a bar of height \\( \\ln 2 \\) and the other that is \\( D \\)-times higher. Now, imagine these bars \"flow\" through \\( \\gamma \\). Do you see it? Yes, the \\( \\gamma \\) neural network will obtain more information from the marginal distribution, and this information could cripple the classification part. In other words, our final model will always be biased toward the marginal part.\n",
    "\n",
    "### Can we do something about it?\n",
    "\n",
    "Fortunately, yes! In [1], it was proposed to consider the convex combination of \\( \\ln p(y|x) \\) and \\( \\ln p(x) \\) as the objective function, namely:\n",
    "\n",
    "$$\n",
    "L(x, y; \\lambda) = (1 - \\lambda) \\ln p(y|x) + \\lambda \\ln p(x)\n",
    "$$\n",
    "\n",
    "where \\( \\lambda \\in [0, 1] \\). Unfortunately, this weighting scheme is not derived from a well-defined distribution, and it breaks the elegance of the likelihood-based approach. However, if you do not mind being inelegant, then this approach should work well!\n",
    "\n",
    "A different approach is proposed in [2] where only \\( \\ln p(x) \\) is weighted:\n",
    "\n",
    "$$\n",
    "\\ell(x, y; \\lambda) = \\ln p(y|x) + \\lambda \\ln p(x)\n",
    "$$\n",
    "\n",
    "where \\( \\lambda \\geq 0 \\). This kind of weighting was proposed in various forms before (e.g., see [3, 4]). Still, the fudge factor \\( \\lambda \\) is not derived from a probabilistic perspective. However, [2] argues that we can interpret \\( \\lambda \\) as a way of encouraging robustness to input variations. They also mention that scaling \\( \\ln p(x) \\) can be seen as a Jacobian-based regularization penalty. It is still not a valid distribution (because it is equivalent to \\( p(x)^\\lambda \\)), but at least we can provide some interpretations.\n",
    "\n",
    "In [2], the hybrid modeling idea has been pursued with \\( p(x) \\) being modeled by flows (in the paper, they used GLOW [5]), and then, the resulting latents \\( z \\) were used as the input to the classifier. In other words, a flow-based model is used for $ p(x) $, and the invertible neural network (e.g., consisting of coupling layers) is shared with the classifier. Then, the final layers on top of the invertible neural network are used to make a decision \\( y \\). The objective function is $ \\ell(x, y; \\lambda) $ as defined in Eq. (6.9). The approach is schematically presented in Fig.3.\n",
    "\n",
    "There are a couple of interesting properties of this approach. First, we can use the invertible neural network for both generative and discriminative parts of the model. Hence, the flow-based model is well-informed about the label. Second, the weighting $ \\lambda $ allows controlling whether the model is more discriminative or more generative. Third, we can use any flow-based model! GLOW was used in [2]; however, [6] used residual flows and [7] applied invertible DenseNets. Fourth, as presented by [2], we can use any classifier (or regressor), e.g., Bayesian classifiers.\n",
    "\n",
    "A potential drawback of this approach lies in the necessity of determining $ \\lambda $. This is an extra hyperparameter that requires tuning. Moreover, as noticed in previous papers [2, 6, 7], the value of $ \\lambda $ drastically changes the performance of the model from discriminative to generative. That is an open question of how to deal with that.\n",
    "![image-2.png](attachment:image-2.png)\n",
    "  \n",
    "*Fig.3: Hybrid modeling using invertible neural networks and flow-based models.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291e3ef0",
   "metadata": {},
   "source": [
    "## Let’s Implement It!\n",
    "\n",
    "Now, it is time to be more specific and formulate a hybrid model. Let us start with the classifier and consider a fully connected neural network to model the conditional distribution $ p(y|x) $, namely:\n",
    "\n",
    "$$\n",
    "z \\rightarrow \\text{Linear}(D, M) \\rightarrow \\text{ReLU} \\rightarrow \\text{Linear}(M, M) \\rightarrow \\text{ReLU} \\rightarrow \\text{Linear}(M, K) \\rightarrow \\text{Softmax}\n",
    "$$\n",
    "\n",
    "where $ D $ is the dimensionality of $ x $ and $ K $ is the number of classes. The softmax gives us probabilities for each class. Remember that $ z = f^{-1}(x) $, where $ f $ is an invertible neural network.\n",
    "\n",
    "In our example, we use the classifier, so we should take the categorical distribution for the conditional $ p(y|x) $:\n",
    "\n",
    "$$\n",
    "p(y|x) = \\prod_{k=1}^{K} \\theta_k(x) [y = k]\n",
    "$$\n",
    "\n",
    "where $ \\theta_k(x) $ is the softmax value for the $ k $-th class and $ [y = k] $ is the Iverson bracket (i.e., $ [y = k] = 1 $ if $ y $ equals $ k $ and 0 otherwise).\n",
    "\n",
    "Next, we focus on modeling $ p(x) $. We can use any marginal model, e.g., we can apply flows and the change of variable formula, namely:\n",
    "\n",
    "$$\n",
    "p(x) = \\pi(z = f^{-1}(x)) |J_f(x)|^{-1}\n",
    "$$\n",
    "\n",
    "where $ J_f(x) $ denotes the Jacobian of the transformation (i.e., neural network) $ f $ evaluated at $ x $. In the case of the flow, we typically use $ \\pi(z) = N(z | 0, 1) $, i.e., the standard Gaussian distribution.\n",
    "\n",
    "Plugging these all distributions into the objective of the hybrid modeling $ \\ell(x, y; \\lambda) $, we get:\n",
    "\n",
    "$$\n",
    "\\ell(x, y; \\lambda) = \\sum_{k=1}^{K} [y = k] \\ln \\theta_{k,g,f}(x) + \\lambda N(z = f^{-1}(x) | 0, 1) - \\ln |J_f(x)|\n",
    "$$\n",
    "\n",
    "where we additionally highlight that $ \\theta_{k,g,f} $ is parameterized by two neural networks: $ f $ from the flow and $ g $ for the final classification.\n",
    "\n",
    "Now, if we would like to follow [2], we could pick coupling layers as the components of $ f $, and eventually, we would model $ p(x) $ using RealNVP or GLOW, for instance. However, we want to be fancier, and we will utilize integer discrete flows (IDFs) [8, 9]. Why? Because we simply can, and also IDFs do not require calculating the Jacobian. Besides, we can practice a bit of formulating various hybrid models.\n",
    "\n",
    "Let us quickly recall IDFs. First, they operate on $ \\mathbb{Z}^D $, i.e., integers. Second, we need to pick an appropriate $ \\pi(z) $ that in this case could be the discretized logistic (DL), $ \\text{DL}(z | \\mu, \\nu) $ with mean $ \\mu $ and scale $ \\nu $. Since the change of variable formula for discrete random variables does not require calculating the Jacobian (remember no change of volume here!), we can rewrite the hybrid modeling objective as follows:\n",
    "\n",
    "$$\n",
    "\\ell(x, y; \\lambda) = \\sum_{k=1}^{K} [y = k] \\ln \\theta_{k,g,f}(x) + \\lambda \\text{DL}(z = f^{-1}(x) | \\mu, \\nu)\n",
    "$$\n",
    "\n",
    "That’s it! Congratulations, if you have followed all these steps, you have arrived at a new hybrid model that uses IDFs to model the distribution of $ x $. Notice that the classifier takes integers as inputs.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4732fc00",
   "metadata": {},
   "source": [
    "## Code\n",
    "\n",
    "We have all the components to implement our own hybrid integer discrete flow (HybridIDF)! Below, there is a code with a lot of comments that should help to understand every single line of it.\n",
    "\n",
    "```python\n",
    "class HybridIDF(nn.Module):\n",
    "    def __init__(self, netts, classnet, num_flows, alpha=1., D=2):\n",
    "        super(HybridIDF, self).__init__()\n",
    "\n",
    "        print('HybridIDF by JT.')\n",
    "\n",
    "        # Here we use the two options discussed previously: a coupling layer or a generalized invertible transformation\n",
    "        # These formulate the transformation f.\n",
    "        # NOTE: Please pay attention to a new variable here, namely, beta. This is the rezero trick used in (van den Berg et al., 2020).\n",
    "        if len(netts) == 1:\n",
    "            self.t = torch.nn.ModuleList([netts[0]() for _ in range(num_flows)])\n",
    "            self.idf_git = 1\n",
    "            self.beta = nn.Parameter(torch.zeros(len(self.t)))\n",
    "        elif len(netts) == 4:\n",
    "            self.t_a = torch.nn.ModuleList([netts[0]() for _ in range(num_flows)])\n",
    "            self.t_b = torch.nn.ModuleList([netts[1]() for _ in range(num_flows)])\n",
    "            self.t_c = torch.nn.ModuleList([netts[2]() for _ in range(num_flows)])\n",
    "            self.t_d = torch.nn.ModuleList([netts[3]() for _ in range(num_flows)])\n",
    "            self.idf_git = 4\n",
    "            self.beta = nn.Parameter(torch.zeros(len(self.t_a)))\n",
    "        else:\n",
    "            raise ValueError('You can provide either 1 or 4 translation nets.')\n",
    "\n",
    "        # This contains extra layers for classification on top of z.\n",
    "        self.classnet = classnet\n",
    "\n",
    "        # The number of flows (i.e., f’s).\n",
    "        self.num_flows = num_flows\n",
    "\n",
    "        # The rounding operator.\n",
    "        self.round = RoundStraightThrough.apply\n",
    "\n",
    "        # The mean and log-scale for the base distribution pi.\n",
    "        self.mean = nn.Parameter(torch.zeros(1, D))\n",
    "        self.logscale = nn.Parameter(torch.ones(1, D))\n",
    "\n",
    "        # The dimensionality of the input.\n",
    "        self.D = D\n",
    "\n",
    "        # Since using \"lambda\" is confusing for Python, we will use alpha in the code for lambda in previous equations (not confusing at all, right?!)\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # We use the built-in PyTorch loss function. It is for educational purposes! Otherwise, we could use the log-categorical.\n",
    "        self.nll = nn.NLLLoss(reduction='none') #it requires log-softmax as input!!\n",
    "\n",
    "    # The coupling layer as introduced before.\n",
    "    # NOTE: We use the rezero trick!\n",
    "    def coupling(self, x, index, forward=True):\n",
    "        if self.idf_git == 1:\n",
    "            (xa, xb) = torch.chunk(x, 2, 1)\n",
    "\n",
    "            if forward:\n",
    "                yb = xb + self.beta[index] * self.round(self.t[index](xa))\n",
    "            else:\n",
    "                yb = xb - self.beta[index] * self.round(self.t[index](xa))\n",
    "\n",
    "            return torch.cat((xa, yb), 1)\n",
    "\n",
    "        elif self.idf_git == 4:\n",
    "            (xa, xb, xc, xd) = torch.chunk(x, 4, 1)\n",
    "\n",
    "            if forward:\n",
    "                ya = xa + self.beta[index] * self.round(self.t_a[index](torch.cat((xb, xc, xd), 1)))\n",
    "                yb = xb + self.beta[index] * self.round(self.t_b[index](torch.cat((ya, xc, xd), 1)))\n",
    "                yc = xc + self.beta[index] * self.round(self.t_c[index](torch.cat((ya, yb, xd), 1)))\n",
    "                yd = xd + self.beta[index] * self.round(self.t_d[index](torch.cat((ya, yb, yc), 1)))\n",
    "            else:\n",
    "                yd = xd - self.beta[index] * self.round(self.t_d[index](torch.cat((xa, xb, xc), 1)))\n",
    "                yc = xc - self.beta[index] * self.round(self.t_c[index](torch.cat((xa, xb, yd), 1)))\n",
    "                yb = xb - self.beta[index] * self.round(self.t_b[index](torch.cat((xa, yc, yd), 1)))\n",
    "                ya = xa - self.beta[index] * self.round(self.t_a[index](torch.cat((yb, yc, yd), 1)))\n",
    "\n",
    "            return torch.cat((ya, yb, yc, yd), 1)\n",
    "\n",
    "    # The permutation layer.\n",
    "    def permute(self, x):\n",
    "        return x.flip(1)\n",
    "\n",
    "    # The flow transformation: forward pass ...\n",
    "    def f(self, x):\n",
    "        z = x\n",
    "        for i in range(self.num_flows):\n",
    "            z = self.coupling(z, i, forward=True)\n",
    "            z = self.permute(z)\n",
    "        return z\n",
    "\n",
    "    # ... and the inverse pass.\n",
    "    def f_inv(self, z):\n",
    "        x = z\n",
    "        for i in reversed(range(self.num_flows)):\n",
    "            x = self.permute(x)\n",
    "            x = self.coupling(x, i, forward=False)\n",
    "        return x\n",
    "\n",
    "    # A new function: This is used for classification. First, we predict probabilities, and then pick the most probable value.\n",
    "    def classify(self, x):\n",
    "        z = self.f(x)\n",
    "        y_pred = self.classnet(z)  # output: probabilities (i.e., softmax)\n",
    "        return torch.argmax(y_pred, dim=1)\n",
    "\n",
    "    # An auxiliary function: We use it for calculating the classification loss, namely, the negative log-likelihood for p(y|x).\n",
    "    # NOTE: We first apply the invertible transformation f.\n",
    "    def class_loss(self, x, y):\n",
    "        z = self.f(x)\n",
    "        y_pred = self.classnet(z)  # output: probabilities (i.e., softmax)\n",
    "        return self.nll(torch.log(y_pred), y)\n",
    "\n",
    "    def sample(self, batchSize):\n",
    "        # sample z:\n",
    "        z = self.prior_sample(batchSize=batchSize, D=self.D)\n",
    "        # x = f^-1(z)\n",
    "        x = self.f_inv(z)\n",
    "        return x.view(batchSize, 1, self.D)\n",
    "\n",
    "    # The log-probability of the base distribution (a.k.a. prior).\n",
    "    def log_prior(self, x):\n",
    "        log_p = log_integer_probability(x, self.mean, self.logscale)\n",
    "        return log_p.sum(1)\n",
    "\n",
    "    # Sampling from the base distribution.\n",
    "    def prior_sample(self, batchSize, D=2):\n",
    "        # Sample from logistic\n",
    "        y = torch.rand(batchSize, self.D)\n",
    "        x = torch.exp(self.logscale) * torch.log(y / (1. - y)) + self.mean\n",
    "        # And then round it to an integer.\n",
    "        return torch.round(x)\n",
    "\n",
    "    # The forward pass: Now, we use the hybrid model objective!\n",
    "    def forward(self, x, y, reduction='avg'):\n",
    "        z = self.f(x)\n",
    "        y_pred = self.classnet(z)  # output: probabilities (i.e., softmax)\n",
    "\n",
    "        idf_loss = -self.log_prior(z)\n",
    "        class_loss = self.nll(torch.log(y_pred), y)  # remember to use logarithm on top of softmax!\n",
    "\n",
    "        if reduction == 'sum':\n",
    "            return (class_loss + self.alpha * idf_loss).sum()\n",
    "        else:\n",
    "            return (class_loss + self.alpha * idf_loss).mean()\n",
    "# The number of invertible transformations\n",
    "num_flows = 2\n",
    "\n",
    "# Here, we present only for the option 1 IDF.\n",
    "nett = lambda: nn.Sequential(nn.Linear(D // 2, M), nn.LeakyReLU(),\n",
    "                              nn.Linear(M, M), nn.LeakyReLU(),\n",
    "                              nn.Linear(M, D // 2))\n",
    "netts = [nett]\n",
    "\n",
    "# And a three-layered classifier.\n",
    "classnet = nn.Sequential(nn.Linear(D, M), nn.LeakyReLU(),\n",
    "                          nn.Linear(M, M), nn.LeakyReLU(),\n",
    "                          nn.Linear(M, K), nn.Softmax(dim=1))\n",
    "\n",
    "# Init HybridIDF\n",
    "model = HybridIDF(netts, classnet, num_flows, D=D, alpha=alpha)\n",
    "\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Fig.4 An example of outcomes after the training: (a) Randomly selected real images. (b) Unconditional generations from the HybridIDF. (c) An example of a validation curve for the classiﬁcation error. (d) An example of a validation curve for the negative log-likelihood, i.e., .− ln p(x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787c9fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assuming `RoundStraightThrough` is defined elsewhere\n",
    "class RoundStraightThrough(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        output = input.round()\n",
    "        ctx.save_for_backward(input)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input == input.round()] = 0  # Zero gradient on rounded values\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "# Define the HybridIDF class\n",
    "class HybridIDF(nn.Module):\n",
    "    def __init__(self, netts, classnet, num_flows, alpha=1., D=2):\n",
    "        super(HybridIDF, self).__init__()\n",
    "\n",
    "        print('HybridIDF by JT.')\n",
    "\n",
    "        # Configuration for translation nets\n",
    "        if len(netts) == 1:\n",
    "            self.t = torch.nn.ModuleList([netts[0]() for _ in range(num_flows)])\n",
    "            self.idf_git = 1\n",
    "            self.beta = nn.Parameter(torch.zeros(len(self.t)))\n",
    "        elif len(netts) == 4:\n",
    "            self.t_a = torch.nn.ModuleList([netts[0]() for _ in range(num_flows)])\n",
    "            self.t_b = torch.nn.ModuleList([netts[1]() for _ in range(num_flows)])\n",
    "            self.t_c = torch.nn.ModuleList([netts[2]() for _ in range(num_flows)])\n",
    "            self.t_d = torch.nn.ModuleList([netts[3]() for _ in range(num_flows)])\n",
    "            self.idf_git = 4\n",
    "            self.beta = nn.Parameter(torch.zeros(len(self.t_a)))\n",
    "        else:\n",
    "            raise ValueError('You can provide either 1 or 4 translation nets.')\n",
    "\n",
    "        # Extra layers for classification on top of z\n",
    "        self.classnet = classnet\n",
    "\n",
    "        # Number of flows (i.e., f’s)\n",
    "        self.num_flows = num_flows\n",
    "\n",
    "        # The rounding operator\n",
    "        self.round = RoundStraightThrough.apply\n",
    "\n",
    "        # Mean and log-scale for the base distribution pi\n",
    "        self.mean = nn.Parameter(torch.zeros(1, D))\n",
    "        self.logscale = nn.Parameter(torch.ones(1, D))\n",
    "\n",
    "        # Dimensionality of input\n",
    "        self.D = D\n",
    "\n",
    "        # Lambda replacement for Python (alpha)\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # PyTorch's negative log-likelihood loss function\n",
    "        self.nll = nn.NLLLoss(reduction='none')  # Requires log-softmax as input!\n",
    "\n",
    "    # The coupling layer as introduced before\n",
    "    def coupling(self, x, index, forward=True):\n",
    "        if self.idf_git == 1:\n",
    "            (xa, xb) = torch.chunk(x, 2, 1)\n",
    "\n",
    "            if forward:\n",
    "                yb = xb + self.beta[index] * self.round(self.t[index](xa))\n",
    "            else:\n",
    "                yb = xb - self.beta[index] * self.round(self.t[index](xa))\n",
    "\n",
    "            return torch.cat((xa, yb), 1)\n",
    "\n",
    "        elif self.idf_git == 4:\n",
    "            (xa, xb, xc, xd) = torch.chunk(x, 4, 1)\n",
    "\n",
    "            if forward:\n",
    "                ya = xa + self.beta[index] * self.round(self.t_a[index](torch.cat((xb, xc, xd), 1)))\n",
    "                yb = xb + self.beta[index] * self.round(self.t_b[index](torch.cat((ya, xc, xd), 1)))\n",
    "                yc = xc + self.beta[index] * self.round(self.t_c[index](torch.cat((ya, yb, xd), 1)))\n",
    "                yd = xd + self.beta[index] * self.round(self.t_d[index](torch.cat((ya, yb, yc), 1)))\n",
    "            else:\n",
    "                yd = xd - self.beta[index] * self.round(self.t_d[index](torch.cat((xa, xb, xc), 1)))\n",
    "                yc = xc - self.beta[index] * self.round(self.t_c[index](torch.cat((xa, xb, yd), 1)))\n",
    "                yb = xb - self.beta[index] * self.round(self.t_b[index](torch.cat((xa, yc, yd), 1)))\n",
    "                ya = xa - self.beta[index] * self.round(self.t_a[index](torch.cat((yb, yc, yd), 1)))\n",
    "\n",
    "            return torch.cat((ya, yb, yc, yd), 1)\n",
    "\n",
    "    # Permutation layer\n",
    "    def permute(self, x):\n",
    "        return x.flip(1)\n",
    "\n",
    "    # Flow transformation: forward pass\n",
    "    def f(self, x):\n",
    "        z = x\n",
    "        for i in range(self.num_flows):\n",
    "            z = self.coupling(z, i, forward=True)\n",
    "            z = self.permute(z)\n",
    "        return z\n",
    "\n",
    "    # Inverse pass for the flow\n",
    "    def f_inv(self, z):\n",
    "        x = z\n",
    "        for i in reversed(range(self.num_flows)):\n",
    "            x = self.permute(x)\n",
    "            x = self.coupling(x, i, forward=False)\n",
    "        return x\n",
    "\n",
    "    # Classification function\n",
    "    def classify(self, x):\n",
    "        z = self.f(x)\n",
    "        y_pred = self.classnet(z)  # output: probabilities (i.e., softmax)\n",
    "        return torch.argmax(y_pred, dim=1)\n",
    "\n",
    "    # Calculate classification loss (negative log-likelihood for p(y|x))\n",
    "    def class_loss(self, x, y):\n",
    "        z = self.f(x)\n",
    "        y_pred = self.classnet(z)  # output: probabilities (i.e., softmax)\n",
    "        return self.nll(torch.log(y_pred), y)\n",
    "\n",
    "    # Sampling function\n",
    "    def sample(self, batchSize):\n",
    "        z = self.prior_sample(batchSize=batchSize, D=self.D)\n",
    "        x = self.f_inv(z)\n",
    "        return x.view(batchSize, 1, self.D)\n",
    "\n",
    "    # Log-probability of the base distribution (a.k.a. prior)\n",
    "    def log_prior(self, x):\n",
    "        log_p = log_integer_probability(x, self.mean, self.logscale)\n",
    "        return log_p.sum(1)\n",
    "\n",
    "    # Sampling from the base distribution\n",
    "    def prior_sample(self, batchSize, D=2):\n",
    "        y = torch.rand(batchSize, self.D)\n",
    "        x = torch.exp(self.logscale) * torch.log(y / (1. - y)) + self.mean\n",
    "        return torch.round(x)\n",
    "\n",
    "    # Hybrid model objective function: forward pass\n",
    "    def forward(self, x, y, reduction='avg'):\n",
    "        z = self.f(x)\n",
    "        y_pred = self.classnet(z)  # output: probabilities (i.e., softmax)\n",
    "\n",
    "        idf_loss = -self.log_prior(z)\n",
    "        class_loss = self.nll(torch.log(y_pred), y)  # Remember to use logarithm on top of softmax!\n",
    "\n",
    "        if reduction == 'sum':\n",
    "            return (class_loss + self.alpha * idf_loss).sum()\n",
    "        else:\n",
    "            return (class_loss + self.alpha * idf_loss).mean()\n",
    "# Number of invertible transformations (flows)\n",
    "num_flows = 2\n",
    "\n",
    "# Define the IDF net (this one uses 1 translation net)\n",
    "nett = lambda: nn.Sequential(\n",
    "    nn.Linear(D // 2, M), \n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(M, M), \n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(M, D // 2)\n",
    ")\n",
    "\n",
    "netts = [nett]\n",
    "\n",
    "# Define a classifier with 3 layers\n",
    "classnet = nn.Sequential(\n",
    "    nn.Linear(D, M),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(M, M),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(M, K),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "# Initialize the HybridIDF model\n",
    "model = HybridIDF(netts, classnet, num_flows, D=D, alpha=alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206a36c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dimensionality of the input and the hidden layer size\n",
    "D = 2  # Dimensionality of the input data\n",
    "M = 64  # Size of hidden layers\n",
    "K = 10  # Number of classes for classification\n",
    "alpha = 1.0  # Scaling factor for the loss function\n",
    "\n",
    "# Number of invertible transformations (flows)\n",
    "num_flows = 2\n",
    "\n",
    "# Define the IDF net (this one uses 1 translation net)\n",
    "nett = lambda: nn.Sequential(\n",
    "    nn.Linear(D // 2, M), \n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(M, M), \n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(M, D // 2)\n",
    ")\n",
    "\n",
    "netts = [nett]\n",
    "\n",
    "# Define a classifier with 3 layers\n",
    "classnet = nn.Sequential(\n",
    "    nn.Linear(D, M),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(M, M),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(M, K),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "# Initialize the HybridIDF model\n",
    "model = HybridIDF(netts, classnet, num_flows, D=D, alpha=alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09f72189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output probabilities: [0.5, 0.5]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "class SimpleNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize weights and biases with small random values\n",
    "        self.weights1 = [[0.01 for _ in range(input_size)] for _ in range(hidden_size)]\n",
    "        self.weights2 = [[0.01 for _ in range(hidden_size)] for _ in range(output_size)]\n",
    "        self.bias1 = [0.01 for _ in range(hidden_size)]\n",
    "        self.bias2 = [0.01 for _ in range(output_size)]\n",
    "    \n",
    "    def relu(self, x):\n",
    "        \"\"\"ReLU activation function\"\"\"\n",
    "        return max(0, x)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        \"\"\"Softmax function to convert outputs into probabilities\"\"\"\n",
    "        exp_values = [math.exp(i) for i in x]\n",
    "        total = sum(exp_values)\n",
    "        return [i / total for i in exp_values]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the network\"\"\"\n",
    "        # First layer (input to hidden)\n",
    "        hidden = [sum([x[i] * self.weights1[j][i] for i in range(len(x))]) + self.bias1[j] for j in range(len(self.weights1))]\n",
    "        hidden = [self.relu(h) for h in hidden]  # Apply ReLU activation\n",
    "        \n",
    "        # Second layer (hidden to output)\n",
    "        output = [sum([hidden[i] * self.weights2[j][i] for i in range(len(hidden))]) + self.bias2[j] for j in range(len(self.weights2))]\n",
    "        \n",
    "        # Softmax output layer\n",
    "        output = self.softmax(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the model\n",
    "    model = SimpleNN(input_size=2, hidden_size=3, output_size=2)\n",
    "\n",
    "    # Example input\n",
    "    x = [1.0, 2.0]\n",
    "    \n",
    "    # Perform a forward pass through the network\n",
    "    output = model.forward(x)\n",
    "    \n",
    "    # Print the output (softmax probabilities)\n",
    "    print(\"Output probabilities:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0775f56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
