{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7d2c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright (c) 2004 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "498d1fb1",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)\n",
    "\n",
    "Fig.15 Diﬀerent amortization strategies for Sylvester normalizing ﬂows and inverse autore- gressive ﬂows (a) Our inference network produces amortized ﬂow parameters. This strategy is also employed by planar ﬂows. (b) Inverse autoregressive ﬂow [18] introduces a measure of .x dependence through a context variable .h(x). This context acts as an additional input for each transformation. The ﬂow parameters themselves are independent of .x.\n",
    "\n",
    "## Amortizing Flow Parameters\n",
    "\n",
    "When using normalizing flows in an amortized inference setting, the parameters of the base distribution as well as the flow parameters can be functions of the data point $ \\mathbf{x} $ [19]. Figure 5.15 (left) shows a diagram of one Sylvester Normalizing Flow (SNF) step and the amortization procedure.\n",
    "\n",
    "The inference network takes datapoints $ \\mathbf{x} $ as input and provides as an output the mean and variance of $ \\mathbf{z}^{(0)} $, such that $ \\mathbf{z}^{(0)} \\sim \\mathcal{N}(\\mathbf{z} | \\mu_0, \\sigma_0) $. Several SNF transformations are then applied to $ \\mathbf{z}^{(0)} \\rightarrow \\mathbf{z}^{(1)} \\rightarrow \\dots \\rightarrow \\mathbf{z}^{(T)} $, producing a flexible posterior distribution for $ \\mathbf{z}^{(T)} $. \n",
    "\n",
    "All of the flow parameters $ (\\mathbf{R}, \\mathbf{R}^\\sim, \\mathbf{Q}) $ for each transformation are produced as an output by the inference network and are thus fully amortized.\n",
    "\n",
    "### Different Amortization Strategies\n",
    "1. **Amortized Flow Parameters (SNF)**: Our inference network produces amortized flow parameters. This strategy is also employed by planar flows.\n",
    "2. **Inverse Autoregressive Flow (IAF)**: The inverse autoregressive flow [18] introduces a measure of $ \\mathbf{x} $-dependence through a context variable $ h(\\mathbf{x}) $. This context acts as an additional input for each transformation, and the flow parameters themselves are independent of $ \\mathbf{x} $.\n",
    "\n",
    "---\n",
    "\n",
    "## Hyperspherical Latent Space\n",
    "\n",
    "### Motivation\n",
    "\n",
    "In the VAE framework, choosing Gaussian priors and Gaussian posteriors for mathematical convenience leads to a Euclidean latent space. However, this choice could be limiting for the following reasons:\n",
    "\n",
    "- **Low-Dimensional Issues**: In low dimensions, the standard Gaussian probability presents a concentrated probability mass around the mean, encouraging points to cluster at the center. This becomes problematic when the data is divided into multiple clusters. A better-suited prior would be uniform, but this is not well-defined on the hyperplane.\n",
    "- **High-Dimensional Issues**: The standard Gaussian distribution in high dimensions tends to resemble a uniform distribution on the surface of a hypersphere, with the majority of its mass concentrated on the hyperspherical shell (the \"soap bubble\" effect).\n",
    "\n",
    "A natural question is whether it would be better to use a distribution defined on the hypersphere. One such distribution that solves both problems is the **von-Mises-Fisher (vMF)** distribution.\n",
    "\n",
    "### von-Mises-Fisher Distribution\n",
    "\n",
    "The von-Mises-Fisher (vMF) distribution is often described as the Gaussian distribution on a hypersphere. Analogous to a Gaussian, it is parameterized by $ \\mu $ (mean direction) and $ \\kappa \\in \\mathbb{R}, \\mu \\in \\mathbb{R}^{\\geq 0} $. For the special case of $ \\kappa = 0 $, the vMF represents a uniform distribution.\n",
    "\n",
    "The probability density function of the vMF distribution for a random unit vector $ \\mathbf{z} \\in \\mathbb{R}^m $ (or $ \\mathbf{z} \\in S^{m-1} $) is:\n",
    "\n",
    "$$\n",
    "q(\\mathbf{z} | \\mu, \\kappa) = C_m (\\kappa) \\exp \\left( \\kappa \\mu^T \\mathbf{z} \\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "C_m (\\kappa) = \\frac{\\kappa^{\\frac{m}{2} - 1}}{(2\\pi)^{\\frac{m}{2}} I_{\\frac{m}{2}-1} (\\kappa)}\n",
    "$$\n",
    "\n",
    "Here, $ I_v(\\kappa) $ is the modified Bessel function of the first kind at order $ v $, and $ \\| \\mu \\|_2 = 1 $.\n",
    "\n",
    "Interestingly, since we define a distribution over a hypersphere, it is possible to formulate a uniform prior over the hypersphere. Using the vMF distribution as the variational posterior, the Kullback-Leibler (KL) divergence between the vMF distribution and the uniform distribution over $ S^{m-1} $ can be calculated analytically as:\n",
    "\n",
    "$$\n",
    "KL[vMF(\\mu, \\kappa) || Unif(S^{m-1})] = \\frac{1}{2} \\kappa + \\log C_m (\\kappa) - \\log \\Gamma\\left( \\frac{m}{2} \\right)\n",
    "$$\n",
    "\n",
    "### Sampling from vMF\n",
    "\n",
    "Sampling from the von-Mises-Fisher distribution requires the acceptance-rejection sampling procedure. The reparameterization trick can be extended to distributions that can be simulated via rejection sampling, as shown in [62].\n",
    "\n",
    "This allows the creation of a **Hyperspherical VAE** that can efficiently model latent space on the hypersphere, making it more appropriate for complex, multi-modal data distributions.\n",
    "\n",
    "For further details and the reparameterization trick, refer to [33] and [62].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80c8175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class InferenceNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, flow_dim):\n",
    "        super(InferenceNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc_mu = nn.Linear(64, flow_dim)  # Mean for z(0)\n",
    "        self.fc_sigma = nn.Linear(64, flow_dim)  # Variance for z(0)\n",
    "        self.fc_R = nn.Linear(64, flow_dim)  # Flow parameter R\n",
    "        self.fc_Q = nn.Linear(64, flow_dim)  # Flow parameter Q\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        mu = self.fc_mu(x)\n",
    "        sigma = self.fc_sigma(x)\n",
    "        R = self.fc_R(x)\n",
    "        Q = self.fc_Q(x)\n",
    "        return mu, sigma, R, Q\n",
    "def snf_flow(z, R, Q, h, num_steps=5):\n",
    "    \"\"\"\n",
    "    Applies Sylvester normalizing flow (SNF) to the latent vector z\n",
    "    using flow parameters R, Q, and a function h (activation).\n",
    "    \"\"\"\n",
    "    for _ in range(num_steps):\n",
    "        Rz = torch.matmul(R, z)  # R * z(t-1)\n",
    "        h_Rz = h(Rz)  # Apply activation\n",
    "        z = z + torch.matmul(Q, h_Rz)  # z(t) = z(t-1) + Q * h(R * z(t-1))\n",
    "    return z\n",
    "import math\n",
    "import torch\n",
    "from torch.distributions import Distribution\n",
    "\n",
    "class vonMisesFisher(Distribution):\n",
    "    def __init__(self, mu, kappa):\n",
    "        self.mu = mu  # Mean direction (normalized)\n",
    "        self.kappa = kappa  # Concentration parameter\n",
    "        self.dim = mu.size(0)  # Dimensionality of the latent space\n",
    "\n",
    "    def log_prob(self, z):\n",
    "        \"\"\"\n",
    "        Compute the log probability of a point z under the vMF distribution.\n",
    "        \"\"\"\n",
    "        dot_product = torch.matmul(self.mu, z)\n",
    "        normalization_constant = self._log_normalizing_constant(self.kappa)\n",
    "        return self.kappa * dot_product - normalization_constant\n",
    "\n",
    "    def _log_normalizing_constant(self, kappa):\n",
    "        \"\"\"\n",
    "        Compute the log normalizing constant of the vMF distribution.\n",
    "        \"\"\"\n",
    "        return (self.dim / 2 - 1) * torch.log(kappa) - self._log_bessel(self.dim / 2 - 1, kappa)\n",
    "\n",
    "    def _log_bessel(self, v, kappa):\n",
    "        \"\"\"\n",
    "        Compute the log Bessel function for the given order v and parameter kappa.\n",
    "        \"\"\"\n",
    "        # Use an approximation or numerical implementation of the Bessel function\n",
    "        return torch.log(torch.i0(kappa))\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        Sample from the von-Mises-Fisher distribution using rejection sampling.\n",
    "        \"\"\"\n",
    "        # Implement the acceptance-rejection procedure for sampling from vMF\n",
    "        pass\n",
    "class HypersphericalVAE(nn.Module):\n",
    "    def __init__(self, input_dim, flow_dim):\n",
    "        super(HypersphericalVAE, self).__init__()\n",
    "        self.inference_network = InferenceNetwork(input_dim, flow_dim)\n",
    "        self.z0_dim = flow_dim  # Dimensionality of the latent variable\n",
    "        self.flow = snf_flow\n",
    "        self.mu_prior = torch.zeros(flow_dim)  # Prior mean (centered at origin)\n",
    "        self.kappa_prior = 1.0  # Prior concentration\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        Encode input x into the parameters for the prior distribution and flow.\n",
    "        \"\"\"\n",
    "        mu, sigma, R, Q = self.inference_network(x)\n",
    "        return mu, sigma, R, Q\n",
    "\n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        Decode latent variable z back into the original space (e.g., reconstruction).\n",
    "        \"\"\"\n",
    "        # Implement decoder network if needed (not shown here)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform forward pass: encoding, SNF flow, and decoding.\n",
    "        \"\"\"\n",
    "        mu, sigma, R, Q = self.encode(x)\n",
    "        z = mu + sigma * torch.randn_like(mu)  # Sampling from N(mu, sigma)\n",
    "        z_transformed = self.flow(z, R, Q, torch.relu)\n",
    "        return self.decode(z_transformed)\n",
    "\n",
    "    def loss_function(self, x, z):\n",
    "        \"\"\"\n",
    "        Compute the loss function, which includes KL divergence and reconstruction loss.\n",
    "        \"\"\"\n",
    "        # Prior distribution (vMF)\n",
    "        vMF_dist = vonMisesFisher(self.mu_prior, self.kappa_prior)\n",
    "        log_p_z = vMF_dist.log_prob(z)\n",
    "\n",
    "        # Likelihood of data given z (reconstruction term)\n",
    "        # Use reconstruction loss (e.g., MSE, CrossEntropy, depending on data)\n",
    "        reconstruction_loss = torch.mean((x - self.decode(z)) ** 2)\n",
    "\n",
    "        # Total loss = reconstruction loss + KL divergence\n",
    "        return reconstruction_loss - torch.mean(log_p_z)\n",
    "\n",
    "# Example training loop\n",
    "input_dim = 20  # Example input dimension\n",
    "flow_dim = 10   # Latent dimension\n",
    "vae = HypersphericalVAE(input_dim, flow_dim)\n",
    "\n",
    "# Example optimizer\n",
    "optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
    "\n",
    "# Dummy input data\n",
    "x_data = torch.randn(64, input_dim)  # Batch of 64 samples, each of dimension input_dim\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    z = vae(x_data)  # Forward pass\n",
    "    loss = vae.loss_function(x_data, z)  # Calculate loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25be3a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
