{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfadff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright (c) 2004 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9ec331",
   "metadata": {},
   "source": [
    "## Flow Matching\n",
    "\n",
    "### A Different Perspective on Generative Models with ODEs: Continuous Normalizing Flows (CNFs)\n",
    "\n",
    "####  About ODEs, Again\n",
    "\n",
    "Previously, we discussed how generative models could be defined through stochastic differential equations (SDEs) or, equivalently, corresponding probability flow ordinary differential equations (PF-ODEs). We showed that by solving SDEs/ODEs using a numerical solver like backward Euler's method, we obtain an iterative generative procedure of turning noise into data. \n",
    "\n",
    "However, there is a question of whether we need to first formulate an SDE and its PF-ODE equivalent or maybe we can take any ODE to define a generative model. \n",
    "\n",
    "**We recall the definition of an ODE:**\n",
    "\n",
    "$$\n",
    "\\frac{d\\mathbf{x}_t}{dt} = \\mathbf{v}(\\mathbf{x}_t, t),\n",
    "$$\n",
    "\n",
    "where the vector field, $\\mathbf{v}(\\mathbf{x}_t, t)$, defines the dynamics. Parameterizing the vector field with a neural network with weights $\\theta$, $\\mathbf{v}_{\\theta}(\\mathbf{x}_t, t)$, leads to a so-called neural ODE [26]. \n",
    "\n",
    "If we denote by $\\mathbf{x}_0$ the initial condition for this neural ODE, e.g., noise, then by solving it, i.e., integrating over time $t$, we get the output (e.g., data):\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_1 = \\mathbf{x}_0 + \\int_{0}^{1} \\mathbf{v}_{\\theta}(\\mathbf{x}_t, t) dt.\n",
    "$$\n",
    "\n",
    "So far so good, but there is (almost) nothing new compared to score-based generative models where we match scores instead of distributions (i.e., an empirical distribution to a model). Could we formulate a likelihood-based training? \n",
    "\n",
    "The short answer is: Yes. Is it easy? Again, the short answer is no. But let us look into both answers more in detail.\n",
    "\n",
    "#### From the Continuity Equation (Conservation of Mass) to the Instantaneous Change of Variables\n",
    "\n",
    "Again, sampling from an ODE, namely, integrating from $t = 0$ to $t = 1$ is not difficult once we have a model of the vector field. We can use Euler's method for that (or any other numerical solver). However, obtaining the model is problematic if we prefer Fitting a data distribution to a distribution induced by $\\mathbf{v}_{\\theta}(\\mathbf{x}, t)$. After all, starting with a known distribution $\\mathbf{x}_0 \\sim \\pi(\\mathbf{x})$ like standard Gaussian and then solving the ODE yield another distribution! We can express this induced distribution analytically using the continuity equation.\n",
    "\n",
    "Here comes some math (and even physics!), so buckle up and let us dive in. Imagine for a second that probability is a mass (I always think of clay, but it could be water if you prefer), something we can touch with our fingers. Now, let us visualize a pipe of the same cross-section volume across its length in which our mass (e.g., water) flows. At each moment of time, we have some flux of this mass, $\\mathbf{f}_t$, i.e., our (probability) mass is moved according to the vector field (or velocity), $\\mathbf{f}_t(\\mathbf{x}_t) = p_t(\\mathbf{x}_t)\\mathbf{v}(\\mathbf{x}_t, t)$. \n",
    "\n",
    "Since we talk about probability mass (or water flowing through the pipe of the same volume of cross sections everywhere), the mass is conserved, i.e., no new mass (water) (dis)appears (no leaking or pouring in). \n",
    "\n",
    "Mathematically, it means that the change of the mass $\\frac{\\partial p_t(\\mathbf{x}_t)}{\\partial t}$ plus the change of the flux volume in all directions (a.k.a. the divergence of the flux) is constant (i.e., the mass is conserved):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial p_t(\\mathbf{x}_t)}{\\partial t} + \\text{div}(p_t(\\mathbf{x}_t)\\mathbf{v}(\\mathbf{x}_t, t)) = 0, \\quad (9.40)\n",
    "$$\n",
    "\n",
    "where $\\text{div}(\\cdot)$ is the divergence defined as follows:\n",
    "\n",
    "$$\n",
    "\\text{div}(\\mathbf{V}(x_1, ..., x_D)) = \\sum_{d=1}^{D} \\frac{\\partial V_d(x_1,...,x_D)}{\\partial x_d},\n",
    "$$\n",
    "\n",
    "i.e., the sum of first derivatives of $\\mathbf{V}$ over all variables separately.\n",
    "\n",
    "**A Side Note:**\n",
    "\n",
    "The trace of the Jacobian matrix is the divergence of the vector field! For a two-dimensional space and a vector field $\\mathbf{V}(x_1, x_2)$,\n",
    "\n",
    "$$\n",
    "\\text{div}(\\mathbf{V}(x_1, x_2)) = \\frac{\\partial V_1(x_1,x_2)}{\\partial x_1} + \\frac{\\partial V_2(x_1,x_2)}{\\partial x_2} = \\text{Tr} \\begin{pmatrix}\n",
    "\\frac{\\partial V_1(x_1,x_2)}{\\partial x_1} & \\frac{\\partial V_1(x_1,x_2)}{\\partial x_2} \\\\\n",
    "\\frac{\\partial V_2(x_1,x_2)}{\\partial x_1} & \\frac{\\partial V_2(x_1,x_2)}{\\partial x_2}\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "\n",
    "where $\\text{Tr}(A) = \\sum_{i} A_{ii}$ is the trace of a matrix $A$. \n",
    "\n",
    "It turns out that applying identities of vector calculus and the properties of the divergence allows us to write the continuity equation using the logarithm of the probability distribution (a.k.a. the instantaneous change of variables [26]):\n",
    "\n",
    "$$\n",
    "\\frac{d \\ln p(\\mathbf{x}_t)}{dt} + \\text{Tr} \\left( \\frac{\\partial \\mathbf{v}(\\mathbf{x}_t, t)}{\\partial \\mathbf{x}_t} \\right) = 0. \\quad (9.41)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bc15730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the ODE solver (using a simple Euler method for illustration)\n",
    "def ode_solve(func, x0, t, dt=0.01):\n",
    "    \"\"\"\n",
    "    Solves the ODE dx/dt = func(x, t) using Euler's method.\n",
    "\n",
    "    Args:\n",
    "        func: The function defining the ODE.\n",
    "        x0: The initial condition.\n",
    "        t: The time interval (e.g., torch.linspace(0, 1, 100)).\n",
    "        dt: The step size for Euler's method.\n",
    "\n",
    "    Returns:\n",
    "        A tensor containing the solution of the ODE at each time step.\n",
    "    \"\"\"\n",
    "    x = x0.clone()\n",
    "    solution = [x0]\n",
    "    for i in range(len(t) - 1):\n",
    "        x = x + dt * func(x, t[i])\n",
    "        solution.append(x)\n",
    "    return torch.stack(solution)\n",
    "\n",
    "# Define the neural network for the vector field\n",
    "class ODEFunc(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(ODEFunc, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, 50),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(50, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        return self.net(x)\n",
    "\n",
    "# Define the CNF model\n",
    "class CNF(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(CNF, self).__init__()\n",
    "        self.ode_func = ODEFunc(dim)\n",
    "\n",
    "    def forward(self, x0, t):\n",
    "        return ode_solve(self.ode_func, x0, t)\n",
    "\n",
    "# Example usage\n",
    "dim = 2  # Dimensionality of the data\n",
    "cnf = CNF(dim)\n",
    "\n",
    "# Sample initial conditions (e.g., from a standard Gaussian)\n",
    "x0 = torch.randn(100, dim) \n",
    "\n",
    "# Define time points\n",
    "t = torch.linspace(0, 1, 100)\n",
    "\n",
    "# Generate samples\n",
    "x_samples = cnf(x0, t)[-1]  # Take the final time step\n",
    "\n",
    "# Define a loss function (e.g., negative log-likelihood)\n",
    "# ... (Implementation depends on the specific data distribution)\n",
    "\n",
    "# Train the CNF model using an optimizer\n",
    "optimizer = optim.Adam(cnf.parameters(), lr=1e-3)\n",
    "# ... (Training loop)\n",
    "\n",
    "# Note: This is a simplified example. \n",
    "# - You'll need to implement the loss function and training loop.\n",
    "# - Consider using more sophisticated ODE solvers (e.g., Runge-Kutta).\n",
    "# - Explore different neural network architectures for the vector field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eba924dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.9208796620368958\n",
      "Epoch: 100, Loss: 0.08247531205415726\n",
      "Epoch: 200, Loss: 0.026448579505085945\n",
      "Epoch: 300, Loss: 0.01268110889941454\n",
      "Epoch: 400, Loss: 0.007367204874753952\n",
      "Epoch: 500, Loss: 0.004787792451679707\n",
      "Epoch: 600, Loss: 0.003347873454913497\n",
      "Epoch: 700, Loss: 0.0024645954836159945\n",
      "Epoch: 800, Loss: 0.0018847165629267693\n",
      "Epoch: 900, Loss: 0.0014840115327388048\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9553/1200579957.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;31m# Plot the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_samples_final\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_samples_final\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Generated Samples'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cv37/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, plotnonfinite, data, **kwargs)\u001b[0m\n\u001b[1;32m   2819\u001b[0m         \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlinewidths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2820\u001b[0m         \u001b[0medgecolors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0medgecolors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplotnonfinite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplotnonfinite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2821\u001b[0;31m         **({\"data\": data} if data is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2822\u001b[0m     \u001b[0msci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2823\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cv37/lib/python3.7/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1412\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1414\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1416\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cv37/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, plotnonfinite, **kwargs)\u001b[0m\n\u001b[1;32m   4363\u001b[0m         \u001b[0;31m# np.ma.ravel yields an ndarray, not a masked array,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4364\u001b[0m         \u001b[0;31m# unless its argument is a masked array.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4365\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4366\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4367\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/numpy/ma/core.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, a, *args, **params)\u001b[0m\n\u001b[1;32m   6768\u001b[0m             \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6770\u001b[0;31m         \u001b[0mmarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6771\u001b[0m         \u001b[0mmethod_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6772\u001b[0m         \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/numpy/ma/core.py\u001b[0m in \u001b[0;36masanyarray\u001b[0;34m(a, dtype)\u001b[0m\n\u001b[1;32m   8000\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8001\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8002\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmasked_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/numpy/ma/core.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, data, mask, dtype, copy, subok, ndmin, fill_value, keep_mask, hard_mask, shrink, order)\u001b[0m\n\u001b[1;32m   2828\u001b[0m         \u001b[0;31m# Process data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2829\u001b[0m         _data = np.array(data, dtype=dtype, copy=copy,\n\u001b[0;32m-> 2830\u001b[0;31m                          order=order, subok=True, ndmin=ndmin)\n\u001b[0m\u001b[1;32m   2831\u001b[0m         \u001b[0m_baseclass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_baseclass'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2832\u001b[0m         \u001b[0;31m# Check that we're not erasing the mask.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cv37/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the ODE solver (using a simple Euler method for illustration)\n",
    "def ode_solve(func, x0, t, dt=0.01):\n",
    "    \"\"\"\n",
    "    Solves the ODE dx/dt = func(x, t) using Euler's method.\n",
    "\n",
    "    Args:\n",
    "        func: The function defining the ODE.\n",
    "        x0: The initial condition.\n",
    "        t: The time interval (e.g., torch.linspace(0, 1, 100)).\n",
    "        dt: The step size for Euler's method.\n",
    "\n",
    "    Returns:\n",
    "        A tensor containing the solution of the ODE at each time step.\n",
    "    \"\"\"\n",
    "    x = x0.clone()\n",
    "    solution = [x0]\n",
    "    for i in range(len(t) - 1):\n",
    "        x = x + dt * func(x, t[i])\n",
    "        solution.append(x)\n",
    "    return torch.stack(solution)\n",
    "\n",
    "# Define the neural network for the vector field\n",
    "class ODEFunc(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(ODEFunc, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, 50),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(50, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        return self.net(x)\n",
    "\n",
    "# Define the CNF model\n",
    "class CNF(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(CNF, self).__init__()\n",
    "        self.ode_func = ODEFunc(dim)\n",
    "\n",
    "    def forward(self, x0, t):\n",
    "        return ode_solve(self.ode_func, x0, t)\n",
    "\n",
    "# Define the loss function (using a simple example)\n",
    "def loss_fn(x_samples):\n",
    "    # Replace this with your actual loss function\n",
    "    # Here, we use the mean squared error as an example\n",
    "    return torch.mean(x_samples**2)\n",
    "\n",
    "# Training parameters\n",
    "dim = 2  # Dimensionality of the data\n",
    "num_epochs = 1000\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Initialize the CNF model\n",
    "cnf = CNF(dim)\n",
    "optimizer = optim.Adam(cnf.parameters(), lr=learning_rate)\n",
    "\n",
    "# Sample initial conditions (e.g., from a standard Gaussian)\n",
    "x0 = torch.randn(100, dim) \n",
    "\n",
    "# Define time points\n",
    "t = torch.linspace(0, 1, 100)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Generate samples\n",
    "    x_samples = cnf(x0, t)[-1]  # Take the final time step\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = loss_fn(x_samples)\n",
    "\n",
    "    # Optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# Generate samples after training\n",
    "x_samples_final = cnf(x0, t)[-1]\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x_samples_final[:, 0], x_samples_final[:, 1], label='Generated Samples')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title('Generated Samples from CNF')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08d73423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sample: [-0.6341038033481257, 0.5815479984358901]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return (math.exp(x) - math.exp(-x)) / (math.exp(x) + math.exp(-x))\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.weights1 = [[random.uniform(-1, 1) for _ in range(hidden_size)] for _ in range(input_size)]\n",
    "        self.biases1 = [random.uniform(-1, 1) for _ in range(hidden_size)]\n",
    "        self.weights2 = [[random.uniform(-1, 1) for _ in range(output_size)] for _ in range(hidden_size)]\n",
    "        self.biases2 = [random.uniform(-1, 1) for _ in range(output_size)]\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        hidden_layer_inputs = [sum(x * w for x, w in zip(inputs, weights)) + bias \n",
    "                              for weights, bias in zip(self.weights1, self.biases1)]\n",
    "        hidden_layer_outputs = [tanh(x) for x in hidden_layer_inputs]\n",
    "\n",
    "        output_layer_inputs = [sum(x * w for x, w in zip(hidden_layer_outputs, weights)) + bias \n",
    "                               for weights, bias in zip(self.weights2, self.biases2)]\n",
    "        outputs = [sigmoid(x) for x in output_layer_inputs]\n",
    "        return outputs\n",
    "\n",
    "# --- SGBM Specifics (Simplified) ---\n",
    "\n",
    "def score_function(model, x):\n",
    "    # This is a simplified example. \n",
    "    # In a real SGBM, you would implement a more complex score function.\n",
    "    # This example assumes a simple neural network for the score function.\n",
    "    return model.forward(x) \n",
    "\n",
    "def sample_from_prior(dim):\n",
    "    # Sample from a standard Gaussian prior\n",
    "    return [random.gauss(0, 1) for _ in range(dim)]\n",
    "\n",
    "def generate_sample(score_function, num_steps, step_size):\n",
    "    x = sample_from_prior(dim) \n",
    "    for _ in range(num_steps):\n",
    "        # Simplified Euler-Maruyama step\n",
    "        noise = [random.gauss(0, 1) for _ in range(dim)] \n",
    "        score = score_function(x)\n",
    "        x = [x_i + step_size * (score_i + noise_i) for x_i, score_i, noise_i in zip(x, score, noise)] \n",
    "    return x\n",
    "\n",
    "# Example Usage\n",
    "dim = 2  # Dimensionality of the data\n",
    "num_steps = 100\n",
    "step_size = 0.01\n",
    "\n",
    "# Create a neural network for the score function\n",
    "score_model = NeuralNetwork(dim, 10, dim) \n",
    "\n",
    "# Generate a sample\n",
    "generated_sample = generate_sample(score_model.forward, num_steps, step_size)\n",
    "print(\"Generated Sample:\", generated_sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6206c80c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return (math.exp(x) - math.exp(-x)) / (math.exp(x) + math.exp(-x))\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.weights1 = [[random.uniform(-1, 1) for _ in range(hidden_size)] for _ in range(input_size)]\n",
    "        self.biases1 = [random.uniform(-1, 1) for _ in range(hidden_size)]\n",
    "        self.weights2 = [[random.uniform(-1, 1) for _ in range(output_size)] for _ in range(hidden_size)]\n",
    "        self.biases2 = [random.uniform(-1, 1) for _ in range(output_size)]\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        hidden_layer_inputs = [sum(x * w for x, w in zip(inputs, weights)) + bias \n",
    "                              for weights, bias in zip(self.weights1, self.biases1)]\n",
    "        hidden_layer_outputs = [tanh(x) for x in hidden_layer_inputs]\n",
    "\n",
    "        output_layer_inputs = [sum(x * w for x, w in zip(hidden_layer_outputs, weights)) + bias \n",
    "                               for weights, bias in zip(self.weights2, self.biases2)]\n",
    "        outputs = [sigmoid(x) for x in output_layer_inputs]\n",
    "        return outputs\n",
    "\n",
    "# --- SGBM Specifics (Simplified) ---\n",
    "\n",
    "def score_function(model, x):\n",
    "    # This is a simplified example. \n",
    "    # In a real SGBM, you would implement a more complex score function.\n",
    "    # This example assumes a simple neural network for the score function.\n",
    "    return model.forward(x) \n",
    "\n",
    "def sample_from_prior(dim):\n",
    "    # Sample from a standard Gaussian prior\n",
    "    return [random.gauss(0, 1) for _ in range(dim)]\n",
    "\n",
    "def generate_sample(score_function, num_steps, step_size, dim):\n",
    "    x = sample_from_prior(dim) \n",
    "    samples = [x]  # Store samples for plotting\n",
    "    for _ in range(num_steps):\n",
    "        # Simplified Euler-Maruyama step\n",
    "        noise = [random.gauss(0, 1) for _ in range(dim)] \n",
    "        score = score_function(x)\n",
    "        x = [x_i + step_size * (score_i + noise_i) for x_i, score_i, noise_i in zip(x, score, noise)] \n",
    "        samples.append(x)\n",
    "    return samples\n",
    "\n",
    "# Example Usage\n",
    "dim = 2  # Dimensionality of the data\n",
    "num_steps = 100\n",
    "step_size = 0.01\n",
    "\n",
    "# Create a neural network for the score function\n",
    "score_model = NeuralNetwork(dim, 10, dim) \n",
    "\n",
    "# Generate samples\n",
    "generated_samples = generate_sample(score_model.forward, num_steps, step_size, dim)\n",
    "\n",
    "# Extract x and y coordinates for plotting\n",
    "x_coords = [sample[0] for sample in generated_samples]\n",
    "y_coords = [sample[1] for sample in generated_samples]\n",
    "\n",
    "# Plot the samples\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(x_coords, y_coords, marker='o', linestyle='-', label='Generated Samples')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title('SGBM Generated Samples')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba10a58",
   "metadata": {},
   "source": [
    "I know you, my curious reader, you dislike being told about something without proof. It is a side note, so if you do not have time for it, just skip it. But I find this pretty cool and it will be also helpful in our further discussion. \n",
    "\n",
    "Ok, so what do we need? \n",
    "\n",
    "First, a general property:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ln p_t}{\\partial t} = \\frac{1}{p_t} \\frac{\\partial p_t}{\\partial t}, \n",
    "$$\n",
    "\n",
    "or equivalently,\n",
    "\n",
    "$$\n",
    "p_t \\frac{\\partial \\ln p_t}{\\partial t} = \\frac{\\partial p_t}{\\partial t}.\n",
    "$$\n",
    "\n",
    "Second, we have:\n",
    "\n",
    "$$\n",
    "\\text{div}(p_t \\mathbf{v}) = \\langle \\nabla_x p_t, \\mathbf{v} \\rangle + p_t \\text{div}(\\mathbf{v}); \n",
    "$$\n",
    "\n",
    "hence, \n",
    "\n",
    "$$\n",
    "\\frac{1}{p_t} \\text{div}(p_t \\mathbf{v}) = \\langle \\nabla_x \\ln p_t, \\mathbf{v} \\rangle + \\text{div}(\\mathbf{v}).\n",
    "$$\n",
    "\n",
    "Third, we saw already that the divergence of the vector field equals its trace:\n",
    "\n",
    "$$\n",
    "\\text{div}(\\mathbf{v}) = \\text{Tr} \\left( \\frac{\\partial \\mathbf{v}(\\mathbf{x}_t, t)}{\\partial \\mathbf{x}_t} \\right).\n",
    "$$\n",
    "\n",
    "Now, we can plug these three facts into the continuity equation:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial p_t(\\mathbf{x}_t)}{\\partial t} + \\text{div}(p_t(\\mathbf{x}_t)\\mathbf{v}(\\mathbf{x}_t, t)) = 0 \\quad (9.42)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{1}{p_t(\\mathbf{x}_t)} \\frac{\\partial p_t(\\mathbf{x}_t)}{\\partial t} + \\frac{1}{p_t(\\mathbf{x}_t)} \\text{div}(p_t(\\mathbf{x}_t)\\mathbf{v}(\\mathbf{x}_t, t)) = 0 \\quad (9.43)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ln p_t(\\mathbf{x}_t)}{\\partial t} + \\langle \\nabla_x \\ln p_t, \\mathbf{v} \\rangle + \\text{div}(\\mathbf{v}(\\mathbf{x}_t, t)) = 0 \\quad (9.44)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ln p_t(\\mathbf{x}_t)}{\\partial t} = - \\langle \\nabla_x \\ln p_t, \\mathbf{v} \\rangle - \\text{div}(\\mathbf{v}(\\mathbf{x}_t, t)) \\quad (9.45)\n",
    "$$\n",
    "\n",
    "Next, calculating the total derivative of $\\ln p_t(\\mathbf{x}_t)$ and plugging in the above equation for $\\langle \\nabla_x \\ln p_t, \\mathbf{v} \\rangle$, we get the following [30]:\n",
    "\n",
    "$$\n",
    "\\frac{d \\ln p(\\mathbf{x}_t)}{dt} = \\frac{\\partial \\ln p(\\mathbf{x}_t)}{\\partial t} + \\left\\langle \\nabla_x \\ln p(\\mathbf{x}_t), \\frac{d\\mathbf{x}_t}{dt} \\right\\rangle \\quad (9.46)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d \\ln p(\\mathbf{x}_t)}{dt} = \\frac{\\partial \\ln p(\\mathbf{x}_t)}{\\partial t} + \\left\\langle \\nabla_x \\ln p(\\mathbf{x}_t), \\mathbf{v}(\\mathbf{x}_t, t) \\right\\rangle \\quad (9.47)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d \\ln p(\\mathbf{x}_t)}{dt} = \\frac{\\partial \\ln p(\\mathbf{x}_t)}{\\partial t} + \\left\\langle \\nabla_x \\ln p(\\mathbf{x}_t), \\mathbf{v}(\\mathbf{x}_t, t) \\right\\rangle \\quad (9.48)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d \\ln p(\\mathbf{x}_t)}{dt} = - \\text{div}(\\mathbf{v}(\\mathbf{x}_t, t)) \\quad (9.49)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d \\ln p(\\mathbf{x}_t)}{dt} = - \\text{Tr} \\left( \\frac{\\partial \\mathbf{v}(\\mathbf{x}_t, t)}{\\partial \\mathbf{x}_t} \\right) \\quad (9.50)\n",
    "$$\n",
    "\n",
    "Then, by integrating across time, we can compute the total change in log-density as follows:\n",
    "\n",
    "$$\n",
    "\\int_0^1 \\frac{d \\ln p(\\mathbf{x}_t)}{dt} dt = - \\int_0^1 \\text{Tr} \\left( \\frac{\\partial \\mathbf{v}(\\mathbf{x}_t, t)}{\\partial \\mathbf{x}_t} \\right) dt \\quad (9.52)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\ln p(\\mathbf{x}_1) - \\ln \\pi(\\mathbf{x}_0) + \\int_0^1 \\text{Tr} \\left( \\frac{\\partial \\mathbf{v}(\\mathbf{x}_t, t)}{\\partial \\mathbf{x}_t} \\right) dt = 0 \\quad (9.53)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\ln p(\\mathbf{x}_1) = \\ln \\pi(\\mathbf{x}_0) - \\int_0^1 \\text{Tr} \\left( \\frac{\\partial \\mathbf{v}(\\mathbf{x}_t, t)}{\\partial \\mathbf{x}_t} \\right) dt. \\quad (9.54)\n",
    "$$\n",
    "\n",
    "Why do we bother to calculate everything as log-probabilities? Because the last line is a continuous version of the change of variables used for normalizing flows! Here, we have the integral over time of the trace of the Jacobian matrix instead of the sum of the log-determinants of the Jacobian matrix. Therefore, training neural ODEs is similar to training normalizing flows but with continuous time. As a result, neural ODEs in this context are referred to as continuous normalizing flows (CNFs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec0d8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the ODE solver (using a simple Euler method for illustration)\n",
    "def ode_solve(func, x0, t, dt=0.01):\n",
    "    \"\"\"\n",
    "    Solves the ODE dx/dt = func(x, t) using Euler's method.\n",
    "\n",
    "    Args:\n",
    "        func: The function defining the ODE.\n",
    "        x0: The initial condition.\n",
    "        t: The time interval (e.g., torch.linspace(0, 1, 100)).\n",
    "        dt: The step size for Euler's method.\n",
    "\n",
    "    Returns:\n",
    "        A tensor containing the solution of the ODE at each time step.\n",
    "    \"\"\"\n",
    "    x = x0.clone()\n",
    "    solution = [x0]\n",
    "    for i in range(len(t) - 1):\n",
    "        x = x + dt * func(x, t[i])\n",
    "        solution.append(x)\n",
    "    return torch.stack(solution)\n",
    "\n",
    "# Define the neural network for the vector field\n",
    "class ODEFunc(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(ODEFunc, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, 50),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(50, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        return self.net(x)\n",
    "\n",
    "# Define the CNF model\n",
    "class CNF(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(CNF, self).__init__()\n",
    "        self.ode_func = ODEFunc(dim)\n",
    "\n",
    "    def forward(self, x0, t):\n",
    "        return ode_solve(self.ode_func, x0, t)\n",
    "\n",
    "# Define the loss function (based on the CNF derivation)\n",
    "def cnf_loss(cnf, x0, t):\n",
    "    \"\"\"\n",
    "    Calculates the CNF loss based on the change of variables formula.\n",
    "\n",
    "    Args:\n",
    "        cnf: The CNF model.\n",
    "        x0: Initial conditions.\n",
    "        t: Time points.\n",
    "\n",
    "    Returns:\n",
    "        The CNF loss.\n",
    "    \"\"\"\n",
    "    x_t = cnf(x0, t)\n",
    "    # Simplified Jacobian calculation (for demonstration)\n",
    "    # In practice, use automatic differentiation (torch.autograd)\n",
    "    jacobian_trace = torch.zeros_like(x_t[:, 0]) \n",
    "    for i in range(x_t.shape[0]):\n",
    "        jacobian_trace[i] = torch.trace(torch.autograd.functional.jacobian(lambda x: cnf(x, t[i]), x_t[i])) \n",
    "    log_det_jacobian = torch.sum(jacobian_trace, dim=0)  # Integrate over time (simplified)\n",
    "    return -log_det_jacobian \n",
    "\n",
    "# Training parameters\n",
    "dim = 2  # Dimensionality of the data\n",
    "num_epochs = 1000\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Initialize the CNF model\n",
    "cnf = CNF(dim)\n",
    "optimizer = optim.Adam(cnf.parameters(), lr=learning_rate)\n",
    "\n",
    "# Sample initial conditions (e.g., from a standard Gaussian)\n",
    "x0 = torch.randn(100, dim) \n",
    "\n",
    "# Define time points\n",
    "t = torch.linspace(0, 1, 100)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Generate samples\n",
    "    x_t = cnf(x0, t) \n",
    "\n",
    "    # Calculate loss\n",
    "    loss = cnf_loss(cnf, x0, t)\n",
    "\n",
    "    # Optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# Generate samples after training\n",
    "x_samples_final = cnf(x0, t)[-1]  # Take the final time step\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x_samples_final[:, 0], x_samples_final[:, 1], label='Generated Samples')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title('Generated Samples from CNF')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec24592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the ODE solver (using a simple Euler method for illustration)\n",
    "def ode_solve(func, x0, t, dt=0.01):\n",
    "    \"\"\"\n",
    "    Solves the ODE dx/dt = func(x, t) using Euler's method.\n",
    "\n",
    "    Args:\n",
    "        func: The function defining the ODE.\n",
    "        x0: The initial condition.\n",
    "        t: The time interval (e.g., torch.linspace(0, 1, 100)).\n",
    "        dt: The step size for Euler's method.\n",
    "\n",
    "    Returns:\n",
    "        A tensor containing the solution of the ODE at each time step.\n",
    "    \"\"\"\n",
    "    x = x0.clone()\n",
    "    solution = [x0]\n",
    "    for i in range(len(t) - 1):\n",
    "        x = x + dt * func(x, t[i])\n",
    "        solution.append(x)\n",
    "    return torch.stack(solution)\n",
    "\n",
    "# Define the neural network for the vector field\n",
    "class ODEFunc(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(ODEFunc, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, 50),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(50, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        return self.net(x)\n",
    "\n",
    "# Define the CNF model\n",
    "class CNF(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(CNF, self).__init__()\n",
    "        self.ode_func = ODEFunc(dim)\n",
    "\n",
    "    def forward(self, x0, t):\n",
    "        return ode_solve(self.ode_func, x0, t)\n",
    "\n",
    "# Define the loss function (based on the CNF derivation)\n",
    "def cnf_loss(cnf, x0, t):\n",
    "    \"\"\"\n",
    "    Calculates the CNF loss based on the change of variables formula.\n",
    "\n",
    "    Args:\n",
    "        cnf: The CNF model.\n",
    "        x0: Initial conditions.\n",
    "        t: Time points.\n",
    "\n",
    "    Returns:\n",
    "        The CNF loss.\n",
    "    \"\"\"\n",
    "    x_t = cnf(x0, t)\n",
    "    # Jacobian calculation using torch.autograd.grad\n",
    "    jacobian = torch.autograd.grad(torch.sum(x_t, dim=0), x0)[0]  # Sum over time steps\n",
    "    if len(x_t.shape) == 2:  # Check if x_t has two dimensions (batch size and feature dimension)\n",
    "        x_t_flat = x_t.view(-1)  # Reshape into a single vector\n",
    "    jacobian = torch.autograd.grad(torch.sum(x_t_flat), x0)[0]\n",
    "    log_det_jacobian = torch.linalg.det(jacobian)  # Calculate determinant\n",
    "    return -log_det_jacobian \n",
    "\n",
    "# Training parameters\n",
    "dim = 2  # Dimensionality of the data\n",
    "num_epochs = 1000\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Initialize the CNF model\n",
    "cnf = CNF(dim)\n",
    "optimizer = optim.Adam(cnf.parameters(), lr=learning_rate)\n",
    "\n",
    "# Sample initial conditions (e.g., from a standard Gaussian)\n",
    "x0 = torch.randn(100, dim) \n",
    "\n",
    "# Define time points\n",
    "t = torch.linspace(0, 1, 100)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Generate samples\n",
    "    x_t = cnf(x0, t) \n",
    "\n",
    "    # Calculate loss\n",
    "    loss = cnf_loss(cnf, x0, t)\n",
    "\n",
    "    # Optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# Generate samples after training\n",
    "x_samples_final = cnf(x0, t)[-1]  # Take the final time step\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x_samples_final[:, 0], x_samples_final[:, 1], label='Generated Samples')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title('Generated Samples from CNF')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae32e9c",
   "metadata": {},
   "source": [
    "However, unlike in discrete time normalizing flows, we do not require invertibility of v; thus, for given datapoint x \n",
    "1\n",
    "\n",
    " , typically, we cannot simply invert the transformation to obtain x \n",
    "0\n",
    "\n",
    " . However, under pretty mild conditions (namely, v and its first derivative are Lipschitz continuous, e.g., for a neural net with Lipschitz continuous activation functions like SELU or SiLU, among others), we can uniquely solve the following problem [31]:\n",
    "\n",
    "$$\n",
    "x_0 = x_1 - \\int_0^1 v_\\theta(x_t, t) dt,\n",
    "$$\n",
    "\n",
    "with the following initial conditions:\n",
    "\n",
    "$$\n",
    "x_t|_{t=0} = x_{data}.\n",
    "$$\n",
    "\n",
    "where x \n",
    "1\n",
    "\n",
    "  is a datapoint x \n",
    "data\n",
    "\n",
    "  and the difference in log-probability is zero. Note that we solve the problem in the reverse order, namely, from data x \n",
    "1\n",
    "\n",
    "  to noise x \n",
    "0\n",
    "\n",
    " .\n",
    "\n",
    "Calculating the Log-Likelihood Function\n",
    "\n",
    "To sum up, we need to carry out the following steps:\n",
    "\n",
    "Take a datapoint x \n",
    "1\n",
    "\n",
    " =x \n",
    "data\n",
    "\n",
    " .\n",
    "Solve the problem in (9.55) by applying a solver to find x \n",
    "0\n",
    "\n",
    "  and keeping track of traces over time.\n",
    "Calculate the log-likelihood by adding ln(x \n",
    "0\n",
    "\n",
    " ) to the sum of negative traces  \n",
    "0\n",
    "1\n",
    "\n",
    " Tr( \n",
    "x \n",
    "t\n",
    "\n",
    " \n",
    "v \n",
    "\n",
    "\n",
    " (x \n",
    "t\n",
    "\n",
    " ,t)\n",
    "\n",
    " )dt.\n",
    "Now, we can backpropagate through a solver and viola! Or really? What about the complexity of this whole procedure? At first glance, it seems very expensive.\n",
    "\n",
    "9.4.1.4 Hutchinsons Trace Estimator\n",
    "\n",
    "If you remember correctly, my curious reader, the problem with normalizing flows was about calculating the log-determinant of the Jacobian matrix of size DD, which in the general case costs O(D \n",
    "3\n",
    " ). Computing the trace requires O(D \n",
    "2\n",
    " ) since we need the sum of the diagonal, but each entry in the diagonal requires a separate forward propagation, thus, the quadratic complexity. It is better than in the discrete case, but there is another caveat: We need to backpropagate through a numerical solver! No free lunch, I am afraid.\n",
    "\n",
    "Chen et al. [26] proposed to use the adjoint sensitivity method, which could be seen as a version of backpropagation with continuous time. Then, the neural ODE is trained by maximizing the log-likelihood lnp(x \n",
    "1\n",
    "\n",
    " ). However, it requires running the numerical method to solve the ODE and then backpropagating through it for each new datapoint. This is a very costly operation! As a result, we need to look for improvements to cut costs everywhere we can.\n",
    "\n",
    "One trick we can apply is about calculating the trace. By utilizing Hutchinsons trace estimator [31], the quadratic complexity is decreased to O(D), and it is relatively easy to calculate for any square matrix A, namely:\n",
    "\n",
    "$$\n",
    "Tr(A) = E_\\epsilon[\\epsilon^T A \\epsilon],\n",
    "$$\n",
    "\n",
    "where  follows a distribution with zero mean and unit variance, e.g., N(0,I). For a specific , the product of A could be calculated in a single forward pass and it is backpropagatable; therefore, we can estimate the trace by taking M Monte Carlo samples:\n",
    "\n",
    "$$\n",
    "Tr(A) \\approx \\frac{1}{M} \\sum_{m=1}^M \\epsilon_m^T A \\epsilon_m.\n",
    "$$\n",
    "\n",
    "In practice, we take M=1, namely, a single sample of  for every newly coming datapoint. This is a noisy estimate, obviously; however, it is unbiased. As a result, during training with a stochastic gradient-based method, it does not matter too much. Eventually, we obtain a procedure that is O(D) plus the cost of running the adjoint sensitivity method (a specific numerical solver). Overall, not bad, but far from fantastic. We do not even provide a code here, because scaling up CNFs is a known problem. Is there any alternative then? Can we do better? Of course, my curious reader, of course we can!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5ad495",
   "metadata": {},
   "source": [
    "\\section{Going with the Flow: Flow Matching}\n",
    "\n",
    "\\subsection{The Idea}\n",
    "\n",
    "Let us consider the following Ordinary Differential Equation (ODE):\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{dx_t}{dt} = v(x_t, t). \\label{eq:9.59}\n",
    "\\end{equation}\n",
    "\n",
    "In addition to this ODE, we assume a known distribution $q_0(x)$ (e.g., the standard Gaussian) and a data distribution $q_1(x)$.\n",
    "\n",
    "\\textbf{A Side Note:} To stay consistent with the flow matching literature, we now go from noise ($t=0$) to data ($t=1$), which defines the forward dynamics and, thus, the generative processunlike the diffusion models (and score-based models), where the time goes in the other direction, i.e., from data to noise.\n",
    "\n",
    "We know from our discussion on Continuous Normalizing Flows (CNFs) that the distribution defined at any moment $t$ is characterized by the continuity equation. Moreover, by applying the instantaneous change of variables, we can find a solution, i.e., a probability distribution. However, this can be computationally heavy.\n",
    "\n",
    "A key question is: if we knew the vector field $v(x_t, t)$ and distributions $p_t(x)$, how could we train our model? And what would be our model?\n",
    "\n",
    "Recall the score matching approach. Consider the denoising score matching loss. What if we apply a similar approach here, namely, instead of looking for a distribution, we find a model of the vector field $v_\\theta(x_t, t)$. Similar to score matching, we solve the regression problem in the following form:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}_{FM}(\\theta) = \\mathbb{E}_{t \\sim U(0,1), x_t \\sim p_t(x)} \\left\\| v_\\theta(x_t, t) - v(x_t, t) \\right\\|^2. \\label{eq:9.60}\n",
    "\\end{equation}\n",
    "\n",
    "instead of looking for a distribution like in CNFs.\n",
    "\n",
    "In plain words, for any time $t$ sampled uniformly at random, we sample $x_t$ from the distribution $p_t(x)$ (we assume we know it!) and aim at minimizing the difference between the model $v_\\theta(x_t, t)$ and the real vector field $v(x_t, t)$ (we assume we know it!). We refer to this objective as flow matching (FM)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1e3520",
   "metadata": {},
   "source": [
    "## Going with the Flow: Flow Matching\n",
    "\n",
    "#### The Idea\n",
    "\n",
    "Let's consider the Ordinary Differential Equation (ODE):\n",
    "\n",
    "$$ \\frac{dx_t}{dt} = v(x_t, t). \\label{eq:ode} $$\n",
    "\n",
    "We assume known distributions $q_0(x)$ (e.g., standard Gaussian) and $q_1(x)$ (data distribution).\n",
    "\n",
    "Note: In flow matching, we go from noise ($t=0$) to data ($t=1$), unlike diffusion/score-based models.\n",
    "\n",
    "From our CNF discussion, the distribution at time $t$ is given by the continuity equation. Applying the instantaneous change of variables gives a solution (a probability distribution). However, this can be computationally expensive.\n",
    "\n",
    "If we knew $v(x_t, t)$ and $p_t(x)$, how could we train a model?\n",
    "\n",
    "Recall score matching and the denoising score matching loss. Instead of finding a distribution, we model the vector field $v_\\theta(x_t, t)$. Similar to score matching, we solve the regression problem:\n",
    "\n",
    "$$ \\mathcal{L}_{FM}(\\theta) = \\mathbb{E}_{t \\sim U(0,1), x_t \\sim p_t(x)} \\left\\| v_\\theta(x_t, t) - v(x_t, t) \\right\\|^2. \\label{eq:flow_matching_loss} $$\n",
    "\n",
    "Here, $t$ is sampled uniformly from $[0, 1]$, $x_t$ is sampled from $p_t(x)$, and we minimize the difference between the model $v_\\theta(x_t, t)$ and the true vector field $v(x_t, t)$. This is called flow matching (FM).\n",
    "\n",
    "We can also express the key concepts more concisely:\n",
    "\n",
    "*  ODE: Describes the flow: $\\frac{dx_t}{dt} = v(x_t, t)$\n",
    "*   Flow Matching Loss: Trains the model $v_\\theta$: $\\mathcal{L}_{FM}(\\theta) = \\mathbb{E}_{t, x_t} \\left\\| v_\\theta(x_t, t) - v(x_t, t) \\right\\|^2$\n",
    "\n",
    "Some important mathematical symbols:\n",
    "\n",
    "*   $\\frac{dx_t}{dt}$: Derivative of $x$ with respect to $t$\n",
    "*   $v(x_t, t)$: Vector field\n",
    "*   $v_\\theta(x_t, t)$: Model of the vector field\n",
    "*   $q_0(x)$: Initial distribution (e.g., noise)\n",
    "*   $q_1(x)$: Data distribution\n",
    "*   $p_t(x)$: Distribution at time $t$\n",
    "*   $\\mathcal{L}_{FM}(\\theta)$: Flow matching loss\n",
    "*   $\\mathbb{E}_{t, x_t}$: Expectation over $t$ and $x_t$\n",
    "*   $\\left\\| \\cdot \\right\\|^2$: Squared Euclidean norm\n",
    "\n",
    "This provides a more structured and detailed explanation of the core concepts.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4daf5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Define the vector field model\n",
    "class VectorField(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(VectorField, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim + 1, hidden_dim),  # Input: x_t and t\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)       # Output: v_theta(x_t, t)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # Concatenate x and t\n",
    "        xt = torch.cat([x, t.unsqueeze(-1)], dim=-1)\n",
    "        return self.net(xt)\n",
    "\n",
    "# Example data generation (replace with your actual data)\n",
    "def sample_data(batch_size, input_dim):\n",
    "    #Simplified example: Gaussian noise at t=0, and a shifted gaussian at t=1\n",
    "    x0 = torch.randn(batch_size, input_dim) \n",
    "    x1 = torch.randn(batch_size, input_dim) + 3 # shifted gaussian\n",
    "    return x0, x1\n",
    "\n",
    "def get_intermediate_data(x0, x1, t):\n",
    "    #Linear interpolation between x0 and x1\n",
    "    return x0 * (1 - t) + x1 * t\n",
    "\n",
    "# Training loop\n",
    "def train(input_dim, hidden_dim, batch_size, epochs, lr):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = VectorField(input_dim, hidden_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        x0, x1 = sample_data(batch_size, input_dim)\n",
    "        x0 = x0.to(device)\n",
    "        x1 = x1.to(device)\n",
    "\n",
    "        for _ in range(10): #Multiple updates per data sample\n",
    "            t = torch.rand(batch_size).to(device) #Sample t uniformly\n",
    "            xt = get_intermediate_data(x0, x1, t)\n",
    "            \n",
    "            #Simplified \"true\" vector field (linear interpolation)\n",
    "            true_v = x1 - x0\n",
    "            \n",
    "            predicted_v = model(xt, t)\n",
    "            loss = torch.mean((predicted_v - true_v)**2)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "input_dim = 2\n",
    "hidden_dim = 64\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "lr = 1e-3\n",
    "\n",
    "trained_model = train(input_dim, hidden_dim, batch_size, epochs, lr)\n",
    "\n",
    "# Example generation\n",
    "num_samples = 100\n",
    "t_gen = torch.linspace(0, 1, 100).to(trained_model.device) # Time steps for generation\n",
    "x0_gen = torch.randn(num_samples, input_dim).to(trained_model.device)\n",
    "x_gen = torch.zeros(num_samples, len(t_gen), input_dim).to(trained_model.device) #Store generated trajectory\n",
    "\n",
    "x_gen[:, 0, :] = x0_gen\n",
    "\n",
    "for i in range(len(t_gen) - 1):\n",
    "    dt = t_gen[i+1] - t_gen[i]\n",
    "    x_gen[:, i+1, :] = x_gen[:, i, :] + trained_model(x_gen[:, i, :], t_gen[i]) * dt\n",
    "\n",
    "# x_gen now contains the generated trajectories. You can visualize or further process it.\n",
    "print(\"Generation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a9952a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the vector field model (using numpy)\n",
    "def vector_field(x, t, weights):\n",
    "    # Simple linear model for demonstration\n",
    "    input_vector = np.concatenate([x, np.array([t])])\n",
    "    return np.dot(input_vector, weights)\n",
    "\n",
    "# Example data generation (replace with your actual data)\n",
    "def sample_data(batch_size, input_dim):\n",
    "    x0 = np.random.randn(batch_size, input_dim)\n",
    "    x1 = np.random.randn(batch_size, input_dim) + 3  # shifted gaussian\n",
    "    return x0, x1\n",
    "\n",
    "def get_intermediate_data(x0, x1, t):\n",
    "    return x0 * (1 - t) + x1 * t\n",
    "\n",
    "# Loss function (MSE)\n",
    "def loss_fn(predicted_v, true_v):\n",
    "    return np.mean((predicted_v - true_v)**2)\n",
    "\n",
    "# Training loop (using gradient descent)\n",
    "def train(input_dim, hidden_dim, batch_size, epochs, lr):\n",
    "    # Initialize weights randomly\n",
    "    weights_shape = (input_dim + 1, input_dim)\n",
    "    weights = np.random.randn(*weights_shape)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        x0, x1 = sample_data(batch_size, input_dim)\n",
    "        for _ in range(10): #Multiple updates per data batch\n",
    "            t = np.random.rand(batch_size)\n",
    "            xt = get_intermediate_data(x0, x1, t)\n",
    "\n",
    "            true_v = x1 - x0  # Simplified \"true\" vector field\n",
    "\n",
    "            predicted_v = np.array([vector_field(xi, ti, weights) for xi, ti in zip(xt, t)])\n",
    "\n",
    "            # Calculate gradients (manual differentiation for simplicity)\n",
    "            grad = np.zeros_like(weights)\n",
    "            for i in range(batch_size):\n",
    "                input_vector = np.concatenate([xt[i], np.array([t[i]])])\n",
    "                error = predicted_v[i] - true_v[i]\n",
    "                grad += 2 * np.outer(input_vector, error) #Outer product for gradient\n",
    "            grad /= batch_size\n",
    "\n",
    "            # Update weights\n",
    "            weights -= lr * grad\n",
    "\n",
    "            loss = loss_fn(predicted_v, true_v)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
    "\n",
    "    return weights\n",
    "\n",
    "# Example usage\n",
    "input_dim = 2\n",
    "hidden_dim = 64 #Not used in this linear model but kept for consistency\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "lr = 0.01\n",
    "\n",
    "trained_weights = train(input_dim, hidden_dim, batch_size, epochs, lr)\n",
    "\n",
    "# Example generation\n",
    "num_samples = 100\n",
    "t_gen = np.linspace(0, 1, 100)\n",
    "x0_gen = np.random.randn(num_samples, input_dim)\n",
    "x_gen = np.zeros((num_samples, len(t_gen), input_dim))\n",
    "\n",
    "x_gen[:, 0, :] = x0_gen\n",
    "\n",
    "for i in range(len(t_gen) - 1):\n",
    "    dt = t_gen[i+1] - t_gen[i]\n",
    "    for j in range(num_samples): #Iterate over samples for generation\n",
    "        x_gen[j, i+1, :] = x_gen[j, i, :] + vector_field(x_gen[j, i, :], t_gen[i], trained_weights) * dt\n",
    "\n",
    "# Plotting the generated trajectories (for 2D data)\n",
    "if input_dim == 2:\n",
    "    for j in range(num_samples):\n",
    "        plt.plot(x_gen[j, :, 0], x_gen[j, :, 1], alpha=0.5)\n",
    "    plt.xlabel(\"x_1\")\n",
    "    plt.ylabel(\"x_2\")\n",
    "    plt.title(\"Generated Trajectories\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Plotting is only supported for 2D data (input_dim=2).\")\n",
    "print(\"Generation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f4197b",
   "metadata": {},
   "source": [
    "\n",
    "## Conditional Flow Matching (CFM)\n",
    "\n",
    "CFM uses an unconditional model of a conditional vector field. We define conditional distributions $p_t(x|z)$ and a conditional vector field $v(x_t, t; z)$.\n",
    "\n",
    "### Why Conditioning?\n",
    "\n",
    "CFM and FM losses are equal up to a constant, so their gradients are equal :\n",
    "\n",
    "\n",
    "If $p_t(x) > 0$ for all $x \\in \\mathbb{R}^D$ and $t \\in [0, 1]$, then $\\nabla_\\theta \\mathcal{L}_{FM}(\\theta) = \\nabla_\\theta \\mathcal{L}_{CFM}(\\theta)$.\n",
    "\n",
    "### Conditioning Variable $z$\n",
    "\n",
    "Common choices for $z$:\n",
    "\n",
    "1.  $z = x_1$ (data point), $q(z) = q_1(z)$.\n",
    "2.  $z = (x_0, x_1)$ (noise and data), $q(z) = q(x_0)q(x_1)$.\n",
    "\n",
    "### Conditional Probability Paths\n",
    "\n",
    "Assume Gaussian conditional probability paths:\n",
    "\n",
    "$$ p_t(x|z) = \\mathcal{N}(x | \\mu(z, t), \\sigma^2(z, t)I). \\label{eq:conditional_path} $$\n",
    "\n",
    "\n",
    "The vector field is:\n",
    "\n",
    "$$ v(x, t; z) = \\frac{\\dot{\\sigma}(z, t)}{\\sigma(z, t)}(x - \\mu(z, t)) + \\dot{\\mu}(z, t). \\label{eq:conditional_vector_field} $$\n",
    "\n",
    "\n",
    "#### Lipman et al. CFM (fm)\n",
    "\n",
    "$z \\equiv x_1$, and:\n",
    "\n",
    "$$ \\mu(z, t) = tx_1, \\label{eq:lipman_mean} $$\n",
    "$$ \\sigma(z, t) = t\\sigma_{const} - t + 1. \\label{eq:lipman_std} $$\n",
    "\n",
    "This gives:\n",
    "\n",
    "$$ p_t(x|z) = \\mathcal{N}\\left(x | tx_1, (t\\sigma_{const} - t + 1)^2 I\\right), \\label{eq:lipman_path} $$\n",
    "$$ v(x, t; z) = \\frac{x_1 - (1 - \\sigma_{const})x}{1 - (1 - \\sigma_{const})t}. \\label{eq:lipman_vector_field} $$\n",
    "\n",
    "From $p_0(x) = \\mathcal{N}(x|0, I)$ to $p_1(x) = \\mathcal{N}(x|x_1, \\sigma_{const}^2 I)$ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8651b11",
   "metadata": {},
   "source": [
    "I'll create a Markdown Jupyter Notebook with LaTeX formatting for the mathematical equations:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Flow Matching Approaches\n",
    "\n",
    "## Introduction to Conditional Flow Matching (CFM)\n",
    "\n",
    "### Lipman et al. CFM and Tong et al. iCFM Comparison\n",
    "\n",
    "#### Mathematical Formulation\n",
    "\n",
    "Consider the following key components:\n",
    "\n",
    "- $\\mathbf{z} \\equiv (x_0, x_1)$\n",
    "- $q(\\mathbf{z}) = q_0(x_0)q_1(x_1)$\n",
    "\n",
    "##### Tong et al. iCFM Approach\n",
    "\n",
    "Mean and standard deviation definition:\n",
    "\n",
    "$$\\mu(\\mathbf{z}, t) = tx_1 + (1 - t)x_0$$\n",
    "$$\\sigma(\\mathbf{z}, t) = \\sigma_{\\text{const}}$$\n",
    "\n",
    "Where $\\sigma_{\\text{const}} > 0$ is a smoothing constant.\n",
    "\n",
    "##### Probability Path and Vector Field\n",
    "\n",
    "Conditional probability path:\n",
    "$$p_t(x|\\mathbf{z}) = \\mathcal{N}\\left(x \\mid tx_1 + (1 - t)x_0, \\sigma_{\\text{const}}^2 I\\right)$$\n",
    "\n",
    "Vector field:\n",
    "$$v(x, t; \\mathbf{z}) = x_1 - x_0$$\n",
    "\n",
    "#### Boundary Distributions\n",
    "\n",
    "Initial distribution:\n",
    "$$p_0(x) = q_0(x) * \\mathcal{N}(x|0, \\sigma_{\\text{const}}^2 I)$$\n",
    "\n",
    "Final distribution:\n",
    "$$p_1(x) = q_1(x) * \\mathcal{N}(x|0, \\sigma_{\\text{const}}^2 I)$$\n",
    "\n",
    "#### Key Differences\n",
    "\n",
    "| Lipman et al. CFM | Tong et al. iCFM |\n",
    "|-------------------|-------------------|\n",
    "| Starts with standard Gaussian | Starts with small Gaussian |\n",
    "| Evolves to smaller Gaussian | Moves small Gaussian to data space |\n",
    "\n",
    "## Visualization\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Fig.8 A schematic visualization of how ow matching approaches work: (a) An example of Lipman et al. CNF. (b) An example of Tong et al. iCNF. The dotted line indicates the interpolation between noise and data.\n",
    "\n",
    "*Figure: Schematic visualization of flow matching approaches*\n",
    "\n",
    "### Code Example (Conceptual Implementation)\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def flow_matching(x0, x1, t, sigma_const):\n",
    "    \"\"\"\n",
    "    Implement basic flow matching interpolation\n",
    "    \n",
    "    Args:\n",
    "        x0 (np.ndarray): Initial noise point\n",
    "        x1 (np.ndarray): Target data point\n",
    "        t (float): Interpolation parameter [0, 1]\n",
    "        sigma_const (float): Smoothing constant\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Interpolated point\n",
    "    \"\"\"\n",
    "    mean_interpolation = t * x1 + (1 - t) * x0\n",
    "    vector_field = x1 - x0\n",
    "    \n",
    "    return mean_interpolation, vector_field\n",
    "```\n",
    "\n",
    "## Theoretical Insights\n",
    "\n",
    "The flow matching approach provides a flexible framework for:\n",
    "- Interpolating between noise and data distributions\n",
    "- Generating smooth probability paths\n",
    "- Enabling advanced generative modeling techniques\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13469d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions as distributions\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class FlowMatchingBase:\n",
    "    \"\"\"Base class for Flow Matching approaches\"\"\"\n",
    "    def __init__(self, sigma_const=0.1):\n",
    "        self.sigma_const = sigma_const\n",
    "\n",
    "    def interpolate_mean(self, x0, x1, t):\n",
    "        \"\"\"Linear interpolation between noise and data points\"\"\"\n",
    "        return t * x1 + (1 - t) * x0\n",
    "\n",
    "    def vector_field(self, x0, x1):\n",
    "        \"\"\"Basic vector field calculation\"\"\"\n",
    "        return x1 - x0\n",
    "\n",
    "class LipmanCFM(FlowMatchingBase):\n",
    "    \"\"\"Lipman et al. Conditional Flow Matching implementation\"\"\"\n",
    "    def probability_path(self, x0, x1, t):\n",
    "        \"\"\"\n",
    "        Probability path with decreasing standard deviation\n",
    "        \n",
    "        Args:\n",
    "            x0 (torch.Tensor): Initial noise point\n",
    "            x1 (torch.Tensor): Target data point\n",
    "            t (float): Interpolation parameter\n",
    "        \n",
    "        Returns:\n",
    "            torch.distributions.Normal: Probability distribution at time t\n",
    "        \"\"\"\n",
    "        mean = self.interpolate_mean(x0, x1, t)\n",
    "        std = torch.sqrt(1 - t) * self.sigma_const\n",
    "        return distributions.Normal(mean, std)\n",
    "\n",
    "class TongICFM(FlowMatchingBase):\n",
    "    \"\"\"Tong et al. Interpolated Conditional Flow Matching implementation\"\"\"\n",
    "    def probability_path(self, x0, x1, t):\n",
    "        \"\"\"\n",
    "        Probability path with fixed standard deviation\n",
    "        \n",
    "        Args:\n",
    "            x0 (torch.Tensor): Initial noise point\n",
    "            x1 (torch.Tensor): Target data point\n",
    "            t (float): Interpolation parameter\n",
    "        \n",
    "        Returns:\n",
    "            torch.distributions.Normal: Probability distribution at time t\n",
    "        \"\"\"\n",
    "        mean = self.interpolate_mean(x0, x1, t)\n",
    "        return distributions.Normal(mean, self.sigma_const)\n",
    "\n",
    "class FlowMatchingVisualizer:\n",
    "    \"\"\"Visualization tools for Flow Matching approaches\"\"\"\n",
    "    @staticmethod\n",
    "    def plot_probability_paths(cfm_methods, x0, x1, num_steps=10):\n",
    "        \"\"\"\n",
    "        Visualize probability path evolution\n",
    "        \n",
    "        Args:\n",
    "            cfm_methods (list): List of Flow Matching method instances\n",
    "            x0 (torch.Tensor): Initial noise point\n",
    "            x1 (torch.Tensor): Target data point\n",
    "            num_steps (int): Number of interpolation steps\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        for idx, method in enumerate(cfm_methods, 1):\n",
    "            plt.subplot(1, len(cfm_methods), idx)\n",
    "            method_name = method.__class__.__name__\n",
    "            \n",
    "            # Generate probability distributions at different time steps\n",
    "            time_steps = np.linspace(0, 1, num_steps)\n",
    "            \n",
    "            # Plot distributions\n",
    "            for t in time_steps:\n",
    "                dist = method.probability_path(x0, x1, t)\n",
    "                \n",
    "                # Convert to numpy for plotting\n",
    "                x_mean = dist.mean.item()\n",
    "                x_std = dist.stddev.item()\n",
    "                \n",
    "                x = np.linspace(x_mean - 4*x_std, x_mean + 4*x_std, 100)\n",
    "                prob_density = np.exp(dist.log_prob(torch.tensor(x)).numpy())\n",
    "                \n",
    "                plt.plot(x, prob_density, alpha=0.5, label=f't={t:.2f}')\n",
    "            \n",
    "            plt.title(f'{method_name} Probability Path')\n",
    "            plt.xlabel('x')\n",
    "            plt.ylabel('Probability Density')\n",
    "            plt.legend(loc='best')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def main():\n",
    "    # Setup\n",
    "    torch.manual_seed(42)\n",
    "    x0 = torch.tensor([0.0])  # Initial noise point\n",
    "    x1 = torch.tensor([5.0])  # Target data point\n",
    "    \n",
    "    # Flow Matching Methods\n",
    "    lipman_cfm = LipmanCFM(sigma_const=0.1)\n",
    "    tong_icfm = TongICFM(sigma_const=0.1)\n",
    "    \n",
    "    # Visualization\n",
    "    FlowMatchingVisualizer.plot_probability_paths(\n",
    "        [lipman_cfm, tong_icfm], x0, x1\n",
    "    )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ac0b3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"LipmanCFM\": [\n",
      "    {\n",
      "      \"time\": 0.0,\n",
      "      \"mean\": 0.0,\n",
      "      \"std_dev\": 0.1\n",
      "    },\n",
      "    {\n",
      "      \"time\": 0.1111111111111111,\n",
      "      \"mean\": 0.5555555555555556,\n",
      "      \"std_dev\": 0.09428090415820634\n",
      "    },\n",
      "    {\n",
      "      \"time\": 0.2222222222222222,\n",
      "      \"mean\": 1.1111111111111112,\n",
      "      \"std_dev\": 0.08819171036881969\n",
      "    },\n",
      "    {\n",
      "      \"time\": 0.3333333333333333,\n",
      "      \"mean\": 1.6666666666666665,\n",
      "      \"std_dev\": 0.08164965809277261\n",
      "    },\n",
      "    {\n",
      "      \"time\": 0.4444444444444444,\n",
      "      \"mean\": 2.2222222222222223,\n",
      "      \"std_dev\": 0.07453559924999299\n",
      "    },\n",
      "    {\n",
      "      \"time\": 0.5555555555555556,\n",
      "      \"mean\": 2.7777777777777777,\n",
      "      \"std_dev\": 0.06666666666666667\n",
      "    },\n",
      "    {\n",
      "      \"time\": 0.6666666666666666,\n",
      "      \"mean\": 3.333333333333333,\n",
      "      \"std_dev\": 0.05773502691896259\n",
      "    },\n",
      "    {\n",
      "      \"time\": 0.7777777777777778,\n",
      "      \"mean\": 3.888888888888889,\n",
      "      \"std_dev\": 0.04714045207910317\n",
      "    },\n",
      "    {\n",
      "      \"time\": 0.8888888888888888,\n",
      "      \"mean\": 4.444444444444445,\n",
      "      \"std_dev\": 0.03333333333333335\n",
      "    },\n",
      "    {\n",
      "      \"time\": 1.0,\n",
      "      \"mean\": 5.0,\n",
      "      \"std_dev\": 0.0\n",
      "    }\n",
      "  ],\n",
      "  \"TongICFM\": [\n",
      "    {\n",
      "      \"time\": 0.0,\n",
      "      \"mean\": 0.0,\n",
      "      \"std_dev\": 0.1\n",
      "    },\n",
      "    {\n",
      "      \"time\": 0.1111111111111111,\n",
      "      \"mean\": 0.5555555555555556,\n",
      "      \"std_dev\": 0.1\n",
      "    },\n",
      "    {\n",
      "      \"time\": 0.2222222222222222,\n",
      "      \"mean\": 1.1111111111111112,\n",
      "      \"std_dev\": 0.1\n",
      "    },\n",
      "    {\n",
      "      \"time\": 0.3333333333333333,\n",
      "      \"mean\": 1.6666666666666665,\n",
      "      \"std_dev\": 0.1\n",
      "    },\n",
      "    {\n",
      "      \"time\": 0.4444444444444444,\n",
      "      \"mean\": 2.2222222222222223,\n",
      "      \"std_dev\": 0.1\n",
      "    },\n",
      "    {\n",
      "      \"time\": 0.5555555555555556,\n",
      "      \"mean\": 2.7777777777777777,\n",
      "      \"std_dev\": 0.1\n",
      "    },\n",
      "    {\n",
      "      \"time\": 0.6666666666666666,\n",
      "      \"mean\": 3.333333333333333,\n",
      "      \"std_dev\": 0.1\n",
      "    },\n",
      "    {\n",
      "      \"time\": 0.7777777777777778,\n",
      "      \"mean\": 3.888888888888889,\n",
      "      \"std_dev\": 0.1\n",
      "    },\n",
      "    {\n",
      "      \"time\": 0.8888888888888888,\n",
      "      \"mean\": 4.444444444444445,\n",
      "      \"std_dev\": 0.1\n",
      "    },\n",
      "    {\n",
      "      \"time\": 1.0,\n",
      "      \"mean\": 5.0,\n",
      "      \"std_dev\": 0.1\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import sys\n",
    "import json\n",
    "\n",
    "class Distribution:\n",
    "    def __init__(self, mean, std_dev):\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_dev\n",
    "    \n",
    "    def log_prob(self, x):\n",
    "        \"\"\"Calculate log probability using normal distribution formula\"\"\"\n",
    "        return -0.5 * math.log(2 * math.pi * self.std_dev**2) - \\\n",
    "               ((x - self.mean)**2 / (2 * self.std_dev**2))\n",
    "\n",
    "class FlowMatchingBase:\n",
    "    def __init__(self, sigma_const=0.1):\n",
    "        self.sigma_const = sigma_const\n",
    "\n",
    "    def interpolate_mean(self, x0, x1, t):\n",
    "        \"\"\"Linear interpolation between noise and data points\"\"\"\n",
    "        return x1 * t + x0 * (1 - t)\n",
    "\n",
    "    def vector_field(self, x0, x1):\n",
    "        \"\"\"Basic vector field calculation\"\"\"\n",
    "        return x1 - x0\n",
    "\n",
    "class LipmanCFM(FlowMatchingBase):\n",
    "    def probability_path(self, x0, x1, t):\n",
    "        \"\"\"\n",
    "        Probability path with decreasing standard deviation\n",
    "        \n",
    "        Args:\n",
    "            x0: Initial noise point\n",
    "            x1: Target data point\n",
    "            t: Interpolation parameter\n",
    "        \n",
    "        Returns:\n",
    "            Distribution at time t\n",
    "        \"\"\"\n",
    "        mean = self.interpolate_mean(x0, x1, t)\n",
    "        std = math.sqrt(1 - t) * self.sigma_const\n",
    "        return Distribution(mean, std)\n",
    "\n",
    "class TongICFM(FlowMatchingBase):\n",
    "    def probability_path(self, x0, x1, t):\n",
    "        \"\"\"\n",
    "        Probability path with fixed standard deviation\n",
    "        \n",
    "        Args:\n",
    "            x0: Initial noise point\n",
    "            x1: Target data point\n",
    "            t: Interpolation parameter\n",
    "        \n",
    "        Returns:\n",
    "            Distribution at time t\n",
    "        \"\"\"\n",
    "        mean = self.interpolate_mean(x0, x1, t)\n",
    "        return Distribution(mean, self.sigma_const)\n",
    "\n",
    "class FlowMatchingAnalyzer:\n",
    "    @staticmethod\n",
    "    def analyze_probability_paths(cfm_methods, x0, x1, num_steps=10):\n",
    "        \"\"\"\n",
    "        Analyze probability path evolution\n",
    "        \n",
    "        Args:\n",
    "            cfm_methods: List of Flow Matching method instances\n",
    "            x0: Initial noise point\n",
    "            x1: Target data point\n",
    "            num_steps: Number of interpolation steps\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with analysis results\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for method in cfm_methods:\n",
    "            method_name = method.__class__.__name__\n",
    "            method_results = []\n",
    "            \n",
    "            # Generate probability distributions at different time steps\n",
    "            for t in [i/float(num_steps-1) for i in range(num_steps)]:\n",
    "                dist = method.probability_path(x0, x1, t)\n",
    "                \n",
    "                # Calculate key distribution properties\n",
    "                method_results.append({\n",
    "                    'time': t,\n",
    "                    'mean': dist.mean,\n",
    "                    'std_dev': dist.std_dev\n",
    "                })\n",
    "            \n",
    "            results[method_name] = method_results\n",
    "        \n",
    "        return results\n",
    "\n",
    "def main():\n",
    "    # Setup random seed for reproducibility\n",
    "    random.seed(42)\n",
    "    \n",
    "    # Initial parameters\n",
    "    x0 = 0.0  # Initial noise point\n",
    "    x1 = 5.0  # Target data point\n",
    "    \n",
    "    # Flow Matching Methods\n",
    "    lipman_cfm = LipmanCFM(sigma_const=0.1)\n",
    "    tong_icfm = TongICFM(sigma_const=0.1)\n",
    "    \n",
    "    # Analyze Flow Matching Methods\n",
    "    analysis_results = FlowMatchingAnalyzer.analyze_probability_paths(\n",
    "        [lipman_cfm, tong_icfm], x0, x1\n",
    "    )\n",
    "    \n",
    "    # Print results\n",
    "    print(json.dumps(analysis_results, indent=2))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b6dec6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability path plot saved to flow_matching_plot.txt\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "class Distribution:\n",
    "    def __init__(self, mean, std_dev):\n",
    "        self.mean = mean\n",
    "        self.std_dev = max(std_dev, 1e-10)  # Ensure minimum standard deviation\n",
    "    \n",
    "    def prob_density(self, x):\n",
    "        \"\"\"Calculate probability density using normal distribution formula\"\"\"\n",
    "        try:\n",
    "            # Prevent domain errors with careful calculation\n",
    "            log_norm = -0.5 * math.log(2 * math.pi * self.std_dev**2)\n",
    "            exp_term = -((x - self.mean)**2 / (2 * self.std_dev**2))\n",
    "            return max(math.exp(log_norm + exp_term), 1e-10)\n",
    "        except Exception:\n",
    "            return 1e-10\n",
    "\n",
    "class FlowMatchingBase:\n",
    "    def __init__(self, sigma_const=0.1):\n",
    "        self.sigma_const = max(sigma_const, 1e-10)\n",
    "\n",
    "    def interpolate_mean(self, x0, x1, t):\n",
    "        \"\"\"Linear interpolation between noise and data points\"\"\"\n",
    "        return x1 * t + x0 * (1 - t)\n",
    "\n",
    "class LipmanCFM(FlowMatchingBase):\n",
    "    def probability_path(self, x0, x1, t):\n",
    "        \"\"\"Probability path with decreasing standard deviation\"\"\"\n",
    "        mean = self.interpolate_mean(x0, x1, t)\n",
    "        std = max(math.sqrt(1 - t) * self.sigma_const, 1e-10)\n",
    "        return Distribution(mean, std)\n",
    "\n",
    "class TongICFM(FlowMatchingBase):\n",
    "    def probability_path(self, x0, x1, t):\n",
    "        \"\"\"Probability path with fixed standard deviation\"\"\"\n",
    "        mean = self.interpolate_mean(x0, x1, t)\n",
    "        return Distribution(mean, self.sigma_const)\n",
    "\n",
    "class GraphPlotter:\n",
    "    @staticmethod\n",
    "    def plot_probability_paths(methods, x0, x1, num_steps=10, filename='flow_matching_plot.txt'):\n",
    "        \"\"\"Plot probability paths and save to text-based graph\"\"\"\n",
    "        with open(filename, 'w') as f:\n",
    "            for method in methods:\n",
    "                method_name = method.__class__.__name__\n",
    "                f.write(f\"{method_name} Probability Path\\n\")\n",
    "                \n",
    "                # Generate x range with more careful interpolation\n",
    "                x_range = [x0 + (x1 - x0) * i / (num_steps - 1) for i in range(num_steps)]\n",
    "                \n",
    "                # Plot for different time steps\n",
    "                for t in [i/float(num_steps-1) for i in range(num_steps)]:\n",
    "                    dist = method.probability_path(x0, x1, t)\n",
    "                    \n",
    "                    # Create plot line\n",
    "                    plot_line = []\n",
    "                    for x in x_range:\n",
    "                        prob = dist.prob_density(x)\n",
    "                        # Scale probability for visualization\n",
    "                        scaled_prob = int(max(prob * 50, 0))\n",
    "                        plot_line.append('*' * min(scaled_prob, 50))\n",
    "                    \n",
    "                    f.write(f\"t = {t:.2f}: \" + \" \".join(plot_line) + \"\\n\")\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "def main():\n",
    "    # Initial parameters\n",
    "    x0 = 0.0  # Initial noise point\n",
    "    x1 = 5.0  # Target data point\n",
    "    \n",
    "    # Flow Matching Methods\n",
    "    methods = [\n",
    "        LipmanCFM(sigma_const=0.1),\n",
    "        TongICFM(sigma_const=0.1)\n",
    "    ]\n",
    "    \n",
    "    # Plot probability paths\n",
    "    GraphPlotter.plot_probability_paths(methods, x0, x1)\n",
    "    print(\"Probability path plot saved to flow_matching_plot.txt\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499dac07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
