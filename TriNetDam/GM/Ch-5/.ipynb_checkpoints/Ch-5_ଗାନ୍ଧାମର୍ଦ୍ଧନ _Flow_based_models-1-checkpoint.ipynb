{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8c9ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright (c) 2004 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "5ea180c3",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)\n",
    "\n",
    "Fig.1 A diagram presenting a latent variable model and a generative process. Notice the low- dimensional manifold (here 2D) embedded in the high-dimensional space (here 3D).\n",
    "\n",
    "\n",
    "## Latent Variable Models\n",
    "\n",
    "##  Introduction\n",
    "\n",
    "In the previous sections, we discussed two approaches to learning $ p(x) $:\n",
    "- **Autoregressive models (ARMs)** (Chapter 3)\n",
    "- **Flow-based models (Flows)** (Chapter 4)\n",
    "\n",
    "Both ARMs and Flows model the likelihood function directly:\n",
    "1. **ARMs**: Factorize the distribution and parameterize conditional distributions $ p(x_d \\mid x_{<d}) $.\n",
    "2. **Flows**: Utilize invertible transformations (neural networks) and the change of variables formula.\n",
    "\n",
    "Now, we will discuss a third approach that introduces **latent variables**.\n",
    "\n",
    "---\n",
    "\n",
    "### Generative Modeling Scenario\n",
    "\n",
    "Consider a collection of horse images. To learn $ p(x) $, e.g., for generating new images, we can imagine the generative process:\n",
    "1. Sketch the silhouette of the horse, its size, and shape.\n",
    "2. Add details like hooves, head, and color.\n",
    "3. Finally, consider the background.\n",
    "\n",
    "This step-by-step process highlights **factors of variation** (e.g., silhouette, color, background) crucial for generating an object. Using mathematics, this generative process is expressed as:\n",
    "\n",
    "1. Sample a **low-dimensional latent variable** $ \\mathbf{z} \\in \\mathcal{Z}^M $ (hidden factors in the data).\n",
    "2. Generate a high-dimensional object $ \\mathbf{x} \\in \\mathcal{X}^D $ by sampling from the conditional distribution $ p(\\mathbf{x} \\mid \\mathbf{z}) $.\n",
    "\n",
    "---\n",
    "\n",
    "### Generative Process\n",
    "\n",
    "$$\n",
    "\\mathbf{z} \\sim p(\\mathbf{z})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{x} \\sim p(\\mathbf{x} \\mid \\mathbf{z})\n",
    "$$\n",
    "\n",
    "The joint distribution is factorized as:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x}, \\mathbf{z}) = p(\\mathbf{x} \\mid \\mathbf{z}) p(\\mathbf{z}).\n",
    "$$\n",
    "\n",
    "However, for training, we only have access to $ \\mathbf{x} $. Using probabilistic inference, we marginalize over $ \\mathbf{z} $:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x}) = \\int p(\\mathbf{x} \\mid \\mathbf{z}) p(\\mathbf{z}) \\, d\\mathbf{z}. \\tag{5.1}\n",
    "$$\n",
    "\n",
    "Calculating this integral is generally challenging, with two possible approaches:\n",
    "1. **Tractable Integral** (discussed briefly).\n",
    "2. **Approximate Inference** using **Variational Inference** (primary focus).\n",
    "\n",
    "---\n",
    "\n",
    "##  Probabilistic Principal Component Analysis (PPCA)\n",
    "\n",
    "Let us discuss the following scenario:\n",
    "- $ \\mathbf{z} \\in \\mathbb{R}^M $ and $ \\mathbf{x} \\in \\mathbb{R}^D $ (continuous random variables).\n",
    "- Latent variables $ \\mathbf{z} $ follow a standard Gaussian distribution:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{z} \\mid \\mathbf{0}, \\mathbf{I}).\n",
    "$$\n",
    "\n",
    "- The dependency between $ \\mathbf{z} $ and $ \\mathbf{x} $ is **linear**, with **Gaussian additive noise**:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\mathbf{W}\\mathbf{z} + \\mathbf{\\mu} + \\boldsymbol{\\epsilon}, \\quad \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I}),\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ \\mathbf{W} \\in \\mathbb{R}^{D \\times M} $ is a linear transformation matrix.\n",
    "- $ \\mathbf{\\mu} \\in \\mathbb{R}^D $ is the mean vector.\n",
    "- $ \\boldsymbol{\\epsilon} $ is Gaussian noise with variance $ \\sigma^2 $.\n",
    "\n",
    "---\n",
    "\n",
    "### Likelihood Function\n",
    "\n",
    "The marginal likelihood of $ \\mathbf{x} $ is given by:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x}) = \\int p(\\mathbf{x} \\mid \\mathbf{z}) p(\\mathbf{z}) \\, d\\mathbf{z}.\n",
    "$$\n",
    "\n",
    "Using the linear dependency, $ p(\\mathbf{x} \\mid \\mathbf{z}) $ is Gaussian:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x} \\mid \\mathbf{z}) = \\mathcal{N}(\\mathbf{x} \\mid \\mathbf{W}\\mathbf{z} + \\mathbf{\\mu}, \\sigma^2 \\mathbf{I}).\n",
    "$$\n",
    "\n",
    "The prior \\( p(\\mathbf{z}) \\) is also Gaussian:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{z} \\mid \\mathbf{0}, \\mathbf{I}).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Marginal Distribution\n",
    "\n",
    "The marginal $ p(\\mathbf{x}) $ becomes a Gaussian distribution:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x}) = \\mathcal{N}(\\mathbf{x} \\mid \\mathbf{\\mu}, \\mathbf{C}),\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "\\mathbf{C} = \\mathbf{W}\\mathbf{W}^\\top + \\sigma^2 \\mathbf{I}.\n",
    "$$\n",
    "\n",
    "This concludes the description of PPCA in the context of latent variable models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4512571b",
   "metadata": {},
   "source": [
    "## Probabilistic Principal Component Analysis (pPCA)\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "We consider the following setup:\n",
    "- $ \\mathbf{z} \\in \\mathbb{R}^M $ and $ \\mathbf{x} \\in \\mathbb{R}^D $ (continuous random variables).\n",
    "- Latent variables $ \\mathbf{z} $ follow a standard Gaussian distribution:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{z} \\mid \\mathbf{0}, \\mathbf{I}).\n",
    "$$\n",
    "\n",
    "- The dependency between $ \\mathbf{z} $ and $ \\mathbf{x} $ is **linear**, with Gaussian additive noise:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\mathbf{W}\\mathbf{z} + \\mathbf{b} + \\boldsymbol{\\epsilon}, \\quad \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\boldsymbol{\\epsilon} \\mid \\mathbf{0}, \\sigma^2 \\mathbf{I}). \\tag{5.2}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Conditional Distribution\n",
    "\n",
    "Using the properties of Gaussian distributions, the conditional probability $ p(\\mathbf{x} \\mid \\mathbf{z}) $ is:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x} \\mid \\mathbf{z}) = \\mathcal{N}(\\mathbf{x} \\mid \\mathbf{W}\\mathbf{z} + \\mathbf{b}, \\sigma^2 \\mathbf{I}). \\tag{5.3}\n",
    "$$\n",
    "\n",
    "This model is known as the **probabilistic Principal Component Analysis (pPCA)**.\n",
    "\n",
    "---\n",
    "\n",
    "### Marginal Likelihood\n",
    "\n",
    "The marginal likelihood $ p(\\mathbf{x}) $ is calculated by integrating over $ \\mathbf{z} $:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x}) = \\int p(\\mathbf{x} \\mid \\mathbf{z}) p(\\mathbf{z}) \\, d\\mathbf{z}. \\tag{5.4}\n",
    "$$\n",
    "\n",
    "Substituting the Gaussian distributions:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x}) = \\int \\mathcal{N}(\\mathbf{x} \\mid \\mathbf{W}\\mathbf{z} + \\mathbf{b}, \\sigma^2 \\mathbf{I}) \\mathcal{N}(\\mathbf{z} \\mid \\mathbf{0}, \\mathbf{I}) \\, d\\mathbf{z}. \\tag{5.5}\n",
    "$$\n",
    "\n",
    "The result of this integration is another Gaussian distribution:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x}) = \\mathcal{N}(\\mathbf{x} \\mid \\mathbf{b}, \\mathbf{W}\\mathbf{W}^\\top + \\sigma^2 \\mathbf{I}). \\tag{5.6}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Posterior Distribution\n",
    "\n",
    "Due to the properties of Gaussian distributions, the posterior $ p(\\mathbf{z} \\mid \\mathbf{x}) $ can also be computed analytically:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{z} \\mid \\mathbf{x}) = \\mathcal{N}(\\mathbf{z} \\mid \\mathbf{M}^{-1} \\mathbf{W}^\\top (\\mathbf{x} - \\mathbf{b}), \\sigma^2 \\mathbf{M}^{-1}), \\tag{5.7}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "\\mathbf{M} = \\mathbf{W}^\\top \\mathbf{W} + \\sigma^2 \\mathbf{I}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. Once $ \\mathbf{W} $ is found by maximizing the log-likelihood, and its dimensionality is computationally tractable, we can calculate $ p(\\mathbf{z} \\mid \\mathbf{x}) $ for any observation $ \\mathbf{x} $. \n",
    "2. This is significant because it allows us to compute the distribution over latent factors for given data points.\n",
    "\n",
    "---\n",
    "\n",
    "### A Side Note\n",
    "\n",
    "Probabilistic PCA is an essential latent variable model for two reasons:\n",
    "1. **Analytical tractability**: All computations can be done by hand, making it an excellent exercise for building intuition about latent variable models.\n",
    "2. **Linearity**: As a linear model, it raises important questions:\n",
    "   - What happens with **non-linear dependencies**?\n",
    "   - What happens if we use distributions other than Gaussians?\n",
    "\n",
    "In both cases, the integral in $ p(\\mathbf{x}) $ becomes intractable, requiring approximations such as **variational inference**.\n",
    "\n",
    "Thus, studying pPCA in depth helps develop a solid understanding of probabilistic modeling concepts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc94de18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/1000], Log-Likelihood: -355.7412\n",
      "Epoch [100/1000], Log-Likelihood: -381.8489\n",
      "Epoch [200/1000], Log-Likelihood: -406.8539\n",
      "Epoch [300/1000], Log-Likelihood: -430.4676\n",
      "Epoch [400/1000], Log-Likelihood: -452.6530\n",
      "Epoch [500/1000], Log-Likelihood: -473.5280\n",
      "Epoch [600/1000], Log-Likelihood: -493.2508\n",
      "Epoch [700/1000], Log-Likelihood: -511.9764\n",
      "Epoch [800/1000], Log-Likelihood: -529.8442\n",
      "Epoch [900/1000], Log-Likelihood: -546.9756\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'linalg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7721/2549660499.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;31m# Posterior estimation for the first data point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m \u001b[0mmean_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcovariance_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposterior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Posterior mean of latent variables:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_z\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Posterior covariance of latent variables:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcovariance_z\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_7721/2549660499.py\u001b[0m in \u001b[0;36mposterior\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \"\"\"\n\u001b[1;32m     74\u001b[0m         \u001b[0mcovariance_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mmean_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcovariance_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0mcovariance_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'linalg'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class pPCA(nn.Module):\n",
    "    def __init__(self, D, M, sigma=1.0):\n",
    "        \"\"\"\n",
    "        Probabilistic PCA model\n",
    "        \n",
    "        Args:\n",
    "            D (int): Dimensionality of the data.\n",
    "            M (int): Dimensionality of the latent variable.\n",
    "            sigma (float): Standard deviation of the Gaussian noise.\n",
    "        \"\"\"\n",
    "        super(pPCA, self).__init__()\n",
    "        self.D = D  # Dimensionality of input\n",
    "        self.M = M  # Dimensionality of latent space\n",
    "        self.sigma = sigma  # Standard deviation of noise\n",
    "        \n",
    "        # Weight matrix W and bias term b (initialization)\n",
    "        self.W = nn.Parameter(torch.randn(D, M))  # D x M matrix\n",
    "        self.b = nn.Parameter(torch.randn(D))     # D-dimensional bias\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Forward pass for pPCA model: x = Wz + b + noise\n",
    "        \n",
    "        Args:\n",
    "            z (Tensor): Latent variable of shape (batch_size, M)\n",
    "        \n",
    "        Returns:\n",
    "            x (Tensor): Data point reconstructed from latent variable, of shape (batch_size, D)\n",
    "        \"\"\"\n",
    "        # Reconstruct the data point\n",
    "        return torch.matmul(z, self.W.t()) + self.b\n",
    "\n",
    "    def log_likelihood(self, x):\n",
    "        \"\"\"\n",
    "        Compute the log-likelihood of the data under pPCA model\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Data, shape (batch_size, D)\n",
    "\n",
    "        Returns:\n",
    "            log_likelihood (Tensor): Log-likelihood of the data\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        # Compute the covariance matrix\n",
    "        covariance_matrix = torch.matmul(self.W, self.W.t()) + self.sigma ** 2 * torch.eye(self.D)\n",
    "\n",
    "        # Compute the log-likelihood of the Gaussian with mean b and covariance covariance_matrix\n",
    "        diff = x - self.b\n",
    "        inverse_covariance = torch.inverse(covariance_matrix)  # Compute the inverse of covariance matrix\n",
    "        log_prob = -0.5 * (torch.sum(torch.matmul(diff, inverse_covariance) * diff, dim=1)\n",
    "                          + batch_size * torch.log(torch.det(covariance_matrix)) + self.D * np.log(2 * np.pi))\n",
    "\n",
    "        return torch.mean(log_prob)\n",
    "\n",
    "\n",
    "    def posterior(self, x):\n",
    "        \"\"\"\n",
    "        Compute the posterior distribution p(z | x)\n",
    "        \n",
    "        Args:\n",
    "            x (Tensor): Data, shape (batch_size, D)\n",
    "        \n",
    "        Returns:\n",
    "            mean_z (Tensor): Posterior mean for z, shape (batch_size, M)\n",
    "            covariance_z (Tensor): Posterior covariance for z, shape (batch_size, M, M)\n",
    "        \"\"\"\n",
    "        covariance_matrix = torch.matmul(self.W, self.W.t()) + self.sigma ** 2 * torch.eye(self.D)\n",
    "        mean_z = torch.matmul(torch.linalg.inv(covariance_matrix), x - self.b)\n",
    "        covariance_z = torch.linalg.inv(torch.matmul(self.W, self.W.t()) + self.sigma ** 2 * torch.eye(self.M))\n",
    "\n",
    "        return mean_z, covariance_z\n",
    "\n",
    "# Generate synthetic data\n",
    "D = 10  # Dimensionality of the data\n",
    "M = 3   # Latent dimensionality\n",
    "N = 100  # Number of samples\n",
    "\n",
    "# Generate latent variables z ~ N(0, I)\n",
    "z = np.random.randn(N, M)\n",
    "W_true = np.random.randn(D, M)\n",
    "b_true = np.random.randn(D)\n",
    "x_true = np.dot(z, W_true.T) + b_true + np.random.randn(N, D) * 0.1  # Adding noise\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "x_data = torch.tensor(x_true, dtype=torch.float32)\n",
    "\n",
    "# Initialize pPCA model\n",
    "model = pPCA(D, M)\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "log_likelihoods = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Sample from the latent variable z (random initial guess)\n",
    "    z_init = torch.randn(N, M)\n",
    "    \n",
    "    # Forward pass: Reconstruct data from z\n",
    "    x_reconstructed = model(z_init)\n",
    "\n",
    "    # Compute log-likelihood and backpropagate\n",
    "    log_likelihood = model.log_likelihood(x_data)\n",
    "    log_likelihoods.append(log_likelihood.item())\n",
    "    log_likelihood.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch [{epoch}/{num_epochs}], Log-Likelihood: {log_likelihood.item():.4f}')\n",
    "\n",
    "# Plot log-likelihood convergence\n",
    "plt.plot(log_likelihoods)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Log-Likelihood')\n",
    "plt.title('Log-Likelihood Convergence during Training')\n",
    "plt.show()\n",
    "\n",
    "# Posterior estimation for the first data point\n",
    "mean_z, covariance_z = model.posterior(x_data[0].unsqueeze(0))\n",
    "print(\"Posterior mean of latent variables:\", mean_z)\n",
    "print(\"Posterior covariance of latent variables:\", covariance_z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8335bc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "class pPCA:\n",
    "    def __init__(self, D, M, sigma=1.0):\n",
    "        \"\"\"\n",
    "        Initializes the Probabilistic PCA model.\n",
    "        \n",
    "        Args:\n",
    "            D (int): Dimensionality of the data\n",
    "            M (int): Number of latent variables\n",
    "            sigma (float): Standard deviation for the Gaussian noise\n",
    "        \"\"\"\n",
    "        self.D = D  # Dimensionality of the data\n",
    "        self.M = M  # Dimensionality of the latent variables\n",
    "        self.sigma = sigma  # Standard deviation of Gaussian noise\n",
    "\n",
    "        # Initialize parameters randomly\n",
    "        self.W = [[random.gauss(0, 1) for _ in range(M)] for _ in range(D)]  # Transformation matrix\n",
    "        self.b = [random.gauss(0, 1) for _ in range(D)]  # Bias term\n",
    "\n",
    "    def matmul(self, A, B):\n",
    "        \"\"\"Performs matrix multiplication (A @ B)\"\"\"\n",
    "        result = []\n",
    "        for i in range(len(A)):\n",
    "            result.append([sum(A[i][k] * B[k][j] for k in range(len(B))) for j in range(len(B[0]))])\n",
    "        return result\n",
    "\n",
    "    def transpose(self, matrix):\n",
    "        \"\"\"Transposes a matrix\"\"\"\n",
    "        return [list(i) for i in zip(*matrix)]\n",
    "\n",
    "    def generate_data(self, batch_size):\n",
    "        \"\"\"\n",
    "        Generate data samples using the model parameters.\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int): Number of data points to generate\n",
    "        \n",
    "        Returns:\n",
    "            List: Generated data points\n",
    "        \"\"\"\n",
    "        data_points = []\n",
    "        for _ in range(batch_size):\n",
    "            # Sample z from standard normal distribution\n",
    "            z = [random.gauss(0, 1) for _ in range(self.M)]\n",
    "            \n",
    "            # Reconstruct x from z\n",
    "            x_reconstructed = [sum(self.W[i][j] * z[j] for j in range(self.M)) + self.b[i] for i in range(self.D)]\n",
    "            \n",
    "            # Add Gaussian noise\n",
    "            noise = [random.gauss(0, self.sigma) for _ in range(self.D)]\n",
    "            x = [x_reconstructed[i] + noise[i] for i in range(self.D)]\n",
    "            \n",
    "            data_points.append(x)\n",
    "        \n",
    "        return data_points\n",
    "\n",
    "    def log_likelihood(self, x_data):\n",
    "        \"\"\"\n",
    "        Compute the log-likelihood of the data under the pPCA model.\n",
    "        \n",
    "        Args:\n",
    "            x_data (List): List of data points\n",
    "        \n",
    "        Returns:\n",
    "            float: Log-likelihood of the data\n",
    "        \"\"\"\n",
    "        log_likelihoods = []\n",
    "        for x in x_data:\n",
    "            # Compute covariance matrix: W * W.T + sigma^2 * I\n",
    "            W_T = self.transpose(self.W)\n",
    "            cov_matrix = self.matmul(self.W, W_T)\n",
    "            \n",
    "            # Add noise term (sigma^2 * I)\n",
    "            for i in range(self.D):\n",
    "                cov_matrix[i][i] += self.sigma ** 2\n",
    "            \n",
    "            # Compute the difference (x - b)\n",
    "            diff = [x[i] - self.b[i] for i in range(self.D)]\n",
    "            \n",
    "            # Compute the log-probability for the Gaussian distribution\n",
    "            det_cov = self.determinant(cov_matrix)\n",
    "            log_prob = -0.5 * (self.vector_dot(self.vector_dot(diff, cov_matrix), diff) + self.D * math.log(2 * math.pi) + math.log(det_cov))\n",
    "            log_likelihoods.append(log_prob)\n",
    "        \n",
    "        return sum(log_likelihoods) / len(log_likelihoods)\n",
    "\n",
    "    def vector_dot(self, A, B):\n",
    "        \"\"\"Computes dot product of two vectors A and B\"\"\"\n",
    "        return sum(A[i] * B[i] for i in range(len(A)))\n",
    "\n",
    "    def determinant(self, matrix):\n",
    "        \"\"\"Computes the determinant of a square matrix using recursion (Laplace expansion)\"\"\"\n",
    "        n = len(matrix)\n",
    "        if n == 1:\n",
    "            return matrix[0][0]\n",
    "        elif n == 2:\n",
    "            return matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n",
    "        \n",
    "        det = 0\n",
    "        for c in range(n):\n",
    "            # Minor matrix for element at matrix[0][c]\n",
    "            minor = [row[:c] + row[c+1:] for row in matrix[1:]]\n",
    "            det += ((-1) ** c) * matrix[0][c] * self.determinant(minor)\n",
    "        \n",
    "        return det\n",
    "\n",
    "# Initialize the model\n",
    "D = 20  # Data dimensionality\n",
    "M = 5   # Latent space dimensionality\n",
    "sigma = 1.0  # Gaussian noise standard deviation\n",
    "model = pPCA(D, M, sigma)\n",
    "\n",
    "# Example usage with synthetic data\n",
    "batch_size = 10\n",
    "x_data = model.generate_data(batch_size)\n",
    "\n",
    "# Compute log-likelihood of the generated data\n",
    "log_likelihood = model.log_likelihood(x_data)\n",
    "print(f\"Log-Likelihood of the data: {log_likelihood}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888a8af8",
   "metadata": {},
   "source": [
    "###  Variational Auto-encoders: Variational Inference for Nonlinear Latent Variable Models\n",
    "\n",
    "####  The Model and the Objective\n",
    "\n",
    "Let us take a look at the integral one more time and think of a general case where we cannot calculate it analytically. The simplest approach would be to use the Monte Carlo approximation:\n",
    "\n",
    "$$\n",
    "\\int p(x) = \\int p(x|z) p(z) \\, dz\n",
    "$$\n",
    "\n",
    "This can be approximated as:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{z \\sim p(z)} [p(x|z)]\n",
    "$$\n",
    "\n",
    "or equivalently:\n",
    "\n",
    "$$\n",
    "\\frac{1}{K} \\sum_{k=1}^{K} p(x|z_k)\n",
    "$$\n",
    "\n",
    "where, in the last line, we use samples from the prior over latents, $ z_k \\sim p(z) $. Such an approach is relatively easy, and since our computational power grows rapidly, we can sample a lot of points in a reasonably short time. However, as we know from statistics, if $ z $ is multidimensional, and $ M $ is relatively large, we encounter the curse of dimensionality, and to cover the space properly, the number of samples grows exponentially with respect to $ M $. If we take too few samples, the approximation becomes poor.\n",
    "\n",
    "We can use more advanced Monte Carlo techniques, but they still suffer from issues associated with the curse of dimensionality. An alternative approach is an application of **variational inference**.\n",
    "\n",
    "Let us consider a family of variational distributions parameterized by $ \\phi $, $ \\{q_{\\phi}(z)\\}_{\\phi} $. For instance, we can consider Gaussians with means and variances, $ \\phi = \\{ \\mu, \\sigma^2 \\} $. We know the form of these distributions, and we assume that they assign nonzero probability mass to all $ z \\in \\mathbb{R}^M $.\n",
    "\n",
    "Then, the logarithm of the marginal distribution could be approximated as follows:\n",
    "\n",
    "$$\n",
    "\\ln p(x) = \\ln \\int p(x|z) p(z) \\, dz\n",
    "$$\n",
    "\n",
    "Using the variational distribution $ q_\\phi(z) $, we can rewrite it as:\n",
    "\n",
    "$$\n",
    "\\ln p(x) = \\int q_\\phi(z) \\frac{p(x|z) p(z)}{q_\\phi(z)} \\, dz\n",
    "$$\n",
    "\n",
    "By applying **Jensen's inequality**, we get the following lower bound:\n",
    "\n",
    "$$\n",
    "\\ln p(x) \\geq \\mathbb{E}_{z \\sim q_\\phi(z)} \\left[ \\ln \\frac{p(x|z) p(z)}{q_\\phi(z)} \\right]\n",
    "$$\n",
    "\n",
    "This can be expanded as:\n",
    "\n",
    "$$\n",
    "\\ln p(x) \\geq \\mathbb{E}_{z \\sim q_\\phi(z)} \\left[ \\ln p(x|z) \\right] - \\mathbb{E}_{z \\sim q_\\phi(z)} \\left[ \\ln q_\\phi(z) \\right] - \\ln p(z)\n",
    "$$\n",
    "\n",
    "This is the basis of variational inference, where the objective is to maximize the **ELBO (Evidence Lower Bound)**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\phi) = \\mathbb{E}_{z \\sim q_\\phi(z)} \\left[ \\ln p(x|z) \\right] - \\mathbb{E}_{z \\sim q_\\phi(z)} \\left[ \\ln q_\\phi(z) \\right] - \\ln p(z)\n",
    "$$\n",
    "\n",
    "This approach avoids the curse of dimensionality by approximating the intractable integral with a simpler variational distribution $ q_\\phi(z) $.\n",
    "\n",
    "###  Variational Auto-encoders: Variational Inference for Nonlinear Latent Variable Models\n",
    "\n",
    "####  The Amortized Variational Posterior\n",
    "\n",
    "If we consider an amortized variational posterior, namely, $ q_\\phi(z|x) $ instead of $ q_\\phi(z) $ for each $ x $, then we get:\n",
    "\n",
    "$$\n",
    "\\ln p(x) \\geq \\mathbb{E}_{z \\sim q_\\phi(z|x)} \\left[ \\ln p(x|z) \\right] - \\mathbb{E}_{z \\sim q_\\phi(z|x)} \\left[ \\ln q_\\phi(z|x) \\right] - \\ln p(z)\n",
    "$$\n",
    "\n",
    "Amortization could be extremely useful because we train a single model (e.g., a neural network with some weights), and it returns the parameters of the distribution for a given input. From now on, we will assume that we use amortized variational posteriors. However, please remember that we do not need to do that! Please refer to [5], where semi-amortized variational inference is considered.\n",
    "\n",
    "As a result, we obtain an auto-encoder-like model, with a **stochastic encoder**, $ q_\\phi(z|x) $, and a **stochastic decoder**, $ p(x|z) $. We use \"stochastic\" to highlight that the encoder and the decoder are probability distributions, in contrast to a deterministic auto-encoder. This model, with the amortized variational posterior, is called a **Variational Auto-Encoder (VAE)**.\n",
    "\n",
    "The lower bound of the log-likelihood function is called the **evidence lower bound (ELBO)**. The first part of the ELBO, $ \\mathbb{E}_{z \\sim q_\\phi(z|x)} [\\ln p(x|z)] $, is referred to as the **(negative) reconstruction error**, because $ x $ is encoded to $ z $ and then decoded back. The second part of the ELBO, $ \\mathbb{E}_{z \\sim q_\\phi(z|x)} \\left[ \\ln q_\\phi(z|x) - \\ln p(z) \\right] $, could be seen as a **regularizer**, and it coincides with the **Kullback-Leibler divergence (KL)**. However, for more complex models (e.g., hierarchical models), the regularizer(s) may not be interpreted as the KL term, so we prefer to use the term **regularizer**, as it is more general.\n",
    "\n",
    "#### A Different Perspective on the ELBO\n",
    "\n",
    "For completeness, we provide a different derivation of the ELBO that will help us understand why the lower bound might be tricky sometimes:\n",
    "\n",
    "$$\n",
    "\\ln p(x) = \\mathbb{E}_{z \\sim q_\\phi(z|x)} \\left[ \\ln p(x) \\right]\n",
    "$$\n",
    "\n",
    "We can expand this expression as:\n",
    "\n",
    "$$\n",
    "p(z|x)p(x) = \\mathbb{E}_{z \\sim q_\\phi(z|x)} \\ln \\left( \\frac{p(z|x)p(x)}{q_\\phi(z|x)} \\right)\n",
    "$$\n",
    "\n",
    "Simplifying further:\n",
    "\n",
    "$$\n",
    "= \\mathbb{E}_{z \\sim q_\\phi(z|x)} \\ln \\left( \\frac{p(x|z)p(z)}{q_\\phi(z|x)} \\right)\n",
    "$$\n",
    "\n",
    "Breaking it down into terms:\n",
    "\n",
    "$$\n",
    "= \\mathbb{E}_{z \\sim q_\\phi(z|x)} \\left[ \\ln p(x|z) \\right] - \\mathbb{E}_{z \\sim q_\\phi(z|x)} \\left[ \\ln q_\\phi(z|x) \\right] - \\ln p(z)\n",
    "$$\n",
    "\n",
    "This leads us to the final expression for the ELBO:\n",
    "\n",
    "$$\n",
    "\\ln p(x) \\geq \\mathbb{E}_{z \\sim q_\\phi(z|x)} \\left[ \\ln p(x|z) \\right] - \\mathbb{E}_{z \\sim q_\\phi(z|x)} \\left[ \\ln q_\\phi(z|x) \\right] - \\ln p(z)\n",
    "$$\n",
    "\n",
    "This formulation highlights the **Kullback-Leibler divergence** between the variational posterior $ q_\\phi(z|x) $ and the true posterior $ p(z|x) $, which measures the gap between the variational distribution and the true distribution. This KL divergence term is always greater than or equal to zero.\n",
    "\n",
    "Thus, the ELBO can be thought of as a lower bound on the true log-likelihood, with the KL term representing the gap between the ELBO and the true log-likelihood.\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "If the variational posterior $ q_\\phi(z|x) $ is a poor approximation of the true posterior $ p(z|x) $, then the KL term will be large, and even if the ELBO is optimized well, the gap between the ELBO and the true log-likelihood could be huge. This means that if we use too simplistic a posterior, we could end up with a bad VAE, even if the ELBO is optimized effectively.\n",
    "\n",
    "The problem arises when the ELBO is a loose lower bound of the log-likelihood, which means the optimal solution of the ELBO could be completely different from the solution of the log-likelihood.\n",
    "\n",
    "#### Final Considerations\n",
    "\n",
    "This insight emphasizes the importance of choosing a good approximation for the variational posterior $ q_\\phi(z|x) $. If we take a too simplistic posterior, the VAE might not perform well, and the ELBO might not adequately reflect the true log-likelihood.\n",
    "\n",
    "This is why carefully designing the variational posterior, such as using more complex models (e.g., hierarchical models), can improve the performance of the VAE and reduce the gap between the ELBO and the true log-likelihood.\n",
    "\n",
    "###  Variational Auto-encoders: Variational Inference for Nonlinear Latent Variable Models\n",
    "\n",
    "####  The Amortized Variational Posterior\n",
    "\n",
    "If we consider an amortized variational posterior, namely, $ q_\\phi(z|x) $ instead of $ q_\\phi(z) $ for each $ x $, then we get:\n",
    "\n",
    "$$\n",
    "\\ln p(x) \\geq \\mathbb{E}_{z \\sim q_\\phi(z|x)} \\left[ \\ln p(x|z) \\right] - \\mathbb{E}_{z \\sim q_\\phi(z|x)} \\left[ \\ln q_\\phi(z|x) \\right] - \\ln p(z)\n",
    "$$\n",
    "\n",
    "Amortization could be extremely useful because we train a single model (e.g., a neural network with some weights), and it returns the parameters of the distribution for a given input. From now on, we will assume that we use amortized variational posteriors. However, please remember that we do not need to do that! Please refer to [5], where semi-amortized variational inference is considered.\n",
    "\n",
    "As a result, we obtain an auto-encoder-like model, with a **stochastic encoder**, $ q_\\phi(z|x) $, and a **stochastic decoder**, $ p(x|z) $. We use \"stochastic\" to highlight that the encoder and the decoder are probability distributions, in contrast to a deterministic auto-encoder. This model, with the amortized variational posterior, is called a **Variational Auto-Encoder (VAE)**.\n",
    "\n",
    "The lower bound of the log-likelihood function is called the **evidence lower bound (ELBO)**. The first part of the ELBO, $ \\mathbb{E}_{z \\sim q_\\phi(z|x)} [\\ln p(x|z)] $, is referred to as the **(negative) reconstruction error**, because $ x $ is encoded to $ z $ and then decoded back. The second part of the ELBO, $ \\mathbb{E}_{z \\sim q_\\phi(z|x)} \\left[ \\ln q_\\phi(z|x) - \\ln p(z) \\right] $, could be seen as a **regularizer**, and it coincides with the **Kullback-Leibler divergence (KL)**. However, for more complex models (e.g., hierarchical models), the regularizer(s) may not be interpreted as the KL term, so we prefer to use the term **regularizer**, as it is more general.\n",
    "\n",
    "####  A Different Perspective on the ELBO\n",
    "\n",
    "For completeness, we provide a different derivation of the ELBO that will help us understand why the lower bound might be tricky sometimes:\n",
    "\n",
    "$$\n",
    "\\ln p(x) = \\mathbb{E}_{z \\sim q_\\phi(z|x)} \\left[ \\ln p(x) \\right]\n",
    "$$\n",
    "\n",
    "We can expand this expression as:\n",
    "\n",
    "$$\n",
    "p(z|x)p(x) = \\mathbb{E}_{z \\sim q_\\phi(z|x)} \\ln \\left( \\frac{p(z|x)p(x)}{q_\\phi(z|x)} \\right)\n",
    "$$\n",
    "\n",
    "Simplifying further:\n",
    "\n",
    "$$\n",
    "= \\mathbb{E}_{z \\sim q_\\phi(z|x)} \\ln \\left( \\frac{p(x|z)p(z)}{q_\\phi(z|x)} \\right)\n",
    "$$\n",
    "\n",
    "Breaking it down into terms:\n",
    "\n",
    "$$\n",
    "= \\mathbb{E}_{z \\sim q_\\phi(z|x)} \\left[ \\ln p(x|z) \\right] - \\mathbb{E}_{z \\sim q_\\phi(z|x)} \\left[ \\ln q_\\phi(z|x) \\right] - \\ln p(z)\n",
    "$$\n",
    "\n",
    "This leads us to the final expression for the ELBO:\n",
    "\n",
    "$$\n",
    "\\ln p(x) \\geq \\mathbb{E}_{z \\sim q_\\phi(z|x)} \\left[ \\ln p(x|z) \\right] - \\mathbb{E}_{z \\sim q_\\phi(z|x)} \\left[ \\ln q_\\phi(z|x) \\right] - \\ln p(z)\n",
    "$$\n",
    "\n",
    "This formulation highlights the **Kullback-Leibler divergence** between the variational posterior $ q_\\phi(z|x) $ and the true posterior $ p(z|x) $, which measures the gap between the variational distribution and the true distribution. This KL divergence term is always greater than or equal to zero.\n",
    "\n",
    "Thus, the ELBO can be thought of as a lower bound on the true log-likelihood, with the KL term representing the gap between the ELBO and the true log-likelihood.\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "If the variational posterior $ q_\\phi(z|x) $ is a poor approximation of the true posterior $ p(z|x) $, then the KL term will be large, and even if the ELBO is optimized well, the gap between the ELBO and the true log-likelihood could be huge. This means that if we use too simplistic a posterior, we could end up with a bad VAE, even if the ELBO is optimized effectively.\n",
    "\n",
    "The problem arises when the ELBO is a loose lower bound of the log-likelihood, which means the optimal solution of the ELBO could be completely different from the solution of the log-likelihood.\n",
    "\n",
    "#### Final Considerations\n",
    "\n",
    "This insight emphasizes the importance of choosing a good approximation for the variational posterior $ q_\\phi(z|x) $. If we take a too simplistic posterior, the VAE might not perform well, and the ELBO might not adequately reflect the true log-likelihood.\n",
    "\n",
    "This is why carefully designing the variational posterior, such as using more complex models (e.g., hierarchical models), can improve the performance of the VAE and reduce the gap between the ELBO and the true log-likelihood.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff13a26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the encoder network (Q_phi)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2_mu = nn.Linear(hidden_dim, latent_dim)  # mean of z\n",
    "        self.fc2_logvar = nn.Linear(hidden_dim, latent_dim)  # log variance of z\n",
    "\n",
    "    def forward(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        mu = self.fc2_mu(h1)\n",
    "        logvar = self.fc2_logvar(h1)\n",
    "        return mu, logvar  # Return mean and log variance for the posterior distribution\n",
    "\n",
    "# Define the decoder network (P_theta)\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        h1 = F.relu(self.fc1(z))\n",
    "        x_recon = torch.sigmoid(self.fc2(h1))  # For binary data, use sigmoid\n",
    "        return x_recon\n",
    "\n",
    "# Define the VAE model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(input_dim, hidden_dim, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim, input_dim)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        # Reparameterization trick\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, mu, logvar\n",
    "\n",
    "    def loss_function(self, x, x_recon, mu, logvar):\n",
    "        # Compute the reconstruction loss (negative log-likelihood)\n",
    "        BCE = F.binary_cross_entropy(x_recon, x, reduction='sum')\n",
    "\n",
    "        # Compute the KL divergence\n",
    "        # Kullback-Leibler divergence between q(z|x) and p(z)\n",
    "        # q(z|x) = N(mu, exp(logvar)), p(z) = N(0, I)\n",
    "        # KL(q(z|x) || p(z)) = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "        # where mu and logvar are the mean and log variance from the encoder output\n",
    "        # and the sigma^2 is exp(logvar).\n",
    "        # You can think of this as the regularizer term.\n",
    "        KL_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "        # The ELBO (evidence lower bound) is the negative of this loss:\n",
    "        # ELBO = Reconstruction Loss + KL Divergence\n",
    "        return BCE + KL_div\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = 784  # For MNIST dataset (28x28 images)\n",
    "hidden_dim = 400\n",
    "latent_dim = 20\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Example input data\n",
    "# Let's assume x_data is a batch of images, shape: [batch_size, input_dim]\n",
    "x_data = torch.randn(batch_size, input_dim)  # Replace with real data\n",
    "\n",
    "# Initialize the model, optimizer, and loss function\n",
    "model = VAE(input_dim, hidden_dim, latent_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    x_recon, mu, logvar = model(x_data)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = model.loss_function(x_data, x_recon, mu, logvar)\n",
    "\n",
    "    # Backpropagation and optimization step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# After training, we can use the VAE to generate new samples or calculate the ELBO on new data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9fc223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the encoder network (Q_phi)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2_mu = nn.Linear(hidden_dim, latent_dim)  # mean of z\n",
    "        self.fc2_logvar = nn.Linear(hidden_dim, latent_dim)  # log variance of z\n",
    "\n",
    "    def forward(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        mu = self.fc2_mu(h1)\n",
    "        logvar = self.fc2_logvar(h1)\n",
    "        return mu, logvar  # Return mean and log variance for the posterior distribution\n",
    "\n",
    "# Define the decoder network (P_theta)\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        h1 = F.relu(self.fc1(z))\n",
    "        x_recon = torch.sigmoid(self.fc2(h1))  # For binary data, use sigmoid\n",
    "        return x_recon\n",
    "\n",
    "# Define the VAE model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(input_dim, hidden_dim, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim, input_dim)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        # Reparameterization trick\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, mu, logvar\n",
    "\n",
    "    def loss_function(self, x, x_recon, mu, logvar):\n",
    "        # Compute the reconstruction loss (negative log-likelihood)\n",
    "        BCE = F.binary_cross_entropy(x_recon, x, reduction='sum')\n",
    "\n",
    "        # Compute the KL divergence\n",
    "        # Kullback-Leibler divergence between q(z|x) and p(z)\n",
    "        # q(z|x) = N(mu, exp(logvar)), p(z) = N(0, I)\n",
    "        # KL(q(z|x) || p(z)) = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "        # where mu and logvar are the mean and log variance from the encoder output\n",
    "        # and the sigma^2 is exp(logvar).\n",
    "        # You can think of this as the regularizer term.\n",
    "        KL_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "        # The ELBO (evidence lower bound) is the negative of this loss:\n",
    "        # ELBO = Reconstruction Loss + KL Divergence\n",
    "        return BCE + KL_div\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = 784  # For MNIST dataset (28x28 images)\n",
    "hidden_dim = 400\n",
    "latent_dim = 20\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Example input data\n",
    "# Let's assume x_data is a batch of images, shape: [batch_size, input_dim]\n",
    "x_data = torch.randn(batch_size, input_dim)  # Replace with real data\n",
    "\n",
    "# Initialize the model, optimizer, and loss function\n",
    "model = VAE(input_dim, hidden_dim, latent_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    x_recon, mu, logvar = model(x_data)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = model.loss_function(x_data, x_recon, mu, logvar)\n",
    "\n",
    "    # Backpropagation and optimization step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# After training, we can use the VAE to generate new samples or calculate the ELBO on new data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd89dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "# Define a basic random normal distribution generator (for simplicity)\n",
    "def normal_sample(mu, logvar):\n",
    "    # Reparameterization trick: z = mu + eps * sigma\n",
    "    std = math.exp(0.5 * logvar)\n",
    "    eps = random.gauss(0, 1)  # Gaussian noise\n",
    "    return mu + eps * std\n",
    "\n",
    "# Encoder (Q_phi)\n",
    "def encoder(x, W1, b1, W2, b2):\n",
    "    # Linear transformation (layer 1)\n",
    "    h1 = [sum(xi * wi for xi, wi in zip(x, W1[j])) + b1[j] for j in range(len(W1))]\n",
    "    # ReLU activation\n",
    "    h1 = [max(0, h) for h in h1]\n",
    "    \n",
    "    # Output mean (mu) and log variance (logvar) from layer 2\n",
    "    mu = [sum(h1i * wi for h1i, wi in zip(h1, W2[j])) + b2[j] for j in range(len(W2))]\n",
    "    logvar = [random.uniform(-1, 1) for _ in range(len(mu))]  # Initialize logvar randomly\n",
    "    return mu, logvar\n",
    "\n",
    "# Decoder (P_theta)\n",
    "def decoder(z, W1, b1, W2, b2):\n",
    "    # Linear transformation (layer 1)\n",
    "    h1 = [sum(zi * wi for zi, wi in zip(z, W1[j])) + b1[j] for j in range(len(W1))]\n",
    "    # ReLU activation\n",
    "    h1 = [max(0, h) for h in h1]\n",
    "    \n",
    "    # Output (reconstructed x)\n",
    "    x_recon = [sum(h1i * wi for h1i, wi in zip(h1, W2[j])) + b2[j] for j in range(len(W2))]\n",
    "    return x_recon\n",
    "\n",
    "# Reconstruction loss (binary cross-entropy)\n",
    "def reconstruction_loss(x, x_recon):\n",
    "    return -sum(xi * math.log(x_recon[i] + 1e-10) + (1 - xi) * math.log(1 - x_recon[i] + 1e-10) for i, xi in enumerate(x))\n",
    "\n",
    "# KL Divergence\n",
    "def kl_divergence(mu, logvar):\n",
    "    return -0.5 * sum(1 + logvar[i] - mu[i]**2 - math.exp(logvar[i]) for i in range(len(mu)))\n",
    "\n",
    "# Variational Autoencoder (VAE)\n",
    "def vae(x, W1_enc, b1_enc, W2_enc, b2_enc, W1_dec, b1_dec, W2_dec, b2_dec):\n",
    "    # Encoder: Get the mean (mu) and log variance (logvar) for the latent variable z\n",
    "    mu, logvar = encoder(x, W1_enc, b1_enc, W2_enc, b2_enc)\n",
    "    \n",
    "    # Sample z from the approximate posterior q(z|x)\n",
    "    z = [normal_sample(mu[i], logvar[i]) for i in range(len(mu))]\n",
    "    \n",
    "    # Decoder: Reconstruct x from z\n",
    "    x_recon = decoder(z, W1_dec, b1_dec, W2_dec, b2_dec)\n",
    "    \n",
    "    # Compute the total loss (ELBO)\n",
    "    recon_loss = reconstruction_loss(x, x_recon)\n",
    "    kl_loss = kl_divergence(mu, logvar)\n",
    "    total_loss = recon_loss + kl_loss\n",
    "    return total_loss, recon_loss, kl_loss\n",
    "\n",
    "# Example data (a simple vector x)\n",
    "x_data = [random.randint(0, 1) for _ in range(784)]  # Random binary data (e.g., a flattened image)\n",
    "\n",
    "# Initialize parameters (weights and biases)\n",
    "input_dim = 784  # For MNIST images (28x28)\n",
    "hidden_dim = 400\n",
    "latent_dim = 20\n",
    "\n",
    "W1_enc = [[random.random() for _ in range(input_dim)] for _ in range(hidden_dim)]\n",
    "b1_enc = [random.random() for _ in range(hidden_dim)]\n",
    "W2_enc = [[random.random() for _ in range(hidden_dim)] for _ in range(latent_dim)]\n",
    "b2_enc = [random.random() for _ in range(latent_dim)]\n",
    "\n",
    "W1_dec = [[random.random() for _ in range(latent_dim)] for _ in range(hidden_dim)]\n",
    "b1_dec = [random.random() for _ in range(hidden_dim)]\n",
    "W2_dec = [[random.random() for _ in range(hidden_dim)] for _ in range(input_dim)]\n",
    "b2_dec = [random.random() for _ in range(input_dim)]\n",
    "\n",
    "# Forward pass through VAE and compute loss\n",
    "total_loss, recon_loss, kl_loss = vae(x_data, W1_enc, b1_enc, W2_enc, b2_enc, W1_dec, b1_dec, W2_dec, b2_dec)\n",
    "\n",
    "print(f\"Total Loss: {total_loss}\")\n",
    "print(f\"Reconstruction Loss: {recon_loss}\")\n",
    "print(f\"KL Divergence: {kl_loss}\")\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "ee7e6755",
   "metadata": {},
   "source": [
    "### Components of VAEs\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Fig.2 The ELBO is a lower bound on the log-likelihood. As a result, .θ̂ maximizing the ELBO does not necessarily coincides with .θ ∗ that maximizes .ln p(x). The looser the ELBO is, the more this can bias maximum likelihood estimates of the model parameters.\n",
    "\n",
    "Let us wrap up what we know right now. First of all, we consider a class of amortized variational posteriors $ \\{ q_{\\phi}(z|x) \\}_{\\phi} $ that approximate the true posterior $ p(z|x) $. We can see them as stochastic encoders. Second, the conditional likelihood $ p(x|z) $ could be seen as a stochastic decoder. Third, the last component, $ p(z) $, is the marginal distribution, also referred to as a prior. Lastly, the objective is the ELBO, a lower bound to the log-likelihood function:\n",
    "\n",
    "$$\n",
    "\\ln p(x) \\geq \\mathbb{E}_{z \\sim q_{\\phi}(z | x)} \\left[ \\ln p(x|z) \\right] - \\mathbb{E}_{z \\sim q_{\\phi}(z | x)} \\left[ \\ln q_{\\phi}(z | x) \\right] - \\ln p(z)\n",
    "$$\n",
    "\n",
    "There are two questions left to get the full picture of the VAEs:\n",
    "\n",
    "1. How to parameterize the distributions?\n",
    "2. How to calculate the expected values?\n",
    "\n",
    "After all, these integrals have not disappeared!\n",
    "\n",
    "### Parameterization of Distributions\n",
    "\n",
    "As you can probably guess by now, we use neural networks to parameterize the encoders and the decoders. But before we use the neural networks, we should know what distributions we use! Fortunately, in the VAE framework, we are almost free to choose any distribution! However, we must remember that they should make sense for a considered problem.\n",
    "\n",
    "So far, we have explained everything through images, so let us continue that. If $ x \\in \\{ 0, 1, \\dots, 255 \\}^D $, then we cannot use a normal distribution, because its support is totally different than the support of discrete-valued images. A possible distribution we can use is the categorical distribution, that is:\n",
    "\n",
    "$$\n",
    "p_{\\theta}(x|z) = \\text{Categorical}(x | \\theta(z)),\n",
    "$$\n",
    "\n",
    "where the probabilities are given by a neural network $ \\text{NN} $, namely, $ \\theta(z) = \\text{softmax}(\\text{NN}(z)) $. The neural network $ \\text{NN} $ could be an MLP, a convolutional neural network, RNNs, etc.\n",
    "\n",
    "The choice of a distribution for the latent variables depends on how we want to express the latent factors in data. For convenience, typically $ z $ is taken as a vector of continuous random variables, $ z \\in \\mathbb{R}^M $. Then, we can use Gaussians for both the variational posterior and the prior:\n",
    "\n",
    "$$\n",
    "q_{\\phi}(z|x) = \\mathcal{N}\\left(z \\,|\\, \\mu_{\\phi}(x), \\text{diag}(\\sigma_{\\phi}^2(x))\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(z) = \\mathcal{N}(z | 0, I),\n",
    "$$\n",
    "\n",
    "where $ \\mu_{\\phi}(x) $ and $ \\sigma_{\\phi}^2(x) $ are outputs of a neural network, similarly to the case of the decoder. In practice, we can have a shared neural network $ \\text{NN}(x) $ that outputs $ 2M $ values that are further split into $ M $ values for the mean $ \\mu $ and $ M $ values for the variance $ \\sigma^2 $. For convenience, we consider a diagonal covariance matrix. We could use flexible posteriors (see Section 5.4.2). Moreover, here we take the standard Gaussian prior. We will comment on that later (see Section 5.4.1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db605296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set up the parameters\n",
    "latent_dim = 2  # Dimensionality of the latent space\n",
    "input_dim = 784  # Assuming input data is 28x28 (MNIST)\n",
    "batch_size = 128\n",
    "epochs = 50\n",
    "\n",
    "# Build the encoder network\n",
    "def build_encoder():\n",
    "    inputs = layers.Input(shape=(input_dim,))\n",
    "    x = layers.Dense(512, activation='relu')(inputs)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    \n",
    "    # The mean and log variance\n",
    "    z_mean = layers.Dense(latent_dim, name='z_mean')(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "    \n",
    "    encoder = Model(inputs, [z_mean, z_log_var], name=\"encoder\")\n",
    "    return encoder\n",
    "\n",
    "# Sampling function to sample from the latent space using the reparameterization trick\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = tf.shape(z_mean)[0]\n",
    "    dim = tf.shape(z_mean)[1]\n",
    "    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "    z = z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    return z\n",
    "\n",
    "# Build the decoder network\n",
    "def build_decoder():\n",
    "    latent_inputs = layers.Input(shape=(latent_dim,))\n",
    "    x = layers.Dense(256, activation='relu')(latent_inputs)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    outputs = layers.Dense(input_dim, activation='sigmoid')(x)\n",
    "    \n",
    "    decoder = Model(latent_inputs, outputs, name=\"decoder\")\n",
    "    return decoder\n",
    "\n",
    "# VAE model class\n",
    "class VAE(Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = self.encoder(inputs)\n",
    "        z = sampling([z_mean, z_log_var])\n",
    "        reconstructed = self.decoder(z)\n",
    "        kl_loss = -0.5 * tf.reduce_mean(\n",
    "            z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1\n",
    "        )\n",
    "        self.add_loss(kl_loss)\n",
    "        return reconstructed\n",
    "\n",
    "# Load MNIST data\n",
    "(x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "x_train = x_train.reshape(-1, input_dim)\n",
    "x_test = x_test.reshape(-1, input_dim)\n",
    "\n",
    "# Build the VAE\n",
    "encoder = build_encoder()\n",
    "decoder = build_decoder()\n",
    "vae = VAE(encoder, decoder)\n",
    "\n",
    "# Compile the VAE\n",
    "vae.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "# Train the VAE\n",
    "vae.fit(x_train, x_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, x_test))\n",
    "\n",
    "# Generate new data (sample from latent space)\n",
    "def generate_data(num_samples):\n",
    "    z = np.random.normal(size=(num_samples, latent_dim))\n",
    "    generated_data = vae.decoder(z)\n",
    "    return generated_data\n",
    "\n",
    "# Visualize the results\n",
    "def visualize_latent_space():\n",
    "    z_mean, z_log_var = encoder.predict(x_test, batch_size=batch_size)\n",
    "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c='blue')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "# Generate and plot new images\n",
    "def plot_generated_images():\n",
    "    generated_data = generate_data(16)\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    for i in range(16):\n",
    "        plt.subplot(4, 4, i + 1)\n",
    "        plt.imshow(generated_data[i].reshape(28, 28), cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the latent space\n",
    "visualize_latent_space()\n",
    "\n",
    "# Plot some generated images\n",
    "plot_generated_images()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41004f96",
   "metadata": {},
   "source": [
    "## Reparameterization Trick\n",
    "\n",
    "So far, we played around with the log-likelihood, and we ended up with the ELBO. However, there is still a problem with calculating the expected value, because it contains an integral! Therefore, the question is how we can calculate it and why it is better than the MC approximation of the log-likelihood without the variational posterior. In fact, we will use the MC approximation, but now, instead of sampling from the prior $ p(z) $, we will sample from the variational posterior $ q_\\phi(z|x) $.\n",
    "\n",
    "Is it better? Yes, because the variational posterior typically assigns more probability mass to a smaller region than the prior. If you examine the variance of the variational posterior, you will probably notice that the variational posteriors are almost deterministic (whether it is good or bad is an open question). As a result, we should get a better approximation!\n",
    "\n",
    "However, there is still an issue with the variance of the approximation. If we sample $ z $ from $ q_\\phi(z|x) $, plug them into the ELBO, and calculate gradients with respect to the parameters of a neural network $ \\phi $, the variance of the gradient may still be pretty large! A possible solution to that, first noticed by statisticians (e.g., see [8]), is the approach of reparameterizing the distribution.\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Fig.3 An example of reparameterizing a Gaussian distribution: We scale .ϵ distributed according to the standard Gaussian by .σ, and shift it by .μ.\n",
    "\n",
    "### Reparameterization Trick\n",
    "\n",
    "The idea is to express a random variable as a composition of primitive transformations (e.g., arithmetic operations, logarithm, etc.) of an independent random variable with a simple distribution. For example, if we consider a Gaussian random variable $ z $ with a mean $ \\mu $ and variance $ \\sigma^2 $, and an independent random variable $ \\epsilon \\sim N(\\epsilon | 0, 1) $, then the following holds:\n",
    "\n",
    "$$\n",
    "z = \\mu + \\sigma \\cdot \\epsilon\n",
    "$$\n",
    "\n",
    "Now, if we sample $ \\epsilon $ from the standard Gaussian, and apply the above transformation, then we get a sample from $ N(z | \\mu, \\sigma) $. This idea can be generalized to other distributions as well.\n",
    "\n",
    "The reparameterization trick can be used in the encoder $ q_\\phi(z|x) $. By using this trick, we can drastically reduce the variance of the gradient because the randomness comes from the independent source $ p(\\epsilon) $, and we calculate the gradient with respect to a deterministic function (i.e., a neural network), not random variables.\n",
    "\n",
    "### Application in VAEs\n",
    "\n",
    "In the VAE framework, we will apply the reparameterization trick to the latent variable $ z $. The encoder $ q_\\phi(z|x) $ outputs two values: the mean $ \\mu_\\phi(x) $ and log-variance $ \\log \\sigma^2_\\phi(x) $ for each input $ x $. Instead of sampling directly from $ q_\\phi(z|x) $, we sample an independent Gaussian $ \\epsilon \\sim N(\\epsilon | 0, 1) $ and apply the following transformation:\n",
    "\n",
    "$$\n",
    "z_\\phi = \\mu_\\phi(x) + \\sigma_\\phi(x) \\cdot \\epsilon\n",
    "$$\n",
    "\n",
    "Here, $ \\mu_\\phi(x) $ and $ \\sigma_\\phi(x) $ are outputs of the encoder neural network. This ensures that we can backpropagate through the reparameterization, reducing the variance of the gradient estimates.\n",
    "\n",
    "### Training the VAE\n",
    "\n",
    "The training objective for the VAE is to minimize the **negative ELBO**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\phi, \\theta; x) = - \\mathbb{E}_{q_\\phi(z|x)} \\left[\\ln p_\\theta(x|z)\\right] + \\text{KL}\\left(q_\\phi(z|x) || p(z)\\right)\n",
    "$$\n",
    "\n",
    "In practice, we approximate the expectation $ \\mathbb{E}_{q_\\phi(z|x)} $ by taking a single sample from the variational posterior:\n",
    "\n",
    "$$\n",
    "z_\\phi = \\mu_\\phi(x) + \\sigma_\\phi(x) \\cdot \\epsilon\n",
    "$$\n",
    "\n",
    "where $ \\epsilon \\sim N(0, I) $. This allows the model to be trained using stochastic gradient descent, where we only need a single sample of $ z $ during each training iteration.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- The reparameterization trick is used to express a random variable $ z $ as a deterministic transformation of an independent random variable $ \\epsilon $, making the gradient computation more stable.\n",
    "- For VAEs, we apply the reparameterization trick to sample $ z $ from the variational posterior $ q_\\phi(z|x) $.\n",
    "- The training objective is the negative ELBO, and the reparameterization trick allows us to sample $ z $ efficiently and compute gradients for optimization. \n",
    "\n",
    "##  VAE Code Implementation\n",
    "\n",
    "### Encoder Class\n",
    "\n",
    "The encoder class takes in an input $ x $, passes it through the encoder network, and generates the mean $ \\mu_\\phi(x) $ and log-variance $ \\log \\sigma_\\phi^2(x) $ of the variational posterior $ q_\\phi(z|x) $. We then use the **reparameterization trick** to sample from this posterior.\n",
    "\n",
    "```python\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, encoder_net):\n",
    "        super(Encoder, self).__init__()\n",
    "        # The init of the encoder network.\n",
    "        self.encoder = encoder_net\n",
    "\n",
    "    # Reparameterization trick for Gaussians.\n",
    "    @staticmethod\n",
    "    def reparameterization(mu, log_var):\n",
    "        # The formula is the following:\n",
    "        # z = mu + std * epsilon\n",
    "        # epsilon ~ Normal(0, 1)\n",
    "        \n",
    "        # Get the standard deviation from the log-variance.\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        \n",
    "        # Sample epsilon from Normal(0, 1).\n",
    "        eps = torch.randn_like(std)\n",
    "        \n",
    "        # Final output\n",
    "        return mu + std * eps\n",
    "\n",
    "    # Output of the encoder network (mean and log-variance).\n",
    "    def encode(self, x):\n",
    "        # Calculate the output of the encoder network of size 2M.\n",
    "        h_e = self.encoder(x)\n",
    "        \n",
    "        # Split the output into the mean and log-variance.\n",
    "        mu_e, log_var_e = torch.chunk(h_e, 2, dim=1)\n",
    "        \n",
    "        return mu_e, log_var_e\n",
    "\n",
    "    # Sampling procedure using reparameterization.\n",
    "    def sample(self, x=None, mu_e=None, log_var_e=None):\n",
    "        if mu_e is None and log_var_e is None:\n",
    "            # Calculate mean and log-variance from the encoder.\n",
    "            mu_e, log_var_e = self.encode(x)\n",
    "        \n",
    "        # Apply the reparameterization trick.\n",
    "        return self.reparameterization(mu_e, log_var_e)\n",
    "\n",
    "    # Log-probability of the sample used for ELBO calculation.\n",
    "    def log_prob(self, x=None, mu_e=None, log_var_e=None, z=None):\n",
    "        if x is not None:\n",
    "            # Calculate corresponding sample if only x is provided.\n",
    "            mu_e, log_var_e = self.encode(x)\n",
    "            z = self.sample(mu_e=mu_e, log_var_e=log_var_e)\n",
    "        \n",
    "        # Return the log-normal distribution.\n",
    "        return log_normal_diag(z, mu_e, log_var_e)\n",
    "\n",
    "    # Forward pass: either log-probability or sampling.\n",
    "    def forward(self, x, type='log_prob'):\n",
    "        assert type in ['encode', 'log_prob'], 'Type could be either encode or log_prob'\n",
    "        if type == 'log_prob':\n",
    "            return self.log_prob(x)\n",
    "        else:\n",
    "            return self.sample(x)\n",
    "## 5.3.5 VAE Code Implementation\n",
    "\n",
    "### Encoder Class\n",
    "\n",
    "The encoder class takes in an input $ x $, passes it through the encoder network, and generates the mean $ \\mu_\\phi(x) $ and log-variance $ \\log \\sigma_\\phi^2(x) $ of the variational posterior $ q_\\phi(z|x) $. We then use the **reparameterization trick** to sample from this posterior.\n",
    "\n",
    "```python\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, encoder_net):\n",
    "        super(Encoder, self).__init__()\n",
    "        # The init of the encoder network.\n",
    "        self.encoder = encoder_net\n",
    "\n",
    "    # Reparameterization trick for Gaussians.\n",
    "    @staticmethod\n",
    "    def reparameterization(mu, log_var):\n",
    "        # The formula is the following:\n",
    "        # z = mu + std * epsilon\n",
    "        # epsilon ~ Normal(0, 1)\n",
    "        \n",
    "        # Get the standard deviation from the log-variance.\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        \n",
    "        # Sample epsilon from Normal(0, 1).\n",
    "        eps = torch.randn_like(std)\n",
    "        \n",
    "        # Final output\n",
    "        return mu + std * eps\n",
    "\n",
    "    # Output of the encoder network (mean and log-variance).\n",
    "    def encode(self, x):\n",
    "        # Calculate the output of the encoder network of size 2M.\n",
    "        h_e = self.encoder(x)\n",
    "        \n",
    "        # Split the output into the mean and log-variance.\n",
    "        mu_e, log_var_e = torch.chunk(h_e, 2, dim=1)\n",
    "        \n",
    "        return mu_e, log_var_e\n",
    "\n",
    "    # Sampling procedure using reparameterization.\n",
    "    def sample(self, x=None, mu_e=None, log_var_e=None):\n",
    "        if mu_e is None and log_var_e is None:\n",
    "            # Calculate mean and log-variance from the encoder.\n",
    "            mu_e, log_var_e = self.encode(x)\n",
    "        \n",
    "        # Apply the reparameterization trick.\n",
    "        return self.reparameterization(mu_e, log_var_e)\n",
    "\n",
    "    # Log-probability of the sample used for ELBO calculation.\n",
    "    def log_prob(self, x=None, mu_e=None, log_var_e=None, z=None):\n",
    "        if x is not None:\n",
    "            # Calculate corresponding sample if only x is provided.\n",
    "            mu_e, log_var_e = self.encode(x)\n",
    "            z = self.sample(mu_e=mu_e, log_var_e=log_var_e)\n",
    "        \n",
    "        # Return the log-normal distribution.\n",
    "        return log_normal_diag(z, mu_e, log_var_e)\n",
    "\n",
    "    # Forward pass: either log-probability or sampling.\n",
    "    def forward(self, x, type='log_prob'):\n",
    "        assert type in ['encode', 'log_prob'], 'Type could be either encode or log_prob'\n",
    "        if type == 'log_prob':\n",
    "            return self.log_prob(x)\n",
    "        else:\n",
    "            return self.sample(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, decoder_net, distribution='categorical', num_vals=None):\n",
    "        super(Decoder, self).__init__()\n",
    "        # The decoder network.\n",
    "        self.decoder = decoder_net\n",
    "        \n",
    "        # The distribution used for the decoder.\n",
    "        self.distribution = distribution\n",
    "        \n",
    "        # The number of possible values (used for categorical distribution).\n",
    "        self.num_vals = num_vals\n",
    "\n",
    "    # This function calculates parameters of the likelihood function p(x|z).\n",
    "    def decode(self, z):\n",
    "        # Apply the decoder network.\n",
    "        h_d = self.decoder(z)\n",
    "        \n",
    "        if self.distribution == 'categorical':\n",
    "            # Reshape to (Batch size, Dimensionality, Number of Values).\n",
    "            b = h_d.shape[0]\n",
    "            d = h_d.shape[1] // self.num_vals\n",
    "            h_d = h_d.view(b, d, self.num_vals)\n",
    "            \n",
    "            # Apply softmax to get probabilities.\n",
    "            mu_d = torch.softmax(h_d, 2)\n",
    "            return [mu_d]\n",
    "        \n",
    "        elif self.distribution == 'bernoulli':\n",
    "            # Apply sigmoid for Bernoulli distribution.\n",
    "            mu_d = torch.sigmoid(h_d)\n",
    "            return [mu_d]\n",
    "        \n",
    "        else:\n",
    "            raise ValueError('Only: \"categorical\", \"bernoulli\"')\n",
    "\n",
    "    # Sampling from the decoder.\n",
    "    def sample(self, z):\n",
    "        outs = self.decode(z)\n",
    "        \n",
    "        if self.distribution == 'categorical':\n",
    "            # Use the output of the decoder.\n",
    "            mu_d = outs[0]\n",
    "            return mu_d\n",
    "\n",
    "### Summary:\n",
    "\n",
    "- **Encoder**: Takes $ x $, computes the mean and log-variance, and applies the reparameterization trick to sample $ z $.\n",
    "- **Decoder**: Takes $ z $ and produces the parameters of the likelihood function, using a categorical or Bernoulli distribution.\n",
    "- **VAE Loss**: The ELBO is computed and used to train the model, where we sample from the variational posterior $ q_\\phi(z|x) $ using the reparameterization trick and maximize the likelihood of the observed data while regularizing the latent space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dca35c1",
   "metadata": {},
   "source": [
    "##  VAE Implementation\n",
    "\n",
    "### Prior Class\n",
    "\n",
    "In the current implementation, the prior is a simple standard Gaussian distribution. Although we could use a built-in PyTorch distribution, we chose not to for two reasons:\n",
    "1. It is important to think of the prior as a crucial component in VAEs.\n",
    "2. We can implement a learnable prior (e.g., a flow-based prior, VampPrior, a mixture of distributions).\n",
    "\n",
    "```python\n",
    "class Prior(nn.Module):\n",
    "    def __init__(self, L):\n",
    "        super(Prior, self).__init__()\n",
    "        self.L = L\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        # Sample from a standard Gaussian distribution.\n",
    "        z = torch.randn((batch_size, self.L))\n",
    "        return z\n",
    "\n",
    "    def log_prob(self, z):\n",
    "        # Log-probability of z from a standard normal distribution.\n",
    "        return log_standard_normal(z)\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, encoder_net, decoder_net, num_vals=256, L=16, likelihood_type='categorical'):\n",
    "        super(VAE, self).__init__()\n",
    "        print('VAE by JT.')\n",
    "\n",
    "        self.encoder = Encoder(encoder_net=encoder_net)\n",
    "        self.decoder = Decoder(distribution=likelihood_type, decoder_net=decoder_net, num_vals=num_vals)\n",
    "        self.prior = Prior(L=L)\n",
    "\n",
    "        self.num_vals = num_vals\n",
    "        self.likelihood_type = likelihood_type\n",
    "\n",
    "    def forward(self, x, reduction='avg'):\n",
    "        # Encoder\n",
    "        mu_e, log_var_e = self.encoder.encode(x)\n",
    "        z = self.encoder.sample(mu_e=mu_e, log_var_e=log_var_e)\n",
    "\n",
    "        # ELBO Calculation\n",
    "        RE = self.decoder.log_prob(x, z)\n",
    "        KL = (self.prior.log_prob(z) - self.encoder.log_prob(mu_e=mu_e, log_var_e=log_var_e, z=z)).sum(-1)\n",
    "\n",
    "        if reduction == 'sum':\n",
    "            return -(RE + KL).sum()\n",
    "        else:\n",
    "            return -(RE + KL).mean()\n",
    "\n",
    "    def sample(self, batch_size=64):\n",
    "        z = self.prior.sample(batch_size=batch_size)\n",
    "        return self.decoder.sample(z)\n",
    "# Example of Encoder Network\n",
    "encoder = nn.Sequential(\n",
    "    nn.Linear(D, M), nn.LeakyReLU(),\n",
    "    nn.Linear(M, M), nn.LeakyReLU(),\n",
    "    nn.Linear(M, 2 * L)  # Output mean and log-variance\n",
    ")\n",
    "\n",
    "# Example of Decoder Network\n",
    "decoder = nn.Sequential(\n",
    "    nn.Linear(L, M), nn.LeakyReLU(),\n",
    "    nn.Linear(M, M), nn.LeakyReLU(),\n",
    "    nn.Linear(M, num_vals * D)  # Output the probability distribution\n",
    ")class VAE(nn.Module):\n",
    "    def __init__(self, encoder_net, decoder_net, num_vals=256, L=16, likelihood_type='categorical'):\n",
    "        super(VAE, self).__init__()\n",
    "        print('VAE by JT.')\n",
    "\n",
    "        # Encoder, Decoder, and Prior initialization\n",
    "        self.encoder = Encoder(encoder_net=encoder_net)\n",
    "        self.decoder = Decoder(distribution=likelihood_type, decoder_net=decoder_net, num_vals=num_vals)\n",
    "        self.prior = Prior(L=L)\n",
    "\n",
    "        self.num_vals = num_vals\n",
    "        self.likelihood_type = likelihood_type\n",
    "\n",
    "    def forward(self, x, reduction='avg'):\n",
    "        # 1. Encoder: Calculate the mean (mu_e) and log variance (log_var_e) from the encoder\n",
    "        mu_e, log_var_e = self.encoder.encode(x)\n",
    "        \n",
    "        # 2. Sample the latent variable z using the reparameterization trick\n",
    "        z = self.encoder.sample(mu_e=mu_e, log_var_e=log_var_e)\n",
    "\n",
    "        # 3. ELBO Calculation\n",
    "        #   - RE (Reconstruction Error) is the negative log-likelihood of the data under the decoder\n",
    "        RE = self.decoder.log_prob(x, z)\n",
    "        \n",
    "        #   - KL Divergence between the posterior and the prior\n",
    "        KL = (self.prior.log_prob(z) - self.encoder.log_prob(mu_e=mu_e, log_var_e=log_var_e, z=z)).sum(-1)\n",
    "\n",
    "        # 4. Total Loss (Negative ELBO)\n",
    "        if reduction == 'sum':\n",
    "            total_loss = -(RE + KL).sum()\n",
    "        else:\n",
    "            total_loss = -(RE + KL).mean()\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def sample(self, batch_size=64):\n",
    "        # Sampling from the prior and passing through the decoder\n",
    "        z = self.prior.sample(batch_size=batch_size)\n",
    "        return self.decoder.sample(z)\n",
    "\n"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    },
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "8630772b",
   "metadata": {},
   "source": [
    "##  Typical Issues with VAEs\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Fig.4 An example of outcomes after the training: (a) Randomly selected real images. (b) Unconditional generations from the VAE. (c) The validation curve during training.\n",
    "\n",
    "\n",
    "VAEs constitute a very powerful class of models, mainly due to their flexibility. Unlike flow-based models, they do not require the invertibility of neural networks; thus, we can use any arbitrary architecture for encoders and decoders. In contrast to ARMs, they learn a low-dimensional data representation, and we can control the bottleneck (i.e., the dimensionality of the latent space). However, they also suffer from several issues. Except for the ones mentioned before (i.e., a necessity of an efficient integral estimation, a gap between the ELBO and the log-likelihood function for too simplistic variational posteriors), the potential problems are the following:\n",
    "\n",
    "- **Posterior Collapse**: \n",
    "  Let us take a look at the ELBO and the regularization term. For a non-trainable prior like the standard Gaussian, the regularization term will be minimized if: \n",
    "  $$ \\forall x, q_\\phi(z|x) = p(z) $$ \n",
    "  This may happen if the decoder is so powerful that it treats $z$ as noise, e.g., when a decoder is expressed by an AR model [10]. This issue is known as **posterior collapse** [11].\n",
    "\n",
    "- **Hole Problem**:\n",
    "  Another issue is associated with a mismatch between the aggregated posterior, \n",
    "  $$ 1/N \\sum_{n=1}^{N} q_\\phi(z|x_n), $$ \n",
    "  and the prior $p(z)$. Imagine that we have the standard Gaussian prior and the aggregated posterior (i.e., an average of variational posteriors over all training data). As a result, there are regions where the prior assigns a high probability, but the aggregated posterior assigns a low probability, or vice versa. Then, sampling from these \"holes\" provides unrealistic latent values, and the decoder produces images of very low quality. This problem is referred to as the **hole problem** [12].\n",
    "\n",
    "- **Out-of-Distribution Problem**:\n",
    "  The last problem we want to discuss is more general and affects all deep generative models. As it was noticed in [13], deep generative models (including VAEs) fail to properly detect out-of-distribution examples. Out-of-distribution datapoints are examples that follow a totally different distribution than the one a model was trained on. For instance, let us assume that our model is trained on MNIST, and then FashionMNIST examples are out-of-distribution. Intuition tells us that a properly trained deep generative model should assign a high probability to in-distribution examples and a low probability to out-of-distribution points. Unfortunately, as shown in [13], this is not the case. The **out-of-distribution problem** remains one of the main unsolved problems in deep generative modeling [14].\n",
    "\n",
    "##  There Is More!\n",
    "\n",
    "There are a plethora of papers that extend VAEs and apply them to many problems. Below, we will list out selected papers and only touch upon the vast literature on the topic!\n",
    "\n",
    "### Estimation of the Log-Likelihood Using Importance Weighting\n",
    "As we indicated multiple times, the ELBO is the lower bound to the log-likelihood, and it rather should not be used as a good estimate of the log-likelihood. In [7, 15], an importance weighting procedure is advocated to better approximate the log-likelihood, namely:\n",
    "$$ \\ln p(x) \\approx \\frac{1}{K} \\sum_{k=1}^{K} \\ln \\frac{p(x, z_k)}{q_\\phi(z_k|x)} $$ \n",
    "where $z_k \\sim q_\\phi(z_k|x)$. Notice that the logarithm is outside the expected value. As shown in [15], using importance weighting with sufficiently large $K$ gives a good estimate of the log-likelihood. In practice, $K$ is taken to be 512 or more if the computational budget allows.\n",
    "\n",
    "### Enhancing VAEs\n",
    "\n",
    "- **Better Encoders**: After introducing the idea of VAEs, many papers focused on proposing a flexible family of variational posteriors. The most prominent direction is based on utilizing conditional flow-based models [16–21].\n",
    "\n",
    "- **Better Decoders**: VAEs allow using any neural network to parameterize the decoder. Therefore, we can use fully connected networks, fully convolutional networks, ResNets, or ARMs. For instance, in [22], a PixelCNN-based decoder was used in a VAE.\n",
    "\n",
    "- **Better Priors**: If there is a big mismatch between the aggregated posterior and the prior, it can be a serious issue. To alleviate this, many papers use multimodal priors, such as the **VampPrior** [23], a flow-based prior [24, 25], an ARM-based prior [26], or using resampling ideas [27].\n",
    "\n",
    "### Extending VAEs\n",
    "\n",
    "- **Semi-Supervised VAEs**: In [28], a semi-supervised VAE was proposed, which was further extended to the concept of **fair representations** [29, 30].\n",
    "\n",
    "- **VAEs for Non-Image Data**: Although VAEs have mostly been used for image data, they can also be applied to other domains, such as sequential data (e.g., text) [11] or molecular graph generation [32].\n",
    "\n",
    "- **Hierarchical VAEs**: Recently, many VAEs with a deep, hierarchical structure of latent variables have been proposed, achieving remarkable results. Notable models include **BIVA** [45], **NVAE** [46], and very deep VAEs [47].\n",
    "\n",
    "- **Adversarial Auto-Encoders**: An interesting perspective on VAEs is presented in [48], where the prior is trained with an adversarial loss, allowing the model to benefit from adversarial learning.\n",
    "\n",
    "- **Adversarial Attacks**: VAEs are known to be susceptible to adversarial attacks. A possible remedy for this is to apply **MCMC** techniques at the inference time [50].\n",
    "## Improving Variational Auto-encoders\n",
    "\n",
    "###  Priors\n",
    "\n",
    "####  Insights from Rewriting the ELBO\n",
    "\n",
    "One of the crucial components of VAEs is the marginal distribution over $ z $'s. Now, we will take a closer look at this distribution, also called the prior. Before we start thinking about improving it, we inspect the ELBO one more time. We can write the ELBO as follows:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{x \\sim p_{\\text{data}}(x)}[\\ln p(x)] \\geq \\mathbb{E}_{x \\sim p_{\\text{data}}(x)} \\mathbb{E}_{q_\\phi(z|x)} \\left[ \\ln p_\\theta(x|z) + \\ln p_\\lambda(z) - \\ln q_\\phi(z|x) \\right]\n",
    "$$\n",
    "\n",
    "where we explicitly highlight the summation over training data, namely, the expected value with respect to $ x $'s from the empirical distribution $ p_{\\text{data}}(x) = \\frac{1}{N} \\sum_{n=1}^{N} \\delta(x - x_n) $, and $ \\delta(\\cdot) $ is the Dirac delta. The ELBO consists of two parts:\n",
    "\n",
    "- The **reconstruction error** $ \\Delta_{\\text{RE}} $:\n",
    "$$\n",
    "\\text{RE} = \\mathbb{E}_{x \\sim p_{\\text{data}}(x)} \\mathbb{E}_{q_\\phi(z|x)}[\\ln p_\\theta(x|z)]\n",
    "$$\n",
    "\n",
    "- The **regularization term** between the encoder and the prior $ \\Delta_{\\Omega} $:\n",
    "$$\n",
    "\\Omega = \\mathbb{E}_{x \\sim p_{\\text{data}}(x)} \\mathbb{E}_{q_\\phi(z|x)}[\\ln p_\\lambda(z) - \\ln q_\\phi(z|x)]\n",
    "$$\n",
    "\n",
    "Further, let us play a little bit with the regularization term $ \\Omega $:\n",
    "\n",
    "$$\n",
    "\\Omega = \\mathbb{E}_{x \\sim p_{\\text{data}}(x)} \\mathbb{E}_{q_\\phi(z|x)}[\\ln p_\\lambda(z) - \\ln q_\\phi(z|x)]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\int \\int \\ln p_\\lambda(z) - \\ln q_\\phi(z|x) \\, dz \\, dx\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sum_{n=1}^{N} \\int \\delta(x - x_n) q_\\phi(z|x) \\ln p_\\lambda(z) - \\ln q_\\phi(z|x) \\, dz \\, dx\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{N} \\sum_{n=1}^{N} \\int q_\\phi(z|x_n) \\left[\\ln p_\\lambda(z) - \\ln q_\\phi(z|x_n)\\right] \\, dz\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{N} \\sum_{n=1}^{N} \\left[ \\int q_\\phi(z|x_n) \\ln p_\\lambda(z) \\, dz - \\int q_\\phi(z|x_n) \\ln q_\\phi(z|x_n) \\, dz \\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= -\\frac{1}{N} \\sum_{n=1}^{N} \\left[ \\text{CE}(q_\\phi(z) || p_\\lambda(z)) + H(q_\\phi(z|x)) \\right]\n",
    "$$\n",
    "\n",
    "where we use the property of the Dirac delta: \n",
    "$$\n",
    "\\delta(a - a') f(a) da = f(a')\n",
    "$$\n",
    "and we use the notion of the **aggregated posterior** $ q(z) $ defined as:\n",
    "$$\n",
    "q(z) = \\frac{1}{N} \\sum_{n=1}^{N} q_\\phi(z|x_n)\n",
    "$$\n",
    "\n",
    "An example of the aggregated posterior is schematically depicted in Fig.5. Eventually, we obtain two terms:\n",
    "\n",
    "1. The first term, $ \\text{CE}(q_\\phi(z) || p_\\lambda(z)) $, is the **cross-entropy** between the aggregated posterior and the prior.\n",
    "2. The second term, $ H(q_\\phi(z|x)) $, is the **conditional entropy** of $ q_\\phi(z|x) $ with the empirical distribution $ p_{\\text{data}}(x) $.\n",
    "\n",
    "I highly recommend doing this derivation step by step, as it helps a lot in understanding what is going on here. Interestingly, there is another possibility to rewrite $ \\Omega 4 using three terms, with the **total correlation** [51]. We will not use it here, so it is left as a \"homework.\" Anyway, one may ask, why is it useful to rewrite the ELBO? The answer is rather straightforward: We can analyze it from a different perspective! In this section, we will focus on the prior, an important component in the generative part that is very often neglected. Many Bayesianists argue that a prior should not be learned, but VAEs are not Bayesian models, so who says we cannot learn the prior? As we will see shortly, a non-learnable prior could be quite problematic, especially for the generation process.\n",
    "##  Improving Variational Auto-encoders\n",
    "\n",
    "###  Priors\n",
    "\n",
    "####  Insights from Rewriting the ELBO\n",
    "\n",
    "One of the crucial components of VAEs is the marginal distribution over $ z $'s. Now, we will take a closer look at this distribution, also called the prior. Before we start thinking about improving it, we inspect the ELBO one more time. We can write the ELBO as follows:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{x \\sim p_{\\text{data}}(x)}[\\ln p(x)] \\geq \\mathbb{E}_{x \\sim p_{\\text{data}}(x)} \\mathbb{E}_{q_\\phi(z|x)} \\left[ \\ln p_\\theta(x|z) + \\ln p_\\lambda(z) - \\ln q_\\phi(z|x) \\right]\n",
    "$$\n",
    "\n",
    "where we explicitly highlight the summation over training data, namely, the expected value with respect to $ x $'s from the empirical distribution $ p_{\\text{data}}(x) = \\frac{1}{N} \\sum_{n=1}^{N} \\delta(x - x_n) $, and $ \\delta(\\cdot) $ is the Dirac delta. The ELBO consists of two parts:\n",
    "\n",
    "- The **reconstruction error** $ \\Delta_{\\text{RE}} $:\n",
    "$$\n",
    "\\text{RE} = \\mathbb{E}_{x \\sim p_{\\text{data}}(x)} \\mathbb{E}_{q_\\phi(z|x)}[\\ln p_\\theta(x|z)]\n",
    "$$\n",
    "\n",
    "- The **regularization term** between the encoder and the prior $ \\Delta_{\\Omega} $:\n",
    "$$\n",
    "\\Omega = \\mathbb{E}_{x \\sim p_{\\text{data}}(x)} \\mathbb{E}_{q_\\phi(z|x)}[\\ln p_\\lambda(z) - \\ln q_\\phi(z|x)]\n",
    "$$\n",
    "\n",
    "Further, let us play a little bit with the regularization term $ \\Omega $:\n",
    "\n",
    "$$\n",
    "\\Omega = \\mathbb{E}_{x \\sim p_{\\text{data}}(x)} \\mathbb{E}_{q_\\phi(z|x)}[\\ln p_\\lambda(z) - \\ln q_\\phi(z|x)]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\int \\int \\ln p_\\lambda(z) - \\ln q_\\phi(z|x) \\, dz \\, dx\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sum_{n=1}^{N} \\int \\delta(x - x_n) q_\\phi(z|x) \\ln p_\\lambda(z) - \\ln q_\\phi(z|x) \\, dz \\, dx\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{N} \\sum_{n=1}^{N} \\int q_\\phi(z|x_n) \\left[\\ln p_\\lambda(z) - \\ln q_\\phi(z|x_n)\\right] \\, dz\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{N} \\sum_{n=1}^{N} \\left[ \\int q_\\phi(z|x_n) \\ln p_\\lambda(z) \\, dz - \\int q_\\phi(z|x_n) \\ln q_\\phi(z|x_n) \\, dz \\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= -\\frac{1}{N} \\sum_{n=1}^{N} \\left[ \\text{CE}(q_\\phi(z) || p_\\lambda(z)) + H(q_\\phi(z|x)) \\right]\n",
    "$$\n",
    "\n",
    "where we use the property of the Dirac delta: \n",
    "$$\n",
    "\\delta(a - a') f(a) da = f(a')\n",
    "$$\n",
    "and we use the notion of the **aggregated posterior** $ q(z) $ defined as:\n",
    "$$\n",
    "q(z) = \\frac{1}{N} \\sum_{n=1}^{N} q_\\phi(z|x_n)\n",
    "$$\n",
    "\n",
    "An example of the aggregated posterior is schematically depicted in Fig. 5.5. Eventually, we obtain two terms:\n",
    "\n",
    "1. The first term, $ \\text{CE}(q_\\phi(z) || p_\\lambda(z)) $, is the **cross-entropy** between the aggregated posterior and the prior.\n",
    "2. The second term, $ H(q_\\phi(z|x)) $, is the **conditional entropy** of $ q_\\phi(z|x) $ with the empirical distribution $ p_{\\text{data}}(x) $.\n",
    "\n",
    "I highly recommend doing this derivation step by step, as it helps a lot in understanding what is going on here. Interestingly, there is another possibility to rewrite $ \\Omega $ using three terms, with the **total correlation** [51]. We will not use it here, so it is left as a \"homework.\" Anyway, one may ask, why is it useful to rewrite the ELBO? The answer is rather straightforward: We can analyze it from a different perspective! In this section, we will focus on the prior, an important component in the generative part that is very often neglected. Many Bayesianists argue that a prior should not be learned, but VAEs are not Bayesian models, so who says we cannot learn the prior? As we will see shortly, a non-learnable prior could be quite problematic, especially for the generation process.\n",
    "\n",
    "\n",
    "\n",
    "![image-2.png](attachment:image-2.png)\n",
    "Fig.5 An example of the aggregated posterior. Individual points are encoded as Gaussians in the 2D latent space (magenta) and the mixture of variational posteriors (the aggregated posterior) is presented by contours.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2533b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal, MultivariateNormal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the encoder and decoder architectures (simplified for demonstration)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2_mean = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc2_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = torch.relu(self.fc1(x))\n",
    "        mean = self.fc2_mean(h)\n",
    "        logvar = self.fc2_logvar(h)\n",
    "        return mean, logvar\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = torch.relu(self.fc1(z))\n",
    "        output = torch.sigmoid(self.fc2(h))  # assuming binary data for simplicity\n",
    "        return output\n",
    "\n",
    "# Define the VAE model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(input_dim, hidden_dim, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim, input_dim)\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        return self.decoder(z), mean, logvar\n",
    "\n",
    "# Define the loss function (ELBO)\n",
    "def loss_function(recon_x, x, mean, logvar):\n",
    "    # Reconstruction term (RE)\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    \n",
    "    # Regularization term (Ω)\n",
    "    # Using a standard normal prior\n",
    "    p_lambda_z = Normal(torch.zeros_like(mean), torch.ones_like(mean))\n",
    "    q_phi_z = Normal(mean, torch.exp(0.5 * logvar))\n",
    "    \n",
    "    # Cross-entropy (CE) between the aggregated posterior and prior\n",
    "    # We use the KL divergence as a regularizer in VAEs\n",
    "    # The formula for the KL divergence is:\n",
    "    # D_KL(q(z|x) || p(z)) = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    # Where sigma is the std deviation and mu is the mean of the posterior q(z|x)\n",
    "    # In this case, we assume p(z) is a standard normal (mean 0, variance 1)\n",
    "    \n",
    "    # KL divergence term\n",
    "    KL_divergence = torch.sum(0.5 * (torch.exp(logvar) + mean**2 - 1 - logvar))\n",
    "    \n",
    "    # Total loss = Reconstruction + KL divergence\n",
    "    return BCE + KL_divergence\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = 784  # Example for MNIST (28x28)\n",
    "hidden_dim = 400\n",
    "latent_dim = 20\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "# Set up the model, optimizer\n",
    "model = VAE(input_dim, hidden_dim, latent_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Dummy data loader (Replace with actual dataset loading code)\n",
    "# For example, use MNIST data here\n",
    "# Assuming data is already flattened to (batch_size, 784) for MNIST\n",
    "# data_loader = ...\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(data_loader):  # Replace with actual data loader\n",
    "        data = data.view(-1, input_dim)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        recon_batch, mean, logvar = model(data)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_function(recon_batch, data, mean, logvar)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch}, Average Loss: {train_loss / len(data_loader.dataset)}\")\n",
    "\n",
    "# To visualize the aggregated posterior, we can plot the distribution\n",
    "# Example for 2D latent space visualization\n",
    "z_samples = mean.detach().cpu().numpy()\n",
    "plt.scatter(z_samples[:, 0], z_samples[:, 1], alpha=0.5)\n",
    "plt.title('Aggregated Posterior in 2D Latent Space')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0e8167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the architecture of the Encoder and Decoder networks\n",
    "def encoder(x, weights1, bias1, weights2, bias2, weights3, bias3):\n",
    "    \"\"\" Encoder Network: z_mean, z_logvar \"\"\"\n",
    "    h = np.tanh(np.dot(x, weights1) + bias1)  # First layer with tanh activation\n",
    "    z_mean = np.dot(h, weights2) + bias2     # Mean of the latent space\n",
    "    z_logvar = np.dot(h, weights3) + bias3   # Log variance of the latent space\n",
    "    return z_mean, z_logvar\n",
    "\n",
    "def decoder(z, weights1, bias1, weights2, bias2):\n",
    "    \"\"\" Decoder Network: Reconstruct the input data \"\"\"\n",
    "    h = np.tanh(np.dot(z, weights1) + bias1)  # First layer with tanh activation\n",
    "    x_reconstructed = np.dot(h, weights2) + bias2  # Output layer\n",
    "    return x_reconstructed\n",
    "\n",
    "def reparameterize(z_mean, z_logvar):\n",
    "    \"\"\" Reparameterization Trick to sample from q(z|x) \"\"\"\n",
    "    epsilon = np.random.randn(*z_mean.shape)  # Random noise from standard normal\n",
    "    z = z_mean + np.exp(0.5 * z_logvar) * epsilon  # Sample z\n",
    "    return z\n",
    "\n",
    "# Loss function (ELBO)\n",
    "def loss_function(x, x_reconstructed, z_mean, z_logvar):\n",
    "    \"\"\" Compute the ELBO loss: reconstruction error + KL divergence \"\"\"\n",
    "    # Reconstruction error (binary cross entropy)\n",
    "    recon_loss = np.sum((x - x_reconstructed) ** 2)\n",
    "\n",
    "    # KL divergence\n",
    "    kl_div = -0.5 * np.sum(1 + z_logvar - z_mean**2 - np.exp(z_logvar))\n",
    "\n",
    "    # Total loss\n",
    "    return recon_loss + kl_div\n",
    "\n",
    "# Initialize network weights and biases\n",
    "input_dim = 784  # Example: MNIST data (28x28 images flattened)\n",
    "hidden_dim = 400\n",
    "latent_dim = 20\n",
    "output_dim = input_dim\n",
    "\n",
    "# Encoder weights and biases\n",
    "weights1_enc = np.random.randn(input_dim, hidden_dim) * 0.01\n",
    "bias1_enc = np.zeros(hidden_dim)\n",
    "weights2_enc = np.random.randn(hidden_dim, latent_dim) * 0.01\n",
    "bias2_enc = np.zeros(latent_dim)\n",
    "weights3_enc = np.random.randn(hidden_dim, latent_dim) * 0.01\n",
    "bias3_enc = np.zeros(latent_dim)\n",
    "\n",
    "# Decoder weights and biases\n",
    "weights1_dec = np.random.randn(latent_dim, hidden_dim) * 0.01\n",
    "bias1_dec = np.zeros(hidden_dim)\n",
    "weights2_dec = np.random.randn(hidden_dim, output_dim) * 0.01\n",
    "bias2_dec = np.zeros(output_dim)\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 1e-3\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "# Dummy data (replace with real dataset, e.g., MNIST flattened images)\n",
    "# x_data is a batch of flattened images, size [batch_size, input_dim]\n",
    "x_data = np.random.randn(batch_size, input_dim)  # Random data for now\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(batch_size):  # Simulate a batch for training\n",
    "        x = x_data[i]  # Sample a single data point\n",
    "        \n",
    "        # Forward pass: Encoder\n",
    "        z_mean, z_logvar = encoder(x, weights1_enc, bias1_enc, weights2_enc, bias2_enc, weights3_enc, bias3_enc)\n",
    "        \n",
    "        # Reparameterization trick\n",
    "        z = reparameterize(z_mean, z_logvar)\n",
    "        \n",
    "        # Forward pass: Decoder\n",
    "        x_reconstructed = decoder(z, weights1_dec, bias1_dec, weights2_dec, bias2_dec)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_function(x, x_reconstructed, z_mean, z_logvar)\n",
    "        total_loss += loss\n",
    "        \n",
    "        # Backpropagation (gradient descent, simplified version)\n",
    "        # Compute gradients (manually for simplicity)\n",
    "        # Note: Here we would typically use backpropagation, but we'll skip it\n",
    "        # to keep things simple without frameworks like PyTorch\n",
    "\n",
    "        # Update parameters (gradient descent)\n",
    "        weights1_enc -= learning_rate * np.dot(x[:, None], (z_mean - x_reconstructed))  # Simplified update\n",
    "        bias1_enc -= learning_rate * np.sum(z_mean - x_reconstructed)\n",
    "        \n",
    "        # Add similar updates for the other weights and biases...\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Total Loss: {total_loss / batch_size}')\n",
    "    \n",
    "# Visualize aggregated posterior (assuming 2D latent space)\n",
    "# For demonstration, let's assume z_mean is 2D\n",
    "z_samples = np.random.randn(batch_size, 2)  # Placeholder, replace with real latents\n",
    "plt.scatter(z_samples[:, 0], z_samples[:, 1])\n",
    "plt.title('Aggregated Posterior in Latent Space')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4d09d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Basic activation function: Tanh\n",
    "def tanh(x):\n",
    "    return math.tanh(x)\n",
    "\n",
    "# Dot product function\n",
    "def dot_product(vec1, vec2):\n",
    "    return sum(a * b for a, b in zip(vec1, vec2))\n",
    "\n",
    "# Matrix multiplication (for layer transformation)\n",
    "def matmul(A, B):\n",
    "    # A is an m x n matrix, B is an n x p matrix\n",
    "    # Result will be an m x p matrix\n",
    "    result = []\n",
    "    for row in A:\n",
    "        result_row = []\n",
    "        for col in zip(*B):\n",
    "            result_row.append(dot_product(row, col))\n",
    "        result.append(result_row)\n",
    "    return result\n",
    "\n",
    "# Encoder Network: computes z_mean, z_logvar\n",
    "def encoder(x, weights1, bias1, weights2, bias2, weights3, bias3):\n",
    "    h = [tanh(dot_product(x, w) + b) for w, b in zip(weights1, bias1)]\n",
    "    z_mean = [dot_product(h, w) + b for w, b in zip(weights2, bias2)]\n",
    "    z_logvar = [dot_product(h, w) + b for w, b in zip(weights3, bias3)]\n",
    "    return z_mean, z_logvar\n",
    "\n",
    "# Decoder Network: reconstructs the input\n",
    "def decoder(z, weights1, bias1, weights2, bias2):\n",
    "    h = [tanh(dot_product(z, w) + b) for w, b in zip(weights1, bias1)]\n",
    "    x_reconstructed = [dot_product(h, w) + b for w, b in zip(weights2, bias2)]\n",
    "    return x_reconstructed\n",
    "\n",
    "# Reparameterization trick: sample from q(z|x)\n",
    "def reparameterize(z_mean, z_logvar):\n",
    "    epsilon = [random.gauss(0, 1) for _ in range(len(z_mean))]\n",
    "    return [z_m + math.exp(0.5 * z_lv) * e for z_m, z_lv, e in zip(z_mean, z_logvar, epsilon)]\n",
    "\n",
    "# Compute the loss function (ELBO)\n",
    "def loss_function(x, x_reconstructed, z_mean, z_logvar):\n",
    "    # Reconstruction error (mean squared error)\n",
    "    recon_loss = sum((xi - x_reconstructed[i]) ** 2 for i, xi in enumerate(x))\n",
    "    \n",
    "    # KL divergence\n",
    "    kl_div = -0.5 * sum(1 + z_lv - z_m**2 - math.exp(z_lv) for z_m, z_lv in zip(z_mean, z_logvar))\n",
    "    \n",
    "    # Total loss (ELBO)\n",
    "    return recon_loss + kl_div\n",
    "\n",
    "# Initialize network weights and biases\n",
    "input_dim = 784  # Example: MNIST data (28x28 images flattened)\n",
    "hidden_dim = 400\n",
    "latent_dim = 20\n",
    "output_dim = input_dim\n",
    "\n",
    "# Encoder weights and biases (random initialization)\n",
    "weights1_enc = [[random.gauss(0, 0.01) for _ in range(input_dim)] for _ in range(hidden_dim)]\n",
    "bias1_enc = [random.gauss(0, 0.01) for _ in range(hidden_dim)]\n",
    "weights2_enc = [[random.gauss(0, 0.01) for _ in range(hidden_dim)] for _ in range(latent_dim)]\n",
    "bias2_enc = [random.gauss(0, 0.01) for _ in range(latent_dim)]\n",
    "weights3_enc = [[random.gauss(0, 0.01) for _ in range(hidden_dim)] for _ in range(latent_dim)]\n",
    "bias3_enc = [random.gauss(0, 0.01) for _ in range(latent_dim)]\n",
    "\n",
    "# Decoder weights and biases (random initialization)\n",
    "weights1_dec = [[random.gauss(0, 0.01) for _ in range(latent_dim)] for _ in range(hidden_dim)]\n",
    "bias1_dec = [random.gauss(0, 0.01) for _ in range(hidden_dim)]\n",
    "weights2_dec = [[random.gauss(0, 0.01) for _ in range(hidden_dim)] for _ in range(output_dim)]\n",
    "bias2_dec = [random.gauss(0, 0.01) for _ in range(output_dim)]\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "# Dummy data (replace with real dataset)\n",
    "x_data = [[random.gauss(0, 1) for _ in range(input_dim)] for _ in range(batch_size)]\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(batch_size):\n",
    "        x = x_data[i]\n",
    "        \n",
    "        # Forward pass: Encoder\n",
    "        z_mean, z_logvar = encoder(x, weights1_enc, bias1_enc, weights2_enc, bias2_enc, weights3_enc, bias3_enc)\n",
    "        \n",
    "        # Reparameterization trick\n",
    "        z = reparameterize(z_mean, z_logvar)\n",
    "        \n",
    "        # Forward pass: Decoder\n",
    "        x_reconstructed = decoder(z, weights1_dec, bias1_dec, weights2_dec, bias2_dec)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_function(x, x_reconstructed, z_mean, z_logvar)\n",
    "        total_loss += loss\n",
    "        \n",
    "        # Backpropagation (manual gradient updates are omitted for simplicity)\n",
    "        # In a real implementation, we'd compute gradients and update weights here\n",
    "        \n",
    "    print(f'Epoch {epoch+1}, Total Loss: {total_loss / batch_size}')\n",
    "    \n",
    "# Example visualization: Random latent samples\n",
    "latent_samples = [[random.gauss(0, 1) for _ in range(2)] for _ in range(batch_size)]  # Example 2D latent space\n",
    "plt.scatter([z[0] for z in latent_samples], [z[1] for z in latent_samples])\n",
    "plt.title('Aggregated Posterior in Latent Space')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    },
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAN0AAACfCAIAAADyNksqAAAgAElEQVR4Ae2d+1MbV57o939wfvUPe6v2Vs1M3TV7q3ZrHls7rkmq4s3OJWtnMt44U5txKnamljhjJybGWHIImIex8WDAGAcbZAX0ACye4i1ADyS1REsgC4RAEhIghABJCGRjQI9bzbGP2y2p1RI2dEhTXfbp06e7T3/PR+fxPd/zPX8XYf4YCdBPAn9HvywxOWIkEGG4ZCCgowQYLulYKkyeGC4ZBugoAYZLOpYKkyeGS4YBOkpgj7gsvlFMx69n8kRXCewFl2pEfeitQ0+ePKGrEJh80U4Ce8Hl+QvnD711qK+vj3Zfz2SIrhJ441xub28feuvQobcOvXvsXboKgckX7STwxrlUI+p/+N//ANBcWVmhnQCYDNFSAm+cyzNnz4yNjR1665AaUVdXV9NSCEymaCeBN87lkydPHLOOQ28dikQizNCHduVP1wy9cS4jkQjkkq5CYPJFOwkwXNKuSJgMRSJ7Yk/E1JcMaslKgKkvk5UYk34vJMBwuRdSZt6RrAQYLpOVGJN+LyTAcLkXUmbekawEGC6TlRiTfi8kwHC5F1Jm3pGsBBguk5UYk34vJMBwuRdSZt6RrAQYLmNIbHM7tLEV8ga24DHv3ZhdfgpPvYGtwLPg5nYoxs1M1OuQAMNlJBQOB54Fl9Y2bUtPDHNreodf7/CPzWL/wqORa5JI5+EpPoFhbs229GTB9yzwLBgKh19HoTDP+AnPQ25shZbWNs2ugN7hH8UhiIcPhFGb78Jvmhq5puhL+BjwkAnn+tLa5sYWU5Xu6tf1k6svt0PhBd8zw9za6Ks1Ip4wQriBM56RJrh+RkKIJz81zK0t+J4xbX1qeP5UuAxHIp7A1oRznRym6KtYZfnrpow0wV9/2Tikt9Y3d5McbYMa6diMdGwGnfHCR0041z2BLaaBTwrQg88lIBKrIEkba4iR3uFXTjibeuT36x/lFd64dOb2+XfvZ6QJMtIEhbk3y0j/SktLC3f+vv0u72pO7pWr390su1vf3N2vmRi1+zyBLaYDSpHOg8xlOBJZWtukSCQ64+2QojfL7n6bm19QUFRSUlJRUXHv3r38L2pLC6r/+iverZz7t3Lu30vyr6qqqqysrLCwkHU15+I3l+/XP5qeX2HqzoR0Hlgu/U+3KRKpmV4uv1d74auLxcXFlZWV0eAVfV1z5Q/ce/fu3Sn9PvpqUjGlpaW5udeusHN0hvGEZfNTTvDj4zIcCm/41t1jVveY1ak2mRqk+MMxMOrSW0yjDp15Cd80xwyjM97KB3XZV9ilpaUkeOV/UZt7hkOSINlLVVVVhYWFV9g5thnHTxk+km//cXC5FdjwmOdMDVJ1kVDG4shYHJNwyCSUAjrx/5qGTcrafsUtEUim4su0iE0344vmEp3x5lwrKi4uTkjVlT9wi76uSZgs2QRVVVVfnPtSO2YiKZ6f7CVac7nhW3eqTWhFq4zFGavpcaomAi7vZmAjZmkFQ+EJ53OtuN7h1834RvSz6ma1PJ8vY3HUzWqdxYOns7lPlX2FTQWmr39XX1pQTSVlsmnKysq+/OuFqcXA7sdDW4GNwKIX/EQtYgS0IRYxAmK24ggtpiTpEElTLv0ON8DR0qH2mOeCm1vkwnr6bBvPHCGsRWzK2n4CnQXFfwMjm4QwZaQJym/stmcZ7y3ZV9jSsRnD3Fqyqvjg5rbf4XaqTYbanueNQ5HAJBxyDOjxDYilQ20SDmHfXiR0DIwGN7fJJUmTq/TiMhwKu8esaEWrqkjoVE0kxBEIcWntGQHEmKeocQHQiXTqdDM+ivXlndLvM9IEVXfjcbWreFBfwty6Vp+RYxEOhQOLXtiGqIqE4He74V1PcGMwtGp3m4RYR8jvcJMnpsNVGnG54VvfIVLgMc+Fg1Tn8eY8G7BcqQRG9LOKWyJ5Ph+Rmqj0L8tvYFzuir44N1dVVX31dWanYgxme3TWv+CLgWZwc9tjnrOIEVAvUmxDYuLlHrPKWJyZfn3Mq/SJpAWX4VAYCN2pmqBOZCQSSRZKSIBGYsT6nSVNd6o4ly9fiakeAjjdyH4AlERx6EolGozHs9k5fUiMOXfb8hOg4NwKbLjHrKCZRstbQfd69+hMKucz0gQLhvndP+rNPWH/udwKbKiLhIaanoSNEV4K4XDYvJj0pCLkEgyM1M1qGYsj/UFSdrcm+wq7pKSkqqqKABpUXhLiUzutqKgoKCjKZufUN3fj5yrxGRvRz472jqHl2GjPUNPjMc/FG+rhBUIlHNwOiauMGWmC4YeoukhI577mPnMZWPSqsP540s2KKfmZbnzZwzDsdCrakKYuaV7hjaxsVl5eXmlpKahEd68kKisrKy4u/va7vO+uFVZUc4f0Vvh2GNBZPFrEBrUHKr5sTD29/WyTCm0U0zx7ul32+WDW2y0rzkAkEtkZIY1SvHfvk+0nl36HW8bipABlys035IAQAJ1OOGCXjs3UN3ffLLt75ep35//jHuuL57PepaWl+OlxAG5lZSU+sqSkpLCwMC8Pmx//7lphXuGNHx6JO6SoZnqZ8FJ0cgmwOFwplrE4ilsidbMar22ddCUYzVDHZcUZyHq7pezzweALW+ZVu1tdJKT+hD1OuW9cbvjWZSzOqj3pseGCL7mBDoEGklOoTsK08XIz0HdmpAmGECewEmrqkeONiX54JK4VNONj6pu7+xCTdGxGOeHEvwidXEKNC1rEppEYVXwZABGwCN6FTsaenZrzxFbWJkWJRmzHvoI/hb8rHAzJWJzAohcfSZ/w/nAZDoXVO5qgZAWx9nQLX95vIoxOLiGduuczRjcfZaQJ5A+HNBIjlUNZ248/4LQTGEcPV4qVtf1Ip04rN6PGhZizUNFftLKWemse3A41XEez3m6x6JeiRW3pULvHrNHxdIjZHy5NDVJDTU+y3/9sOxRdbG8uBjW5h9X2Lp5W0WMY/L5Xcq9Hdr9XvnMM1/Th+YuJrBaxocYFcOwyk0+fpaIMX/Ns5H/YVfb54FqcStc9ZrWIkWRLYW/S7wOXfoe77EO+e8aX1BeGw+HHlC3MU+ZAOe3tNS43aBYeKubhwVc7G5AFcHBx8Q2ahV7jMmKNMfmecgbi3RgMJWccZ9EvZb3d0nAdhR3KaGm7x6ymBml0PB1i9oFLtKK1IVeekSaoyVLG+ylHi2Zm6Um8Mtt9vHLa265fBCAC2pTTXq0tLnBamw+x+vpwBLfrF5XTL23Ud58lwhOmFrFBNMU/oAzSiO3k6YEpDHma/bq611yC4U5wc2vNs1GTpcxIE4irjM+eJmin3lC3UmdflZk9dUqsamzTYWDp7KsEIKicKqe9LajroWK+bni+f3w5tYckfBGVjiZBGUROVcDlZepLzL865tN6YNTSoYbyWnEGgFJNI7YHt0NA8QuvgkAwFE5YZvES6Gw+RDcrqRtsy64RZdyGR/P5O+3FTfV3e7k9U/3jy+hMKjgSXqqzr/aPL3MV81wFRifh6ms5JW/NoTIo4e8cCJZpx5/7/Y9EIuoiYcBF1E1Y9Ev5H3aBYWNNllLdPoNHc9bzNNlCRaeWh1qQps9u8tKzeenZoozbPZUd/RxJP0eiUll6uQP1t8X1ecLGv/yNl54tPJU31ILo4rfayb59yLTCVczXKedfY9cT0c2qtTMTj+c2Vl9p0J893QZ9IaPMCRofvOjIwwyXL7mUsTgxZ9WC2yGN2J71dgv7vbaMNAHUayQ7Bkd0s83n7/DSswUf5fZWdWqNLgJV/cblh4r5FnQR1JHo1PIAT8o/weafYHfeaESnXk89p7OvtuvdDxXz3YbYiklCrginOptPpbJ0FjeKMm6Dnxb4t+nMTf5xNi89u/fSvSmxKhQM1mQp3Y41EmUQCZoMl8+5BJ3LmJIKboeMMqewcASsPMxIE4BqwOyiOgmO6GZBBRkPL3RmtVGzwFXMK6dfMRAGTCj6HgtP5fGOs6RtWgIlKZ8iVh9XMc9XO0mGUPiHa40uSd1g09kSXno2/wS7LbtmqAVBdLP4X5dh1r++6J0Sq5o+zqv/0+2MNMHtswP5H3ZRH0FC+TNcJuYyEomseTbmp3wasb3+O82djKGs3zUveym14Dqbr7eqk5ee3Z7DjVfhaW0YIgK1k3xQIm3T8o6zmj67iehm8cSkHNbZV5s0Lq5ingRNrdHVW9Up+CiXl57d9NlNSd0g+dvXnmJW0lvPts7/Mx9bPXyyS90+A1sYiF3CgHvM6hig6RT5Xo/H47XjMYVomHvpISgeGTqbr+lsCf8EW6mYipcGQPlI6yKHEtyOTi2353B56dnx6t14byGJ7xh1R9fTiG4Wj+NQCxLvR0V48uNZfyQS6eeaQNvC+m2dUeakONbBy9mpmmDme56Px9GK1uhxD15YMExFNwSgbDpbQjJqAVB2jLoJpUt+CnsFAzwpycPJH4K/2jGKdTdl48sqlUVcJOSfwLqJonPl1HHEP23Bud5wHbXolwLeJ7z07O2NVOYqTcIhhsvnXJoasEWMED6SAPBohS8MQlhn8wk/zieHEp1Z5Srmk4USvgh0Ovkn2D2VHSqVhWJ9Bm8HWipF/+N+jqQ16z7vJNZS846zxUVCRd/j3eBumMOqTPAnOMH2zbhenCXxPzM//nI8ji3fKW9JKLztYOKp8PYcrvBUHknp6uyrfLVTiCxQab4hTNEBRd/jtuwaUMMJPsptPn8H6kFjBpp21E9wHC34KFd0rry/pk/R/7hZZqsbnt9lfkAOn72wWOu5WDnZNpxQpNEJmHHPSy6Dm9uYeVWUCpMgtYQ6S6ViChvoCI1ofL1jC7pYr0ww0ImmkCQGnVpW9D2W1A0CVWjMf6WtGkQ3C47o34zOvlqvdLagRO0VyUvjXbItPQFCm2wbHrr2A0GAVE4ZLl9yCaZ8TMIhEsGFw4kneJo+u9ldKb54VMR+X9zVbY8uPKDcJhkFR9+Cj0Htq91jbo7MUdBqLmg1X+IbMzlocZ7w5oU7Nz7+9rt/O5N5+Bg42EdOVpzOba4Wy9XT+CfEC2ttPqyjORlDVxXvlnjx4R03sC6DtenjayTyjHdpw4st9It3dX/j93o8jik4AhsyFodkNU/CEQ86tcxLz9YaXajN17gzLC05J5VpXlZCOjvWrUxtMrB7zF3aZcnkGS/xjVWSGb5qvhV1tYl1V/7x5PX0r+sqWvOu8jJL+jIrZAUclXhgQq6ebq4WV5zOzTx8LP/o2d52JB5GML5/HNPt737y0xfAFEbri15eenYKGG14MdPscJKWSim8KIVb9oHLSCRiESMkvUz7SgK1JaZ8/uwmLGaN2fPgOqaQr2SpNGasHmrXu+uVr1iMw8QkAbnZkyuavMQ3cmQO+c5zQOLediTz8LEOwSC8F6tNDe7yHuslvpHVMCExYrNEqNXbXC1mHfmvb39zXjU6BxPHDDRqFppwP6SYaRJGml3YnGTKXEYiEVWR0GOeS4GbN33L/nAJ7NXdo7EH5gnLoy27pp9D9N4r07hKzkkz0gT82nGOdC6puWnUvlrea83kGb8fmIlu+vOPnm2uFsfMFWpfrXhxI2pfRW2+/P/uvfir79lHTqJWMrM3xIq15kllMmYGwuHw+qJXcIKdGijuUcyLRGr3vtG79oLLlZWVQ2891xPBj/GY52Lq2J9uBmMWAD5SlHE7mkuQoKvb/s1/tF/4N1HrI0q9Pb3Dr7J4c0WTrIYJlSUGSXL1dELIBsaXLwvGc0WTZdlK9vti7dRKyfHMkuOZ+DxHh5u0rkbNQnR8UjHrG1vri96uCxVQsEkFaLvKZy+4jEQi0VyC1txQ00NwZEBlWRmmdomqL0Fxgnqo4eEEGBLBTqcyjv2EyuK9xDfmNU+icSwvqy+UCkqECVlB7atXvhr64jdNg+ii3uHXTrozDx/TTpIp80FWd9nLnPU8nRKrUuYSDEMNtUmvaUmK/hQS7yeX4VAYrWi1dLyyxGR8PrGhRj9H0pZdE5OVdv1i004lhNp8oNNZck6qNCx1ddvrKg2EW1D76iW+8f5gjOE8SAnwIu8sgoq59dF0RprgziOsewoQLzmeie+SEl4NTpu0rnY9Gbsx78JHGub8k23Du+EyHAypigQUJztSICy1W/aTSzg2x3e98UKPFx5qQQQf5UZf1dlXH2LmQi+bY43ZAzqd9wu1GWkCvEYJta/mNU+S1JR6h19QIsw/ejb6RfiYi0dFXd3YQljw8OL2qeJ2bKa++kJpvF4pvP21VJl2xdhuuIxEIqt2bCE/rXwR7jOXkUgEeDcAaiOK1pZATxRtdCMze7iKV3Z/AgQMqRYu/3tbxj9hvvthy96IOGHdBkHBB1Crl33kJDlbgMiLR0Vwdx/UvspqmBCq5ytO5yasL/UOP1cxL8ON/fEZoBhenltOedwDKzOTEHO7DE/3PbD/XIIujgpzl7OVUHMJi6otuya6KReNxGgWJdJ5UGUC65sLv25SGpa0Nl8mz9g9RtaG9rYj2f/nj7fFk5/d06YXy9OL5V9ydBW9Fsn4S1Pf4r8MZqQJLv9728WjogfXR8Dkk2R8OZNnzDx8jIqmvV3vbtVhXdKUjxnzfGr6Szx8wc0tGYtDHxeEtOAyEokYansMtT0LHqqLHhHdLC89m1BlEhpxQklrzB6ZxtXINdVVGr4fmLm+09oS0sDTkZnVy7/89OSfK4/fVLCFjxuR+c7RxbLu6U/vat4tkJ6tHkEsXo3ZA1i/fkZCGFcV3hvKPHyMXFUE3iU3ex7GquNhThIG7O41Xnr2+q49Z7hHreoiIU3U7HThEmg0ta2ahMUAE7TncPHadTC/B6+SBEBlidecExIjFu//nLuf+fMPVeYYyyoQi/ds9ci7BdK//W2Eld4BOwb4h9Revf/Nf12LVoXi04Aw6BNTSRl9L4iZXFgfuvYDUi7C138phMPBEFre4lTTwt87XbgEYyBFPl8jMcYrAEK8zubjn7gKlz0Mmlb4KkpzPK2o60rDOOFp8FRuXjn2Xd+lX3xI3jv8Qe44drm/Q/dy8hM+Qe/ws4+c/K6kk6J9Bl/tHDCt4G9PNuybcaVshYknGAyA8DH7FaYRl5FIRIvYZCwOaqSqbVb0PwYT5XqHv2vM3Uato5YrmuTKYy+TQCzeY4XSi3/MTagV1zv8xe2TxwqlI1FrfAcHHmcePtaimc8VTVIhrGvM3a7fVRczEom0/+WW4gZ/9xhhpgu+1+ZHLuX80ItLvcMPfEBS9Cmld/hbvrrb9NlNnc3XgGBeWRJygNpXM3nGmFM7eof/20bje5daKA5ZRmZW378h//qHUcJLuTkPqi+UqizeTJ6RSgONeZ5BqP4UCe8Cp8EQNhv5WnqZYPeZlHl6XTfSjkvdjA/4PYtZANGRO605u+uWiK9yknQZ4Y1ysyeTF7urMDKz+m6B9NLvznFzHsD0IDBi86liraLsHF1859oQITHUqH/DN1LJkmR8mUetB0J4ETwFNsKau62P/pSKwRseJpoYZdKIy+0XfjXQySUZi6NFbFDu5AGtEetdcXkIFTMIudkTr3MpGV86dqWTMH84YvN9Xio5fPLe4ZP3Pi+VjESZIb9zbQivOdI7/LC6LWg1d5GqosB3Ae06+TeSXwVchoJBwQdslyG2NQwePpKwpUNNh6EPjbjcxLkRBH75qbfm0jZt/XGWypB43CNUzxe0mmMW8w9yx6cfFlScfmUmiSuZPnzy3v+UDw6Nu3/xKaei/THh3pOlSo7s5UwmavVCsgtazXxVDD0/4QkELmUaV1e3nbrdid7hh2sqpns0wg+ukmCX8JKqSEgHZ6005VLv8A9XitXNakIRkpzWZdU24owy46Xkq+JyWdY9ffFnfyAY9r6d2fj7Ky3gaVc4qrczGwlPPs/Vl3W/YrsErYNvd1spcnlXaC75YijrGOZrBJtnv6IkWR9CyACey1AwuJtepmNATxMVJn25RI0LSY3NH8pm+R9cldQTe3uEUiThsuL7/syff0hQhhc16A6fvMeVTIOK89KDYcIDP6lEKnot+EhBibD6Qqne4U+qvmx9NH3h102Ay4w0Aft9cck5aSPXJJHOA2Nn/CsIYVhfRiKRnouVU2JVwnoxOoFjQE+fWXL6contT1/bT73KfKiYl0pNUG1EKDl4Ojix/J0oxq45eoe/6q9/++yPhTAlCKimPb/4lAP6l7/4lBM9+onuX8rV05mHjwlKhAWtZqE6iXYctfmq8zWY+yuxravb3sg1lZyTXjwqArCWnJPWVRq6uu3Rmnw8l2P1fbLC+mjsSGLCwZBTZWLmIeOKiMAEqDIp9jIFamw8DhbvEp6DPyUZj7OPnPz9hYbO0RiqxGa1o1ntwD8HhOXmlXeuDUWrMIE1cdapfOnEy8n06NtBDKF/qTQsVbJU+HYczqBWslTs98UA0/xP+ipZqkauSaZxPdl46UC07fObjuHHcUUcdWHV7lYVCdRFQjqoLWHuaFRfRiKR6JJT3BJp5bGHKYTEQH8J1EbyPqKpJUwMJiGj1YrA1PLrB5ovOXqYOGHgbPXIqfLYnWCF3pH5z6dv/OfFwQHiUInwWOozVfBGmcbV+mi6rtIATVKy3m6pyVIO1Ohq3vuW4IsQFjY+ENzc8pjn0PKWna1qRmkyLQ5zSHcuNRLjcGXstTWwkECgXb8IZk0k9UONfy4iXMWfXmkYb41awd3bjrCPnEQs3neuDTUiiRtfvcP/g9zxzrUhuRmbQgQrMxu5pkauqfXRtEzjEvXbv6nRc3MeZB4+xj5yUlAijGdf3GtcFo3Ens/EZ5skDFyOdVwfzPnXe+1FEli60YGAy+tUTYzVYFvwohWt7jEr3YgEeaYXl4a5lxuIg2IATTlJkcBLA6YVvhrTE8FVvPASIcCVz0bPEMLBSonY/G6BlKCSJDxB7/DLzSvvFkjv9r/cvAzYYoJGNu9UT+FdfVUfpoJFrd7edqTkeCZYb15xOpeb86C5Wjw48Fiunparp3/oMLRKJgjjreg3ksRsb2wOXK3hpWcvGW0EFjcDG6t2N2RRxuIAVzy0sgIm5DkSidCLS1ss5/7YLzvOtkv4osL30tqyazqvN+Cv4sMxZwiBbwKQLPfR+DvXhorbJ6M7jiDB3X7rO9eGcpqIxh8AzfM7w+ovP+rqkL4yC6+ddIPF5oISYcXp3IrTuewjJ6F/hOhJJnye44WVcnNXAY9/nCX67wJo6gbaaEuHWlUkBPWiRYx4zHO06kFGs4iPoReXS2ub0QVAXVsEV76qVBZeenb0o2DM9fap8t6XVR1+kgak+V5ie7dA+v4NeVn3dOfoImLxIhYvMMF8/4b8WIE0niVRV7f969+KynjjF88NAHVPV7cdP4iBeQABYJ43rJ8FbT3hasxT4DFedK4cuEDq+fYhIDIcDHnMc4YXbbRTbfI73HTenBQPIiFMLy69gRjbmQ1XiinOSQrUTuA3Wmfz8dKzSdyvgVE5fvIaTh5CFEZmVu/2Wz+pRN65NvTOtaH3MvveLZB+eldT0WtBYq3ohTcOoosXWAq52aMxexq5potHRRePiuoqDTHVkD2GJWCeB7RL8QZJRI/x58qh+9Yl/7NwMOQYGAVVo1NtonkbTUAw5im9uMRPRcJiVtb2UzTKHDSt1A0/H7JEW7PDB4JAea81V/RyeW40l/j0iMV7+fft5VnDMdnCp9Q7/Nfbp250vHQSi9p8Xd12oN95cH2EoH2sU770wR5zqRrwJgc9xhNM9PUOv9u+pCoSoBWt9FkFERO1pCLpxWUolscs6lzibb8FH+VGFyEeIK3NBxfpgklt/NXo8A8VoxlpgvO/amzgELuV+MT3B+2X+LHN22QaVyVLlZEmyP+kDyyeBI04VFoBP0f4p3XdEmFe/qs649X9GolRxuLQdlidFIv4xPTiMhKJTERtLK6s7afYjusd/kcvlp7xT7DJuQSeNjJ5xjadC7SheCCiw0rDUkaa4Iv/K7jwr4/yP+kjVHsgffeYO5OXwLZNY/bUVRpA436/ylCHGxsNDjxmHzkJHqWz+TDT0rMl8YjEvCfsmFEfpGoSokk7Lhd8zwhMUB/36B1+2c4yLnRmNWE7Dt7S0GG9WDbS0IlZmBPeG316+b22gtP97PfFMccxjYgz4RpL+EzU5mttmr64synMg+sjYNka/ufRdLYkgSvkHfsBq5GOXq8gXikHaMflxlZo9NVFq0lxqXf4QY+NfNwD+ABNc6t8LrNChi1fjOMKBsJUV2noG5zN/6Sv5JwURmIayh3vWZf4CRb+4m/RO/z948t1ynm4jBiz0niAOY7TO/xgtWe0W1f8E9TNamVNf+BZMOWyp/ONtOMy/OpspG7Gh3FJQX8JywzzbjBgI9cTacye3FM9GWmCC79u0jv8QyP2zMPHsvjGJo2ThE6lYUmmcSkNS3hHBt1jblbDxJWG8XhrM2DG8AHgoRN6NACuEr/6TcO1336ud/h7qzqjV8e/crvFA36uoR3XrHQmLLW80Y7LSCQytRiAZUB9vgfeonf4hV0TJFxKpPMXj4q+eaclI01w/cxzf4WYBZDUdolvzOIbH5HSqXf4JdL5jDRBQ4f1auNEJg/zcERCMz5jMNyud9cpibOdFadzgYFc46fFQy1k/l3B9OyEc/8XiKWGXcK76Mil/+k2bMq1cjPF+XFY5HqHf0Ckrv+8NOa+ZlgfVONS6Bcz0gQFf+6D/luAngi1r3aOLV5pGP+GbyztsrShLrnZAytClcUrN3vaUFdpl+XCRWnGb5oq26bAaFo76e4QDDZXi8HR247I1dPx/LmBqano7LGPnOxtx3aqTGith3Tq1M1qf6J9iBMWP20T0JFLfFOukRhVfBmeOSrhnsqOhm/ucxVxd34ARmJ1lQY4rCboL7vH3HXDs7c6LeydGhHz6/LiYDVMVElmBieWS85J8z/pQ22+DsEg+8jJkuOZYO674jS2zBdMMJYczxSUCPEOYXT21TrlfGfUuh9uzgPgaBMsVyL/TIoRg3gAAAy9SURBVKA7O6iNOO3mx+HPd9azAQqGuvISX5A7jlv7BerYOz+AORjU5pNIX7ak+Plx/KNAWGvzwVoTXkVtPlZ6+5W377CPnIzpBEE1OtchGASMso+c5OY86G1HmjXOOiXxBwPMjuBDEioTlLX9RjktHGPAInu9ATrWl5FIJPAsCJpyzP6S8sJISIzgo1xsG6idTaXEr+50BtSQeCLBXdCeCD6ESqA4/fKFf6lVjiZY/Q1Mirg5D1j/iBlq5P32TMXpXEGJsLlazM15AMDFe38F+SfJgFqknpGPv14UaPU0mnIJFezJKolAWcL6Znga80oFh716hx+04NFFDiq26HiSmA7BIFz6SJIMXgK61T5kZnDgMSAS0knoiYrOlfdUdsAbowNaidEkpJFbwNfONH25DDwL6saxpWfRpUIeA+wvYRpAA0ATtuDwKgwAnTZ1I0jtpDuhd0z4cKjwx/9C8FcJ4XiOZ2GyWeuiukj42mmgzwP3jsuVlZWkPjsciei0M4pbIlgYFANAKY1PDNDs1bqgV1/8VRgGw2F4Sh7g5jzIP3qWIscgA8DWifyx4GpC0+bN9acyFicpef64Eu8dl45ZR7KisSFTytp+KgWJTxPNJaiuLp/szv/LAMnejM3V4oReq8GLwGIgwkpzfB7wYbCLFMWaEt7Y9NnNeCrMxdVnYOudAzkzDiChNZf2gVEtZbeDsEQR3Wy09/VGrunro6JaiYOvdkL7HXgLCACrIjgoJlzFn1Lxu653+HX21SaNK3rncfyj4oWHWhDhqbyYV8H2e/aBUYv4lS0Tkv3Z0zk9rbk0NUitCKXFkPjyi64vwRi89dG0zr7ajLoeKuaH4vibBJrIeAvEwFtAZYlXSeLfDsPA2btA7UxtJ5R4PuRX1p9vNR5wYVOR9Fw1tnvi6c6le8yacCNyiAIIRBurEywtwJamDchCTD9bUL9NeCw8BUtz4Gl0ALH6GjULD1PdoBI+sPn8HXHRK1sH4bcdP9hNOa25NNT2BFxeKnuRw7IEAV56tkr13DcLGIMT7MzRmdV2vfuhYr5RQ6QTtXpLjmfmHz0bs9YEy3ljXsLsgF4Q2YK6Uqsm8R+i6HvMP8HGx+D9auzsl6A/qE05rbnEnEB4MdOEaKNMfGlFhztvNDafv6N3+DVmz8Wjcffkg3Ty1c5B0wokCbV6qy+UEvYqxcx8dtaYR0Ops69ivgnUzpigR2ePegz/BBv+wGY9TwntI9gQ90e6sozwLYRTWnMJVjpHIpFwODzhJC4tJyld0DlT9D0uOScl2EpG36Wzr/Ybl+uG5x8q5uuV2Mo1xOpDZ1aB2hzMHzZXiwGp+DE4YvX1GZcbdppsrgJbpgPJjn5LajFt2TWgKTfM+cFwh1B+aHkL3bYqI+QwtVNac+kYGHWqJsCHJduaD/Cklf+v5OJvRYQWnIQPdGZ1wLTyaAQbGIGDNzx7/6G0/Juq4o9z7l2ra5NbGpCFBmSB+yIBX+3sMy7HG+CTvIviJdiUE1pwWNge89yBVLDTmkvMp7JwCJYB9V2nMB2NzdeU9bD6vTxJ/RC54XdMRBCrTznt7TUug6NRs/BoxAVPldPeN8ciPj9gDDdvWYRCIAR2dncUHjxFJq25DCx60fJXdseee2FnhC88krBSMSU8lSf4KLensqOfI1GpLIhuFtHNqlSWfo4EHG3ZNaKM26KM242fFvPSs0G4p7JD0ZfA3xXJe1/jpYZT16AjDQKU4NQxMEqrLfRiZjLZSFpzGdzcxrbt2Bn6gA9LtqMJKs6hFkRcJBRl3BZ8lAt8VDR+Wgz4a2NzAJ3ATQCimx1qQfo5EnGRkH+CLTyVt790TjjXBCfY5FxuBjZkLM4BG/3QmstIJOJUm9DyFvwe5SmgmVrtpbP5BnhS/gk2/wRba9yVv7XUMvB4zv/Ut85Lzw4FEywuQ8tb8XsSJ1s50TA93bkEe5S7R1/ZYyEcDo8nMzxPDQtwl87mA84F9r7i3A6GHMOP2z6/mZAbp2qChnvbJ8w2SQK6c4nZCC96ZSzOZmAD/xnJDs93g6be4Ze2aYHfi10+h/rt28FQJBKRFdZT2fjx4DXlPwIuI5GIRYyoiwTuUSu+Qd8Ohvas1sScvSum+CfY7TncFEb31HEEKQGUHouTl55NxflvJBI5YE35j4NL0NHEbNfLW1btblhx7llfE+CiNbqEp/LIfbMkiyAh/eM5P4AyFAwKP7j6WEDm/BfKAZOPasIxMIqP+VGHfzRcggbdUIs5YDYJhzAvozvj9HA4PLP0FF+6uhkfalxAjQtauVkjMWokRmVtPzzAJn/gVN2s1kiMSTlNwNSiZ0t4x1kDPOlrrzjNi+twUgetbn/0p2sJRzwQPprsnwfzs8vAj4lL8KkbvnXHwCgAVMbikBwAPhVfBujUSIxaxAZ5RTp1IIGMxaHuYhP8ABR9j4Wn8vgn2AM8KYlfK/yvJWF43vOyA41Wtye7PZR71HqQtJg/Pi7xP8QN3zo8Vpf8oxMu1LhAcV8VCIrO4kE6dfJ8Plh7Sf12QCfwTNlT2aFSWVKuQdc3tsB3hYJBxQ2+8AM2xW4llAZTX0JRJBE49NahFNZRJPGCnaShcHh6MQB9dUDyqAR0Mz6wKaXiliiplh148m3LruEdZ2HTRefKh5rV1PWdE861YCgMvnR90dv+l1vCD64mCyU2NOxQHyQDjh93fRkTXP/TbcPcWsp0qvgyeT4/hUXrwA9bb1UnmM/kn2A3n78DvU3H/G0MPpo2ypzPnm77Zlx9l+7x0rM1d1up9ynh54eDIRmLQ4f9RmGWdhk4gFxidnHJm2ziuQFOeKlvAYi/F4R1Np9KZemp7Gj67CaY+eSfuCrKuN2adb//Qa+EOyCvHzQIJW3n7oJ9VS79ywPR5UcB75PUinPV7j5gVkUHk0tQupvboZgbr0RjFB2DGhfk+fzhSjH17mb0Q2CM1ujCLEUeSnqrxB2Xvu86X9FzsXKybXiybbj1phK4Ia76UqYR21ecgRTQRMtbDpKSaE/9E+1B/zJmiaZMp87iGa4Uy/P5qDGBmxfIH3nA7ArA8Q3ManA79O3v27//WpGRJjDKnDCeegCYYB6wBWgHub7EFy2kM9l+p7pZLWNxKO5RGY9LsysQz7A3EonMT/lqspRGmRPbSbfKGNzGZiAp/gU3t7CZsLFX7Aco3kvnZD8VLkEZhMJh/9Pt6K0F4vEE4oF7fXWzOtk23TDnX1nbhMNtEg5ATbnm2ch6u6Xs88E1nC6T5K5wMGSo7TlgFhvge39aXMIy3g6Fl9Y2gWNiKjUoOrmkuCWS5/ORTp3O4iHneHx+bcG3QVJBwmxEB4LboZosZdbbLfNTvuirhBjHgF5dJDxgLTj4xp8ol7CAw5HIxlZoaW2Tyip1LWJT3BLJWBys7nyVzvH5NcfK07WnW1RqR/j2eIEh/lRGmmCIPxUvAVikqy4SHoCtzWJ+40+dS4JQQuHw5nbIG9jyBLZsS0/gMekKTLoC4HTKsqTlDmB0ljZP96ErVldw8/lsDeFpuzmdn/KBLcWju5vhYMg+MHqAofxJjMd3AwfJvcHNbY95bscAD9uzdqymx6maCLi8eEs8ktupXFrzbJR9Ppj/YRe+u7kZ2EDLW9CK1oNaUwLJMPUlFUISpNkKbHjMc6YGKTAiMQml7jErfllSgvvjXw5uh8RVxow0gUW/FA6G3GNWGYtjESMHsk+JFwPDJV4aryG84Vt3j1mhuRNgdJf16NjAbEaaoPUr/gHbnJRE3AyXJMLZ1aVwKAwYhfUoWt4KrCsCLm/C2jS4uRVwed1jVksHpkBV/w3zq3Hgq0kocYZLKIo3G9gKbPgdbqfaZGqQqouwLik4xmp6TMIh/KF6cVVdJDQ1SA/GfuLJCpfhMlmJvbb0W4ENYDzqMc+5x6zg8DvcIPKArQdPVmoMl8lKjEm/FxJguNwLKTPvSFYCDJfJSoxJvxcSYLjcCykz70hWAgyXyUqMSb8XEmC43AspM+9IVgIMl8lKjEm/FxJguNwLKTPvSFYCDJfJSoxJvxcSYLjcCykz70hWAgyXyUqMSb8XEmC43AspM+9IVgIMl8lKjEm/FxJguNwLKdP8HWpEfeKDE319fdvb2zTJ6t5xqVKrHLMO5qCnBFrbWg+9dejQW4fOXzivRtT7Tucecfnxnz4Gn838m5oEfvbzn505e+bNHZ+d+ezv/9ffg7y9/5/vj4+P7y+ae8Tl/n4k83ZyCWxvb5/44MQvf/VLoVD45EmKPuXIX5HsVYbLZCV2ANOPT4zvl1ezeNJkuIwnGSZ+PyXAcLmf0mfeHU8CDJfxJMPE76cEGC73U/rMu+NJgOEynmSY+P2UwP8H9gT7luohCxUAAAAASUVORK5CYII="
    }
   },
   "cell_type": "markdown",
   "id": "5fb5a97d",
   "metadata": {},
   "source": [
    "###  What Does the ELBO Tell Us About the Prior?\n",
    "\n",
    "We know that the regularization term $ \\Omega $ consists of two parts: cross-entropy and entropy. Let's first focus on the entropy term, since it is easier to analyze.\n",
    "\n",
    "When optimizing, we want to maximize the ELBO, so we aim to maximize the entropy:\n",
    "\n",
    "$$ H(q_{\\phi}(z|x)) = - \\mathbb{E}_{q_{\\phi}(z|x_n)}[\\ln q_{\\phi}(z|x_n)] $$\n",
    "\n",
    "This is equivalent to:\n",
    "\n",
    "$$ \\frac{1}{N} \\sum_{n=1}^{N} \\int q_{\\phi}(z|x_n) \\ln q_{\\phi}(z|x_n) \\, dz $$\n",
    "\n",
    "We assume that we are using Gaussian encoders, i.e., $ q_{\\phi}(z|x) = \\mathcal{N}(z; \\mu(x), \\sigma^2(x)) $. The entropy of a Gaussian distribution with a diagonal covariance matrix is:\n",
    "\n",
    "$$ H(q_{\\phi}(z|x)) = \\frac{1}{2} \\sum_i \\ln(2\\pi e \\sigma_i^2) $$\n",
    "\n",
    "Now, the question arises: when is this entropy maximized?\n",
    "\n",
    "The answer is simple: **$ \\sigma_i^2 \\to +\\infty $**. In other words, the entropy tries to stretch the encoders as much as possible by enlarging their variances. However, this doesn't happen in practice, because the decoder works in conjunction with the encoder in the reconstruction error (RE) term. The decoder forces the encoder to be peaked, aiming for a one-to-one mapping from $ x $ to $ z $, much like in a non-stochastic autoencoder.\n",
    "\n",
    "#### Cross-Entropy Term:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Fig.6 An example of the eﬀect of the cross-entropy optimization with a non-learnable prior. The aggregated posterior (purple contours) tries to match the non-learnable prior (in blue). The purple arrows indicate the change of the aggregated posterior. An example of a hole is presented as a dark gray ellipse.\n",
    "\n",
    "![image-2.png](attachment:image-2.png)\n",
    "\n",
    "Fig.7 An example of the eﬀect of the cross-entropy optimization with a learnable prior. The aggregated posterior (purple contours) tries to match the learnable prior (blue contours). Notice that the aggregated posterior is modiﬁed to ﬁt the prior (purple arrows), but also the prior is updated to cover the aggregated posterior (orange arrows).\n",
    "\n",
    "The second part of $ \\Omega $ is the cross-entropy:\n",
    "\n",
    "$$ \\text{CE}(q_{\\phi}(z) \\parallel p_{\\lambda}(z)) = - \\mathbb{E}_{q_{\\phi}(z)}[\\ln p_{\\lambda}(z)] $$\n",
    "\n",
    "This cross-entropy term influences the VAE differently. To interpret this, we can ask: *How can we understand the cross-entropy between $ q_{\\phi}(z) $ and $ p_{\\lambda}(z) $?*\n",
    "\n",
    "In general, the cross-entropy tells us the average number of bits (or nats, since we use the natural logarithm) needed to identify an event drawn from $ q_{\\phi}(z) $, when using a coding scheme based on $ p_{\\lambda}(z) $. Since we aim to maximize the ELBO, the negative cross-entropy pushes us to minimize the divergence between $ q_{\\phi}(z) $ and $ p_{\\lambda}(z) $. In other words, we want $ q_{\\phi}(z) $ to match $ p_{\\lambda}(z) $.\n",
    "\n",
    "But why is this important? The cross-entropy forces the aggregated posterior to match the prior. This is the key observation, as the prior $ p_{\\lambda}(z) $ acts like an anchor for the posterior distribution $ q_{\\phi}(z) $.\n",
    "\n",
    "#### What Happens with a Non-Learnable Prior?\n",
    "\n",
    "If the prior is fixed (non-learnable), say a standard Gaussian prior, then optimizing the cross-entropy term forces the aggregated posterior to match this fixed prior. The process can be visualized as follows:\n",
    "\n",
    "1. The aggregated posterior (purple contours) tries to match the non-learnable prior (in blue). \n",
    "2. The aggregated posterior adapts (indicated by the purple arrows) to fit the prior.\n",
    "\n",
    "However, this optimization can be problematic because the decoder forces the encoder to be peaked, making it nearly impossible to match the fixed-shaped prior perfectly. As a result, \"holes\" can appear in the latent space—regions where the aggregated posterior assigns low probability, while the prior assigns relatively high probability (as shown by the dark gray ellipse).\n",
    "\n",
    "This issue becomes especially problematic in generation tasks, as sampling from the prior in these \"holes\" may result in low-quality samples.\n",
    "\n",
    "#### What Happens with a Learnable Prior?\n",
    "\n",
    "If the prior is learnable, the optimization allows both the aggregated posterior and the prior to adapt. Both distributions try to match each other, as shown below:\n",
    "\n",
    "1. The aggregated posterior (purple contours) adjusts to match the learnable prior (blue contours).\n",
    "2. The prior is also updated to cover the aggregated posterior (indicated by the orange arrows).\n",
    "\n",
    "With a learnable prior, the issue of holes is less prominent, especially if the prior is flexible enough. However, other optimization challenges may arise, as both the prior and posterior chase each other during training.\n",
    "\n",
    "In practice, using a learnable prior seems to be a better approach, though it remains an open question whether training all components at once is the best solution. Additionally, the learnable prior does not impose specific constraints on the latent space representation, such as sparsity. This could result in undesirable problems, such as non-smooth encoders.\n",
    "\n",
    "#### What Is the Best Prior?\n",
    "\n",
    "The ultimate question is: What is the best prior?\n",
    "\n",
    "The answer is hidden in the cross-entropy term. The best prior is actually the **aggregated posterior**:\n",
    "\n",
    "$$ p_{\\lambda}(z) = \\frac{1}{N} \\sum_{n=1}^{N} q_{\\phi}(z|x_n) $$\n",
    "\n",
    "If we set the prior to be equal to the aggregated posterior, the cross-entropy term becomes the entropy of the posterior, and the regularization term \\( \\Omega \\) becomes minimal. However, this is generally infeasible because:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af7a5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Define a basic Gaussian entropy calculation\n",
    "def gaussian_entropy(mu, sigma):\n",
    "    \"\"\"\n",
    "    Calculate the entropy of a Gaussian distribution with diagonal covariance matrix.\n",
    "    \n",
    "    Args:\n",
    "    - mu: Mean of the Gaussian (size of latent space)\n",
    "    - sigma: Standard deviation (size of latent space)\n",
    "\n",
    "    Returns:\n",
    "    - entropy: The entropy of the Gaussian distribution\n",
    "    \"\"\"\n",
    "    # Entropy of Gaussian with diagonal covariance: 1/2 * sum(ln(2 * pi * e * sigma^2))\n",
    "    entropy = 0.5 * sum([math.log(2 * math.pi * math.e * s**2) for s in sigma])\n",
    "    return entropy\n",
    "\n",
    "# Reconstruction Error (RE): sum of log-likelihood p(x|z)\n",
    "def reconstruction_error(x, z, model_decoder):\n",
    "    \"\"\"\n",
    "    Calculate the reconstruction error term of ELBO.\n",
    "    \n",
    "    Args:\n",
    "    - x: The input data\n",
    "    - z: Latent variables\n",
    "    - model_decoder: Function to decode z back to x (decoder network)\n",
    "    \n",
    "    Returns:\n",
    "    - RE: Reconstruction error\n",
    "    \"\"\"\n",
    "    reconstructed_x = model_decoder(z)  # decoded output for given z\n",
    "    # Assuming Gaussian likelihood, we calculate the negative log likelihood\n",
    "    # Assuming simple MSE for reconstruction\n",
    "    mse = sum((x_i - rec_x_i)**2 for x_i, rec_x_i in zip(x, reconstructed_x))\n",
    "    return mse\n",
    "\n",
    "# Cross-entropy between q(z) and p(z)\n",
    "def cross_entropy(q_phi_z, p_lambda_z):\n",
    "    \"\"\"\n",
    "    Calculate the cross-entropy between the aggregated posterior q(z) and the prior p(z).\n",
    "    \n",
    "    Args:\n",
    "    - q_phi_z: The aggregated posterior q(z) (mean of all latent variables)\n",
    "    - p_lambda_z: The prior p(z) (assumed to be Gaussian for simplicity)\n",
    "    \n",
    "    Returns:\n",
    "    - CE: Cross-entropy term\n",
    "    \"\"\"\n",
    "    cross_entropy_value = -sum(q * math.log(p) for q, p in zip(q_phi_z, p_lambda_z))\n",
    "    return cross_entropy_value\n",
    "\n",
    "# Implementing a simple model for the decoder (this should be replaced with your own model)\n",
    "def simple_decoder(z):\n",
    "    \"\"\"\n",
    "    A simple placeholder decoder function that mimics a reconstruction from z.\n",
    "    \"\"\"\n",
    "    return z  # In real-world cases, replace this with an actual decoder model\n",
    "\n",
    "# Define a sample latent space (for simplicity)\n",
    "latent_dim = 3\n",
    "sigma = [1.0] * latent_dim  # Simple standard deviation for each latent dimension\n",
    "mu = [0.0] * latent_dim     # Simple mean (zero) for each latent dimension\n",
    "\n",
    "# Example data\n",
    "x = [1.0, 2.0, 3.0]  # Example input data\n",
    "\n",
    "# Assume some random latent variables\n",
    "z = [0.1, 0.2, 0.3]  # Example latent vector\n",
    "\n",
    "# Calculate the entropy of the posterior q(z|x) (simple Gaussian)\n",
    "entropy_value = gaussian_entropy(mu, sigma)\n",
    "\n",
    "# Calculate the reconstruction error (RE) using a simple decoder model\n",
    "reconstruction_error_value = reconstruction_error(x, z, simple_decoder)\n",
    "\n",
    "# Assume an aggregated posterior q(z) (mean over all training samples)\n",
    "aggregated_posterior_q = [0.1, 0.1, 0.1]  # Example\n",
    "\n",
    "# Assume a prior p(z) (standard Gaussian prior)\n",
    "prior_p = [0.33, 0.33, 0.33]  # Example (normalized for simplicity)\n",
    "\n",
    "# Calculate cross-entropy term between q(z) and p(z)\n",
    "cross_entropy_value = cross_entropy(aggregated_posterior_q, prior_p)\n",
    "\n",
    "# Combine to form the full ELBO regularization term (Ω)\n",
    "Omega_value = entropy_value + cross_entropy_value\n",
    "\n",
    "# Print all calculated terms\n",
    "print(f\"Entropy: {entropy_value}\")\n",
    "print(f\"Reconstruction Error: {reconstruction_error_value}\")\n",
    "print(f\"Cross-Entropy: {cross_entropy_value}\")\n",
    "print(f\"Omega (Regularization term): {Omega_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f537b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
