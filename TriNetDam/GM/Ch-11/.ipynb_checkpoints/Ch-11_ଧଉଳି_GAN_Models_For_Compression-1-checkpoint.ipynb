{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a722fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright (c) 2004 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b133f57b",
   "metadata": {},
   "source": [
    "```markdown\n",
    "# Large Language Models and Generative AI Systems\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Communication is an essential aspect of human life, and the ability to express thoughts and ideas through language is a distinctive trait of our species. Writing, a significant breakthrough in human history, allowed us to preserve and transmit information over long distances and time.\n",
    "\n",
    "Since the 1950s, artificial intelligence (AI) has been fascinated with the idea of building communicating bots. The famous Turing Test, proposed by Alan Turing, states that a machine can be considered intelligent if it can converse with a human without being recognized as a machine. The first chatbot, ELIZA, was introduced in the 1960s and could match patterns and mimic conversations, but it lacked true intelligence.\n",
    "\n",
    "It took over 60 years to reach a point where we can debate intelligence, originality, and novelty in AI systems. This progress is largely due to generative modeling, which has enabled machines to understand and generate human-like language.\n",
    "\n",
    "Natural language processing (NLP) is a field that focuses on building machines that can manipulate human language, specifically text. NLP tasks include text classification, text correction, machine translation, semantic analysis, text generation, text summarization, named entity recognition, information retrieval, question answering, and chatbots.\n",
    "\n",
    "The main challenge in NLP is representing text in a way that is useful for downstream tasks. Classical NLP methods focused on syntactic structures, but semantics is crucial for proper communication. The Word2Vec method, introduced by [2], revolutionized NLP by allowing neural networks to learn word embeddings from raw text, capturing semantic meaning.\n",
    "\n",
    "## Large Language Models (LLMs)\n",
    "\n",
    "### What are Large Language Models?\n",
    "\n",
    "Large Language Models (LLMs) are language models parameterized by neural networks with millions or billions of weights. They have been developed to address the challenges of representing text and understanding its semantic meaning.\n",
    "\n",
    "LLMs have had a significant impact on NLP, enabling machines to learn and generate human-like language. They have been applied to various tasks, including text classification, machine translation, and text generation.\n",
    "\n",
    "### Natural Language Processing and Deep Learning\n",
    "\n",
    "The combination of deep learning and NLP has given rise to LLMs. Deep learning has enabled the development of powerful neural network architectures that can learn complex representations of text. These models can capture semantic meaning and generate coherent and contextually relevant language.\n",
    "\n",
    "### Multimodality and Generative AI Systems (GenAISys)\n",
    "\n",
    "While LLMs focus on processing text, the field of Generative AI Systems (GenAISys) aims to develop multimodal AI systems that can understand and generate content across various modalities, including text, images, audio, and more.\n",
    "\n",
    "GenAISys builds upon the advancements in LLMs and extends them to other modalities, enabling machines to understand and generate content in a more holistic and human-like manner.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Large Language Models have revolutionized natural language processing and opened up new possibilities for AI systems. By combining deep learning and NLP, LLMs have enabled machines to learn and generate human-like language, leading to significant advancements in various NLP tasks.\n",
    "\n",
    "The field of Generative AI Systems takes this a step further by aiming to develop multimodal AI systems that can understand and generate content across multiple modalities. This approach brings us closer to creating AI systems that can communicate and interact with humans in a more natural and intuitive way.\n",
    "\n",
    "\n",
    "\n",
    "# Large Language Models and Generative AI Systems\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Communication is an essential aspect of human life, and the ability to express thoughts and ideas through language is a distinctive trait of our species. Writing, a significant breakthrough in human history, allowed us to preserve and transmit information over long distances and time.\n",
    "\n",
    "Since the 1950s, artificial intelligence (AI) has been fascinated with the idea of building communicating bots. The famous Turing Test, proposed by Alan Turing, states that a machine can be considered intelligent if it can converse with a human without being recognized as a machine. The first chatbot, ELIZA, was introduced in the 1960s and could match patterns and mimic conversations, but it lacked true intelligence.\n",
    "\n",
    "It took over 60 years to reach a point where we can debate intelligence, originality, and novelty in AI systems. This progress is largely due to generative modeling, which has enabled machines to understand and generate human-like language.\n",
    "\n",
    "Natural language processing (NLP) is a field that focuses on building machines that can manipulate human language, specifically text. NLP tasks include text classification, text correction, machine translation, semantic analysis, text generation, text summarization, named entity recognition, information retrieval, question answering, and chatbots.\n",
    "\n",
    "The main challenge in NLP is representing text in a way that is useful for downstream tasks. Classical NLP methods focused on syntactic structures, but semantics is crucial for proper communication. The Word2Vec method revolutionized NLP by allowing neural networks to learn word embeddings from raw text, capturing semantic meaning.\n",
    "\n",
    "## Large Language Models (LLMs)\n",
    "\n",
    "### What are Large Language Models?\n",
    "\n",
    "Large Language Models (LLMs) are language models parameterized by neural networks with millions or billions of weights. They have been developed to address the challenges of representing text and understanding its semantic meaning.\n",
    "\n",
    "LLMs have had a significant impact on NLP, enabling machines to learn and generate human-like language. They have been applied to various tasks, including text classification, machine translation, and text generation.\n",
    "\n",
    "### Natural Language Processing and Deep Learning\n",
    "\n",
    "The combination of deep learning and NLP has given rise to LLMs. Deep learning has enabled the development of powerful neural network architectures that can learn complex representations of text. These models can capture semantic meaning and generate coherent and contextually relevant language.\n",
    "\n",
    "### Multimodality and Generative AI Systems (GenAISys)\n",
    "\n",
    "While LLMs focus on processing text, the field of Generative AI Systems (GenAISys) aims to develop multimodal AI systems that can understand and generate content across various modalities, including text, images, audio, and more.\n",
    "\n",
    "GenAISys builds upon the advancements in LLMs and extends them to other modalities, enabling machines to understand and generate content in a more holistic and human-like manner.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Large Language Models have revolutionized natural language processing and opened up new possibilities for AI systems. By combining deep learning and NLP, LLMs have enabled machines to learn and generate human-like language, leading to significant advancements in various NLP tasks.\n",
    "\n",
    "The field of Generative AI Systems takes this a step further by aiming to develop multimodal AI systems that can understand and generate content across multiple modalities. This approach brings us closer to creating AI systems that can communicate and interact with humans in a more natural and intuitive way.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b413535b",
   "metadata": {},
   "source": [
    "# Large Language Models (LLMs)\n",
    "\n",
    "## General Architectures of LLMs\n",
    "\n",
    "LLMs have three main types of architectures:\n",
    "\n",
    "1. **Encoders**: These LLMs take a piece of text (string) and return an encoding, which is a numerical representation of the input. Encoders have access to the entire input at any point during processing and do not require specific constraints. They provide outputs in a single forward run during both training and inference.\n",
    "\n",
    "2. **Decoders**: This class of LLMs is used for generating new text. They are autoregressive models, meaning the neural networks used to parameterize them must be causal. The sampling procedure for decoders is an iterative process, making it typically slower than encoders.\n",
    "\n",
    "3. **Encoder-Decoders and Encoder-Encoders**: LLMs can be conditional, meaning they require a combination of an encoder to process conditioning and another encoder or decoder to provide an encoding of the input text or generate new text, respectively. Figure 11.2a shows a schematic representation of an encoder or decoder, while Figure 11.2b illustrates an encoder-encoder or encoder-decoder.\n",
    "\n",
    "## Parameterizations\n",
    "\n",
    "The parameterization of LLMs is a crucial aspect, and it involves using hierarchical, deep neural networks. The choice of neural network architecture depends on the specific requirements of the LLM.\n",
    "\n",
    "### Recurrent Neural Networks (RNNs)\n",
    "\n",
    "RNNs were among the first neural networks used for language modeling due to their intrinsic sequential structure. They were used to formulate RNN-based decoders [4] and encoder-decoders [5]. However, RNNs suffer from forgetting issues.\n",
    "\n",
    "### Convolutional Neural Networks (CNNs)\n",
    "\n",
    "CNNs were successfully used in the first encoders [7] and later in decoders [8] and encoder-decoders [9]. The combination of CNNs and RNNs showed improvements in language modeling [6]. However, CNN-based language models also faced scaling issues.\n",
    "\n",
    "### Transformers\n",
    "\n",
    "The introduction of transformers [10] brought a significant breakthrough in LLMs. Transformers utilize (multihead) attention layers and implement important techniques like multihead attention, layer normalization, and positional embeddings. They excel at scaling up models, resulting in models with billions of weights. However, transformers have quadratic time and memory complexity in the number of tokens.\n",
    "\n",
    "### Lean Large Language Models (L3M)\n",
    "\n",
    "There is a growing focus on making LLMs leaner, known as L3M. The main aspects of obtaining L3M include:\n",
    "\n",
    "1. **Quantization**: Reducing the number of bits per weight to lower physical memory requirements on disk.\n",
    "2. **Faster Training and Inference**: Various methods have been proposed to speed up training and inference, such as FlashAttention [11] and the linear transformer [13].\n",
    "\n",
    "### Selective State Space Models (S3Ms)\n",
    "\n",
    "S3Ms are built on state space models and operate like RNNs at inference time, offering fully parallelizable training. They provide an alternative approach to transformer-based language models.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The parameterization and architecture of LLMs are critical aspects that determine their performance and efficiency. The choice of neural network architecture depends on the specific requirements and constraints of the LLM. Transformers have revolutionized LLMs, but there is ongoing research to make them more efficient and scalable.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cee45d68",
   "metadata": {},
   "source": [
    "# Learning Large Language Models (LLMs)\n",
    "\n",
    "## Training Procedure of FMs and LLMs\n",
    "\n",
    "The training procedure of LLMs typically involves two stages:\n",
    "\n",
    "1. **Pre-training**: This initial stage aims to prepare an LLM for further tasks. The model is trained using either the masked loss or the negative log-likelihood. The masked loss is used for encoders, while the negative log-likelihood is typically utilized by decoders. The pre-training stage requires a large amount of data to train general patterns in the data, such as grammar, word co-occurrences, or specific programming languages.\n",
    "\n",
    "2. **Fine-tuning**: A pre-trained model is further specialized on another dataset for a downstream task. This stage involves fine-tuning the LLM on specific data, such as legal data for generating legal documents or a new programming language. LLMs can also be fine-tuned for various tasks like text summarization, Q&A, text classification, sentiment analysis, and more.\n",
    "\n",
    "## Losses and Objectives\n",
    "\n",
    "During the training process, different losses and objectives are used depending on the task:\n",
    "\n",
    "- **Masked Loss**: Used for encoders, it aims to reconstruct masked tokens from the input.\n",
    "- **Negative Log-Likelihood**: Typically utilized by decoders, it minimizes the negative log-likelihood of the generated output.\n",
    "- **Additional Neural Network**: For certain tasks, an additional neural network is optimized using a different objective, denoted as $\\mathcal{L}_{\\text{pred}}(\\theta, \\phi)$.\n",
    "\n",
    "## Combining Losses\n",
    "\n",
    "It is possible to combine different losses, such as $\\mathcal{L}_{\\alpha}(\\theta) = \\mathcal{L}(\\theta) + \\alpha \\mathcal{L}_{\\text{masked}}(\\theta)$, which can be seen as a penalized negative log-likelihood objective. This idea has been utilized in pre-training LLMs for various problems simultaneously [25] or for pre-training LLMs for molecules [26].\n",
    "\n",
    "## Fine-tuning Challenges and Solutions\n",
    "\n",
    "Fine-tuning LLMs can be challenging due to their large size, making it computationally expensive and time-consuming. To address this issue, techniques like Low-Rank Adaptation (LoRA) [27] have been proposed. LoRA introduces a new set of learnable matrices, $A$ and $B$, during fine-tuning, while keeping the original weight matrix $W$ fixed. The forward pass is then calculated as:\n",
    "\n",
    "$$h_l = W h_{l-1} + B A h_{l-1}$$\n",
    "\n",
    "LoRA updates only a small fraction of the original number of weights, typically less than 1% of the LLM's weights. This approach allows for efficient fine-tuning while achieving similar results as full fine-tuning.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The training procedure of LLMs involves pre-training and fine-tuning stages, with different losses and objectives depending on the task. Fine-tuning LLMs can be computationally intensive, but techniques like LoRA provide an efficient solution by updating only a small subset of the model's weights.\n",
    "\n",
    "# teenyGPT: A Tiny Implementation of a Decoder LLM\n",
    "\n",
    "## Loss Function\n",
    "\n",
    "The loss function used in teenyGPT is the negative log-likelihood, which is implemented in the `LossFun` class. This class takes the model's output (`y_model`) and the true labels (`y_true`) as input and calculates the loss. The loss is computed as the negative log-likelihood of the predicted probabilities.\n",
    "\n",
    "```python\n",
    "class LossFun(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loss = nn.NLLLoss(reduction='none')\n",
    "\n",
    "    def forward(self, y_model, y_true, reduction='sum'):\n",
    "        # y_model: B(atch) x T(okens) x V(alues)\n",
    "        # y_true: B x T\n",
    "        B, T, V = y_model.size()\n",
    "\n",
    "        y_model = y_model.view(B * T, V)\n",
    "        y_true = y_true.view(B * T)\n",
    "\n",
    "        loss_matrix = self.loss(y_model, y_true)  # B*T\n",
    "\n",
    "        if reduction == 'sum':\n",
    "            return torch.sum(loss_matrix)\n",
    "        elif reduction == 'mean':\n",
    "            loss_matrix = loss_matrix.view(B, T)\n",
    "            return torch.mean(torch.sum(loss_matrix, 1))\n",
    "        else:\n",
    "            raise ValueError(\"Reduction could be either 'sum' or 'mean'.\")\n",
    "\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Fig.3 Examples of results: (a) The negative log-likelihood calculated on the validation set. (b) The top one reconstruction accuracy calculated on the validation set.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ff211af",
   "metadata": {},
   "source": [
    "```markdown\n",
    "# teenyGPT: A Tiny Implementation of a Decoder LLM\n",
    "\n",
    "## Loss Function\n",
    "\n",
    "The loss function used in teenyGPT is the negative log-likelihood, which is implemented in the `LossFun` class. This class takes the model's output (`y_model`) and the true labels (`y_true`) as input and calculates the loss. The loss is computed as the negative log-likelihood of the predicted probabilities.\n",
    "\n",
    "```python\n",
    "class LossFun(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loss = nn.NLLLoss(reduction='none')\n",
    "\n",
    "    def forward(self, y_model, y_true, reduction='sum'):\n",
    "        # y_model: B(atch) x T(okens) x V(alues)\n",
    "        # y_true: B x T\n",
    "        B, T, V = y_model.size()\n",
    "\n",
    "        y_model = y_model.view(B * T, V)\n",
    "        y_true = y_true.view(B * T)\n",
    "\n",
    "        loss_matrix = self.loss(y_model, y_true)  # B*T\n",
    "\n",
    "        if reduction == 'sum':\n",
    "            return torch.sum(loss_matrix)\n",
    "        elif reduction == 'mean':\n",
    "            loss_matrix = loss_matrix.view(B, T)\n",
    "            return torch.mean(torch.sum(loss_matrix, 1))\n",
    "        else:\n",
    "            raise ValueError(\"Reduction could be either 'sum' or 'mean'.\")\n",
    "```\n",
    "\n",
    "## Transformer Block\n",
    "\n",
    "The transformer block is a crucial component of teenyGPT. It is implemented using the PyTorch implementation of multi-head attention layers. The `TransformerBlock` class takes the number of embeddings, number of neurons, and number of heads as input.\n",
    "\n",
    "```python\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, num_emb, num_neurons, num_heads=4):\n",
    "        super().__init__()\n",
    "\n",
    "        # hyperparameters\n",
    "        self.D = num_emb\n",
    "        self.H = num_heads\n",
    "        self.neurons = num_neurons\n",
    "\n",
    "        # components\n",
    "        self.msha = nn.MultiheadAttention(embed_dim=self.D, num_heads=self.H, batch_first=True)\n",
    "        self.layer_norm1 = nn.LayerNorm(self.D)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.D)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.D, self.neurons * self.D),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.neurons * self.D, self.D)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, causal=True):\n",
    "        # Multi-Head Self-Attention\n",
    "        x_attn, _ = self.msha(x, x, x, is_causal=causal, attn_mask=torch.empty(1, 1), need_weights=False)\n",
    "\n",
    "        # LayerNorm\n",
    "        x = self.layer_norm1(x_attn + x)\n",
    "\n",
    "        # MLP\n",
    "        x_mlp = self.mlp(x)\n",
    "\n",
    "        # LayerNorm\n",
    "        x = self.layer_norm2(x_mlp + x)\n",
    "\n",
    "        return x\n",
    "```\n",
    "\n",
    "## teenyGPT Class\n",
    "\n",
    "The `teenyGPT` class defines the forward pass for the transformer and the sampling procedure. It also includes an auxiliary metric, top-one reconstruction accuracy, which checks if the most probable token matches the input token.\n",
    "\n",
    "```python\n",
    "class teenyGPT(nn.Module):\n",
    "    def __init__(self, num_tokens, num_token_vals, num_emb, num_neurons, num_heads=2, dropout_prob=0.1, num_blocks=10, device='cpu'):\n",
    "        super().__init__()\n",
    "\n",
    "        # hyperparameters\n",
    "        self.device = device\n",
    "        self.num_tokens = num_tokens\n",
    "        self.num_token_vals = num_token_vals\n",
    "        self.num_emb = num_emb\n",
    "        self.num_blocks = num_blocks\n",
    "\n",
    "        # embedding layer\n",
    "        self.embedding = torch.nn.Embedding(num_token_vals, num_emb)\n",
    "\n",
    "        # positional embedding\n",
    "        self.positional_embedding = nn.Embedding(num_tokens, num_emb)\n",
    "\n",
    "        # transformer blocks\n",
    "        self.transformer_blocks = nn.ModuleList()\n",
    "        for _ in range(num_blocks):\n",
    "            self.transformer_blocks.append(\n",
    "                TransformerBlock(num_emb=num_emb, num_neurons=num_neurons, num_heads=num_heads)\n",
    "            )\n",
    "\n",
    "        # output layer (logits + softmax)\n",
    "        self.logits = nn.Sequential(nn.Linear(num_emb, num_token_vals))\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # loss function\n",
    "        self.loss_fun = LossFun()\n",
    "\n",
    "    def transformer_forward(self, x, causal=True, temperature=1.0):\n",
    "        # x: B(atch) x T(okens)\n",
    "\n",
    "        # embedding of tokens\n",
    "        x = self.embedding(x)  # B x T x D\n",
    "\n",
    "        # embedding of positions\n",
    "        pos = torch.arange(0, x.shape[1], dtype=torch.long).unsqueeze(0).to(self.device)\n",
    "        pos_emb = self.positional_embedding(pos)\n",
    "\n",
    "        # dropout of embedding of inputs\n",
    "        x = self.dropout(x + pos_emb)\n",
    "\n",
    "        # transformer blocks\n",
    "        for i in range(self.num_blocks):\n",
    "            x = self.transformer_blocks[i](x)\n",
    "\n",
    "        # output logits\n",
    "        out = self.logits(x)\n",
    "\n",
    "        return F.log_softmax(out / temperature, 2)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, batch_size=4, temperature=1.0):\n",
    "        x_seq = np.asarray([[self.num_token_vals - 1] for i in range(batch_size)])\n",
    "\n",
    "        # sample next tokens\n",
    "        for i in range(self.num_tokens - 1):\n",
    "            xx = torch.tensor(x_seq, dtype=torch.long, device=self.device)\n",
    "            x_log_probs = self.transformer_forward(xx, temperature=temperature)\n",
    "            x_i_sample = torch.multinomial(torch.exp(x_log_probs[:, i]), 1).to(self.device)\n",
    "            x_seq = np.concatenate((x_seq, x_i_sample.to('cpu').detach().numpy()), 1)\n",
    "\n",
    "        return x_seq\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def top1_rec(self, x, causal=True):\n",
    "        x_prob = torch.exp(self.transformer_forward(x, causal=True))[:, :-1, :].contiguous()\n",
    "        _, x_rec_max = torch.max(x_prob, dim=2)\n",
    "        return torch.sum(torch.mean((x_rec_max.float() == x[:, 1:].float().to(self.device)).float(), 1).float())\n",
    "\n",
    "    def forward(self, x, causal=True, temperature=1.0, reduction='mean'):\n",
    "        log_prob = self.transformer_forward(x, causal=causal, temperature=temperature)\n",
    "        return self.loss_fun(log_prob[:, :-1].contiguous(), x[:, 1:].contiguous(), reduction=reduction)\n",
    "```\n",
    "\n",
    "## Results\n",
    "\n",
    "With 128 neurons in MLPs, 8 attention heads, 4 transformer blocks, and an embedding size of 32, teenyGPT has approximately 1 million weights. The model's performance is illustrated in Figure 11.3 and Tables 11.3 and 11.4. As you can see, even with a small amount of data and a limited number of weights, teenyGPT trains successfully and achieves reasonable performance.\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "Fig.3: Examples of results: (a) Negative log-likelihood calculated on the validation set. (b) Top-one reconstruction accuracy calculated on the validation set.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The teenyGPT implementation demonstrates the feasibility of training a small decoder LLM with a limited amount of data and a relatively small number of weights. Despite its simplicity, teenyGPT achieves reasonable performance, showcasing the potential of LLMs in natural language processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f61e033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class LossFun(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loss = nn.NLLLoss(reduction='none')\n",
    "\n",
    "    def forward(self, y_model, y_true, reduction='sum'):\n",
    "        B, T, V = y_model.size()\n",
    "        y_model = y_model.view(B * T, V)\n",
    "        y_true = y_true.view(B * T)\n",
    "        loss_matrix = self.loss(y_model, y_true)\n",
    "        if reduction == 'sum':\n",
    "            return torch.sum(loss_matrix)\n",
    "        elif reduction == 'mean':\n",
    "            loss_matrix = loss_matrix.view(B, T)\n",
    "            return torch.mean(torch.sum(loss_matrix, 1))\n",
    "        else:\n",
    "            raise ValueError(\"Reduction could be either 'sum' or 'mean'.\")\n",
    "\n",
    "import torch.nn.Transformer as nn_Transformer\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, num_emb, num_neurons, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.D = num_emb\n",
    "        self.H = num_heads\n",
    "        self.neurons = num_neurons\n",
    "        self.msha = nn_Transformer.TransformerEncoderLayer(d_model=self.D, nhead=self.H)\n",
    "        self.layer_norm1 = nn.LayerNorm(self.D)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.D)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.D, self.neurons * self.D),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.neurons * self.D, self.D)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_attn = self.msha(x)\n",
    "        x = self.layer_norm1(x_attn + x)\n",
    "        x_mlp = self.mlp(x)\n",
    "        x = self.layer_norm2(x_mlp + x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class teenyGPT(nn.Module):\n",
    "    def __init__(self, num_tokens, num_token_vals, num_emb, num_neurons, num_heads=2, dropout_prob=0.1, num_blocks=10, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.num_tokens = num_tokens\n",
    "        self.num_token_vals = num_token_vals\n",
    "        self.num_emb = num_emb\n",
    "        self.num_blocks = num_blocks\n",
    "        self.embedding = torch.nn.Embedding(num_token_vals, num_emb)\n",
    "        self.positional_embedding = nn.Embedding(num_tokens, num_emb)\n",
    "        self.transformer_blocks = nn.ModuleList()\n",
    "        for _ in range(num_blocks):\n",
    "            self.transformer_blocks.append(\n",
    "                TransformerBlock(num_emb=num_emb, num_neurons=num_neurons, num_heads=num_heads)\n",
    "            )\n",
    "        self.logits = nn.Sequential(nn.Linear(num_emb, num_token_vals))\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.loss_fun = LossFun()\n",
    "\n",
    "    def transformer_forward(self, x, causal=True, temperature=1.0):\n",
    "        x = self.embedding(x)\n",
    "        pos = torch.arange(0, x.shape[1], dtype=torch.long).unsqueeze(0).to(self.device)\n",
    "        pos_emb = self.positional_embedding(pos)\n",
    "        x = self.dropout(x + pos_emb)\n",
    "        for i in range(self.num_blocks):\n",
    "            x = self.transformer_blocks[i](x)\n",
    "        out = self.logits(x)\n",
    "        return F.log_softmax(out / temperature, 2)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, batch_size=4, temperature=1.0):\n",
    "        x_seq = np.asarray([[self.num_token_vals - 1] for i in range(batch_size)])\n",
    "        for i in range(self.num_tokens - 1):\n",
    "            xx = torch.tensor(x_seq, dtype=torch.long, device=self.device)\n",
    "            x_log_probs = self.transformer_forward(xx, temperature=temperature)\n",
    "            x_i_sample = torch.multinomial(torch.exp(x_log_probs[:, i]), 1).to(self.device)\n",
    "            x_seq = np.concatenate((x_seq, x_i_sample.to('cpu').detach().numpy()), 1)\n",
    "        return x_seq\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def top1_rec(self, x, causal=True):\n",
    "        x_prob = torch.exp(self.transformer_forward(x, causal=True))[:, :-1, :].contiguous()\n",
    "        _, x_rec_max = torch.max(x_prob, dim=2)\n",
    "        return torch.sum(torch.mean((x_rec_max.float() == x[:, 1:].float().to(self.device)).float(), 1).float())\n",
    "\n",
    "    def forward(self, x, causal=True, temperature=1.0, reduction='mean'):\n",
    "        log_prob = self.transformer_forward(x, causal=causal, temperature=temperature)\n",
    "        return self.loss_fun(log_prob[:, :-1].contiguous(), x[:, 1:].contiguous(), reduction=reduction)\n",
    "\n",
    "# Example usage\n",
    "num_tokens = 100\n",
    "num_token_vals = 20\n",
    "num_emb = 32\n",
    "num_neurons = 128\n",
    "num_heads = 8\n",
    "dropout_prob = 0.1\n",
    "num_blocks = 4\n",
    "device = 'cpu'\n",
    "\n",
    "model = teenyGPT(num_tokens, num_token_vals, num_emb, num_neurons, num_heads, dropout_prob, num_blocks, device)\n",
    "\n",
    "# Example input data\n",
    "x = torch.randint(0, num_token_vals, (4, num_tokens))\n",
    "\n",
    "# Forward pass\n",
    "loss = model(x)\n",
    "print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "# Sampling\n",
    "samples = model.sample(batch_size=4)\n",
    "print(f\"Sampled sequences: {samples}\")\n",
    "\n",
    "# Top-one reconstruction accuracy\n",
    "acc = model.top1_rec(x)\n",
    "print(f\"Top-one reconstruction accuracy: {acc.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d79179",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFun:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, y_model, y_true, reduction='sum'):\n",
    "        total_loss = 0\n",
    "        for i in range(len(y_model)):\n",
    "            if reduction == 'sum':\n",
    "                total_loss += self.loss(y_model[i], y_true[i])\n",
    "            elif reduction == 'mean':\n",
    "                total_loss += self.loss(y_model[i], y_true[i]) / len(y_model)\n",
    "        return total_loss\n",
    "\n",
    "    def loss(self, y_model, y_true):\n",
    "        return -sum(y_true[i] * y_model[i] for i in range(len(y_true)))\n",
    "\n",
    "class TransformerBlock:\n",
    "    def __init__(self, num_emb, num_neurons, num_heads=4):\n",
    "        self.D = num_emb\n",
    "        self.H = num_heads\n",
    "        self.neurons = num_neurons\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_attn = self.msha(x)\n",
    "        x = self.layer_norm1(x_attn + x)\n",
    "        x_mlp = self.mlp(x)\n",
    "        x = self.layer_norm2(x_mlp + x)\n",
    "        return x\n",
    "\n",
    "    def msha(self, x):\n",
    "        return [sum(x[i] * x[j] for j in range(len(x))) for i in range(len(x))]\n",
    "\n",
    "    def layer_norm1(self, x):\n",
    "        return [x[i] / sum(x) for i in range(len(x))]\n",
    "\n",
    "    def mlp(self, x):\n",
    "        return [sum(x[i] * x[j] for j in range(len(x))) for i in range(len(x))]\n",
    "\n",
    "    def layer_norm2(self, x):\n",
    "        return [x[i] / sum(x) for i in range(len(x))]\n",
    "\n",
    "class teenyGPT:\n",
    "    def __init__(self, num_tokens, num_token_vals, num_emb, num_neurons, num_heads=2, dropout_prob=0.1, num_blocks=10, device='cpu'):\n",
    "        self.device = device\n",
    "        self.num_tokens = num_tokens\n",
    "        self.num_token_vals = num_token_vals\n",
    "        self.num_emb = num_emb\n",
    "        self.num_blocks = num_blocks\n",
    "        self.embedding = [i for i in range(num_token_vals)]\n",
    "        self.positional_embedding = [i for i in range(num_tokens)]\n",
    "        self.transformer_blocks = [TransformerBlock(num_emb, num_neurons, num_heads) for _ in range(num_blocks)]\n",
    "        self.logits = [i for i in range(num_token_vals)]\n",
    "        self.dropout = dropout_prob\n",
    "        self.loss_fun = LossFun()\n",
    "\n",
    "    def __call__(self, x, causal=True, temperature=1.0, reduction='mean'):\n",
    "        return self.forward(x, causal, temperature, reduction)\n",
    "\n",
    "    def forward(self, x, causal=True, temperature=1.0, reduction='mean'):\n",
    "        log_prob = self.transformer_forward(x, causal, temperature)\n",
    "        return self.loss_fun.forward(log_prob, x, reduction)\n",
    "\n",
    "    def transformer_forward(self, x, causal=True, temperature=1.0):\n",
    "        x = [self.embedding[i] for i in x]\n",
    "        pos = [i for i in range(len(x))]\n",
    "        pos_emb = [self.positional_embedding[i] for i in pos]\n",
    "        x = [x[i] + pos_emb[i] for i in range(len(x))]\n",
    "        for i in range(self.num_blocks):\n",
    "            x = self.transformer_blocks[i].forward(x)\n",
    "        out = [self.logits[i] for i in x]\n",
    "        return out\n",
    "\n",
    "    def sample(self, batch_size=4, temperature=1.0):\n",
    "        x_seq = [[self.num_token_vals - 1] for _ in range(batch_size)]\n",
    "        for _ in range(self.num_tokens - 1):\n",
    "            xx = [x_seq[i][-1] for i in range(len(x_seq))]\n",
    "            x_log_probs = self.transformer_forward(xx, temperature=temperature)\n",
    "            x_i_sample = [x_log_probs[i].index(max(x_log_probs[i])) for i in range(len(x_log_probs))]\n",
    "            x_seq = [x_seq[i] + [x_i_sample[i]] for i in range(len(x_seq))]\n",
    "        return x_seq\n",
    "\n",
    "    def top1_rec(self, x, causal=True):\n",
    "        x_prob = self.transformer_forward(x, causal=True)\n",
    "        x_rec_max = [x_prob[i].index(max(x_prob[i])) for i in range(len(x_prob))]\n",
    "        return sum(1 for i in range(len(x)) if x_rec_max[i] == x[i][1]) / len(x)\n",
    "\n",
    "# Example usage\n",
    "num_tokens = 100\n",
    "num_token_vals = 20\n",
    "num_emb = 32\n",
    "num_neurons = 128\n",
    "num_heads = 8\n",
    "dropout_prob = 0.1\n",
    "num_blocks = 4\n",
    "device = 'cpu'\n",
    "\n",
    "model = teenyGPT(num_tokens, num_token_vals, num_emb, num_neurons, num_heads, dropout_prob, num_blocks, device)\n",
    "\n",
    "# Example input data\n",
    "x = [[i % num_token_vals] for i in range(4 * num_tokens)]\n",
    "\n",
    "# Forward pass\n",
    "loss = model(x)\n",
    "print(f\"Loss: {loss}\")\n",
    "\n",
    "# Sampling\n",
    "samples = model.sample(batch_size=4)\n",
    "print(f\"Sampled sequences: {samples}\")\n",
    "\n",
    "# Top-one reconstruction accuracy\n",
    "acc = model.top1_rec(x)\n",
    "print(f\"Top-one reconstruction accuracy: {acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaa25f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
